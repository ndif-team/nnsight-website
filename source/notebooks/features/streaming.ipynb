{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming enables users apply functions and datasets locally during remote model execution. This allows users to stream results for immediate consumption (i.e., seeing tokens as they are generated) or applying non-whitelisted functions such as model tokenizers, large local datasets, and more!\n",
    "\n",
    "*   `nnsight.local()` context sends values immediately to user's local machine from server\n",
    "*   Intervention graph is executed locally on downstream nodes\n",
    "*   Exiting local context uploads data back to server\n",
    "*   `@nnsight.trace` function decorator enables custom functions to be added to intervention graph when using `nnsight.local()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in Google Colab, install nnsight\n",
    "try:\n",
    "    import google.colab\n",
    "    is_colab = True\n",
    "except ImportError:\n",
    "    is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    !pip install -U nnsight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nnsight.local()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may sometimes want to locally access and manipulate values during remote execution. Using `.local()` on a proxy, you can send remote content to your local machine and apply local functions. The intervention graph is then executed locally on downstream nodes (until you send execution back to the remote server by exiting the `.local()` context).\n",
    "\n",
    "There are a few use cases for streaming with `.local()`, including live chat generation and applying large datasets or non-whitelisted local functions to the intervention graph.\n",
    "\n",
    "Now let's explore how streaming works. We'll start by grabbing some hidden states of the model and printing their value using `tracer.log()`. Without calling `nnsight.local()`, these operations will all occur remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import CONFIG\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if is_colab:\n",
    "    # include your HuggingFace Token and NNsight API key on Colab secrets\n",
    "    from google.colab import userdata\n",
    "    NDIF_API = userdata.get('NDIF_API')\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "    CONFIG.set_default_api_key(NDIF_API)\n",
    "    !huggingface-cli login -token HF_TOKEN\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "llama = LanguageModel(\"meta-llama/Meta-Llama-3.1-70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:10:19,417 270a6e4d-53a5-4929-9f1d-d82fdef7292d - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-17 15:10:20,756 270a6e4d-53a5-4929-9f1d-d82fdef7292d - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-17 15:10:21,638 270a6e4d-53a5-4929-9f1d-d82fdef7292d - RUNNING: Your job has started running.\n",
      "2025-03-17 15:10:23,319 270a6e4d-53a5-4929-9f1d-d82fdef7292d - LOG: tensor(5.4688, device='cuda:2')\n",
      "2025-03-17 15:10:25,060 270a6e4d-53a5-4929-9f1d-d82fdef7292d - COMPLETED: Your job has been completed.\n",
      "Downloading result:   0%|          | 0.00/514k [00:00<?, ?B/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading result: 100%|██████████| 514k/514k [00:00<00:00, 1.92MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.3750,  8.6250, 13.0000,  ..., -4.1562, -4.1562, -4.1562],\n",
      "         [10.5000,  2.6406,  4.7812,  ..., -8.8750, -8.8750, -8.8750]]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This will give you a remote LOG response because it's coming from the remote server\n",
    "with llama.trace(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    hs = llama.model.layers[-1].output[0]\n",
    "\n",
    "    tracer.log(hs[0,0,0])\n",
    "\n",
    "    out =  llama.lm_head.output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:10:42,787 1f677938-114a-4efe-8b5b-eabbb6090ebf - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-17 15:10:43,386 1f677938-114a-4efe-8b5b-eabbb6090ebf - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-17 15:10:43,690 1f677938-114a-4efe-8b5b-eabbb6090ebf - RUNNING: Your job has started running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4688, dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:10:44,819 1f677938-114a-4efe-8b5b-eabbb6090ebf - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 514k/514k [00:00<00:00, 1.77MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.3750,  8.6250, 13.0000,  ..., -4.1562, -4.1562, -4.1562],\n",
      "         [10.5000,  2.6406,  4.7812,  ..., -8.8750, -8.8750, -8.8750]]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nnsight\n",
    "# This will print locally because it's already local\n",
    "with llama.trace(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    with nnsight.local():\n",
    "        hs = llama.model.layers[-1].output[0]\n",
    "        tracer.log(hs[0,0,0])\n",
    "\n",
    "    out =  llama.lm_head.output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `@nnsight.trace` function decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use function decorators to create custom functions to be used during `.local` calls. This is a handy way to enable live streaming of a chat or to train probing classifiers on model hidden states.\n",
    "\n",
    "Let's try out `@nnsight.trace` and `nnsight.local()` to access a custom function during remote execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:10:50,961 bf34e807-2ae2-4ab4-be77-3ac0836c7e28 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-03-17 15:10:54,240 bf34e807-2ae2-4ab4-be77-3ac0836c7e28 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-03-17 15:10:54,244 bf34e807-2ae2-4ab4-be77-3ac0836c7e28 - RUNNING: Your job has started running.\n",
      "2025-03-17 15:10:54,842 bf34e807-2ae2-4ab4-be77-3ac0836c7e28 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 258k/258k [00:00<00:00, 1.14MB/s]\n"
     ]
    }
   ],
   "source": [
    "# first, let's define our function\n",
    "@nnsight.trace # decorator that enables this function to be added to the intervention graph\n",
    "def my_local_fn(value):\n",
    "    return value * 0\n",
    "\n",
    "# We use a local function to ablate some hidden states\n",
    "# This downloads the data for the .local context, and then uploads it back to set the value.\n",
    "with llama.generate(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    hs = llama.model.layers[-1].output[0]\n",
    "\n",
    "    with nnsight.local():\n",
    "\n",
    "        hs = my_local_fn(hs)\n",
    "\n",
    "    llama.model.layers[-1].output[0][:] = hs\n",
    "\n",
    "    out =  llama.lm_head.output.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that without calling `.local`, the remote API does not know about `my_local_fn` and will throw a whitelist error. A whitelist error occurs because you are being allowed access to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FunctionWhitelistError",
     "evalue": "Function with name `__main__.my_local_fn` not in function whitelist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFunctionWhitelistError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m llama\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, remote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m      3\u001b[0m     hs \u001b[38;5;241m=\u001b[39m llama\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     hs \u001b[38;5;241m=\u001b[39m my_local_fn(hs) \u001b[38;5;66;03m# no .local - will cause an error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:82\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     78\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     80\u001b[0m graph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/intervention/backends/remote.py:77\u001b[0m, in \u001b[0;36mRemoteBackend.__call__\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocking:\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# Do blocking request.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocking_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# Otherwise we are getting the status / result of the existing job.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_blocking_request(graph)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/intervention/backends/remote.py:289\u001b[0m, in \u001b[0;36mRemoteBackend.blocking_request\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    280\u001b[0m sio\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mws_address,\n\u001b[1;32m    282\u001b[0m     socketio_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ws/socket.io\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    283\u001b[0m     transports\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebsocket\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    284\u001b[0m     wait_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m remote_graph \u001b[38;5;241m=\u001b[39m preprocess(graph)\n\u001b[0;32m--> 289\u001b[0m data, headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremote_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sio\u001b[38;5;241m.\u001b[39msid\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Submit request via\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/intervention/backends/remote.py:60\u001b[0m, in \u001b[0;36mRemoteBackend.request\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: Graph) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mRequestModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_key,\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent-timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()),\n\u001b[1;32m     68\u001b[0m     }\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data, headers\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/request.py:43\u001b[0m, in \u001b[0;36mRequestModel.serialize\u001b[0;34m(graph, format, _zlib)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialize\u001b[39m(graph: Graph, \u001b[38;5;28mformat\u001b[39m:\u001b[38;5;28mstr\u001b[39m, _zlib:\u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mRequestModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         json \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmodel_dump(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m         data \u001b[38;5;241m=\u001b[39m msgspec\u001b[38;5;241m.\u001b[39mjson\u001b[38;5;241m.\u001b[39mencode(json)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/request.py:30\u001b[0m, in \u001b[0;36mRequestModel.__init__\u001b[0;34m(self, memo, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, memo: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemo \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mMEMO}\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/format/types.py:276\u001b[0m, in \u001b[0;36mGraphModel.to_model\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_model\u001b[39m(value: Graph) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGraphModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/format/types.py:77\u001b[0m, in \u001b[0;36mmemoized.<locals>.inner\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(value):\n\u001b[0;32m---> 77\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     _id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(value)\n\u001b[1;32m     81\u001b[0m     MEMO[_id] \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/format/types.py:101\u001b[0m, in \u001b[0;36mNodeModel.to_model\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@memoized\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_model\u001b[39m(value: Node) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNodeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/format/types.py:244\u001b[0m, in \u001b[0;36mFunctionModel.to_model\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_model\u001b[39m(value:FUNCTION):\n\u001b[1;32m    242\u001b[0m     model \u001b[38;5;241m=\u001b[39m FunctionModel(function_name\u001b[38;5;241m=\u001b[39mget_function_name(value))\n\u001b[0;32m--> 244\u001b[0m     \u001b[43mFunctionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_function_whitelist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nnsight/lib/python3.10/site-packages/nnsight/schema/format/types.py:251\u001b[0m, in \u001b[0;36mFunctionModel.check_function_whitelist\u001b[0;34m(cls, qualname)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_function_whitelist\u001b[39m(\u001b[38;5;28mcls\u001b[39m, qualname: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m qualname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m FUNCTIONS_WHITELIST:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FunctionWhitelistError(\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction with name `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` not in function whitelist.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         )\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qualname\n",
      "\u001b[0;31mFunctionWhitelistError\u001b[0m: Function with name `__main__.my_local_fn` not in function whitelist."
     ]
    }
   ],
   "source": [
    "with llama.trace(\"hello\", remote=True) as tracer:\n",
    "\n",
    "    hs = llama.model.layers[-1].output[0]\n",
    "\n",
    "    hs = my_local_fn(hs) # no .local - will cause an error\n",
    "\n",
    "    llama.model.layers[-1].output[0][:] = hs * 2\n",
    "\n",
    "    out =  llama.lm_head.output.save()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Live-streaming remote chat\n",
    "\n",
    "Now that we can access data within the tracing context on our local computer, we can apply non-whitelisted functions, such as the model's tokenizer, within our tracing context.\n",
    "\n",
    "Let's build a decoding function that will decode tokens into words and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnsight.trace\n",
    "def my_decoding_function(tokens, model, max_length=80, state=None):\n",
    "    # Initialize state if not provided\n",
    "    if state is None:\n",
    "        state = {'current_line': '', 'current_line_length': 0}\n",
    "\n",
    "    token = tokens[-1] # only use last token\n",
    "\n",
    "    # Decode the token\n",
    "    decoded_token = llama.tokenizer.decode(token).encode(\"unicode_escape\").decode()\n",
    "\n",
    "    if (decoded_token == '\\\\n') or (decoded_token == '\\n'):  # Handle explicit newline tokens\n",
    "        # Print the current line and reset state\n",
    "        print('',flush=True)\n",
    "        state['current_line'] = ''\n",
    "        state['current_line_length'] = 0\n",
    "    else:\n",
    "        # Check if adding the token would exceed the max length\n",
    "        if state['current_line_length'] + len(decoded_token) > max_length:\n",
    "            print('',flush=True)\n",
    "            state['current_line'] = decoded_token  # Start a new line with the current token\n",
    "            state['current_line_length'] = len(decoded_token)\n",
    "            print(state['current_line'], flush=True, end=\"\")  # Print the current line\n",
    "        else:\n",
    "            # Add a space if the line isn't empty and append the token\n",
    "            if state['current_line']:\n",
    "                state['current_line'] += decoded_token\n",
    "            else:\n",
    "                state['current_line'] = decoded_token\n",
    "            state['current_line_length'] += len(decoded_token)\n",
    "            print(state['current_line'], flush=True, end=\"\")  # Print the current line\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can decode and print our model outputs throughout token generation by accessing our decoding function through `nnsight.local()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  The Eiffel Tower is in the city of \n",
      " Paris, France. It is a very famous landmark. It is built in 1889. the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading result: 100%|██████████| 258k/258k [00:00<00:00, 1.01MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "nnsight.CONFIG.APP.REMOTE_LOGGING = False\n",
    "\n",
    "prompt = \"A press release is an official statement delivered to members of the news media for the purpose of\"\n",
    "prompt = \"The Eiffel Tower is in the city of\"\n",
    "\n",
    "print(\"Prompt: \",prompt,'\\n', end =\"\")\n",
    "\n",
    "# Initialize the state for decoding\n",
    "state = {'current_line': '', 'current_line_length': 0}\n",
    "\n",
    "with llama.generate(prompt, remote=True, max_new_tokens = 20) as generator:\n",
    "    # Call .all() to apply to each new token\n",
    "    llama.all()\n",
    "\n",
    "    all_tokens = nnsight.list().save()\n",
    "\n",
    "    # Access model output\n",
    "    out = llama.lm_head.output.save()\n",
    "\n",
    "    # Apply softmax to obtain probabilities and save the result\n",
    "    probs = torch.nn.functional.softmax(out, dim=-1)\n",
    "    max_probs = torch.max(probs, dim=-1)\n",
    "    tokens = max_probs.indices.cpu().tolist()\n",
    "    all_tokens.append(tokens[0]).save()\n",
    "\n",
    "    with nnsight.local():\n",
    "        state = my_decoding_function(tokens[0], llama, max_length=12, state=state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsight_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
