{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Scfr942GwO4"
   },
   "source": [
    "# Walkthrough\n",
    "\n",
    "## The API for a transparent science on black-box AI\n",
    "\n",
    "In this era of large-scale deep learning, the most interesting AI models are\n",
    "massive black boxes that are hard to run. Ordinary commercial inference service\n",
    "APIs let us interact with huge models, but they do not let us access model\n",
    "internals.\n",
    "\n",
    "The `nnsight` library is different: it provides full access to all neural\n",
    "network internals. When using `nnsight` together with a remote service like the\n",
    "[National Deep Inference Fabric](https://www.ndif.us)\n",
    "(NDIF), it is possible to run complex experiments on huge open models easily\n",
    "with fully transparent access.\n",
    "\n",
    "Through NDIF and NNsight, our team wants to enable entire labs and independent researchers alike, as we\n",
    "believe a large, passionate, and collaborative community will produce the next\n",
    "big insights on this profoundly important field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1OemD2VGyZx"
   },
   "source": [
    "# 1️⃣ First, let's start small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS9-avFkTITd"
   },
   "source": [
    "[Run an interactive version of this walkthrough in Google Colab](https://colab.research.google.com/drive/1qKY0fvNL-jtUKoD1gfItYfXl92PlxNoP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrwwLsJVTITe"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install NNsight:\n",
    "```\n",
    "pip install nnsight\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyLMmhrAKTNM"
   },
   "source": [
    "## Tracing Context\n",
    "\n",
    "To demonstrate the core functionality and syntax of nnsight, we'll define and\n",
    "use a tiny two layer neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgydw7i3HmIH"
   },
   "source": [
    "Our little model here is composed of two submodules – linear layers `layer1` and `layer2`. We specify the sizes of each of these modules and create\n",
    "some complementary example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pX2Wg8Ceo6N"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "input_size = 5\n",
    "hidden_dims = 10\n",
    "output_size = 2\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"layer1\", torch.nn.Linear(input_size, hidden_dims)),\n",
    "            (\"layer2\", torch.nn.Linear(hidden_dims, output_size)),\n",
    "        ]\n",
    "    )\n",
    ").requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIPa2h2pJIwl"
   },
   "source": [
    "The core object of the NNsight package is `NNsight`. This wraps around a given\n",
    "PyTorch model to enable investigation of its internal parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H9R_ynTJI5y"
   },
   "outputs": [],
   "source": [
    "from nnsight import NNsight\n",
    "\n",
    "tiny_model = NNsight(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NfISlQ_Ilvp"
   },
   "source": [
    "Printing a PyTorch model shows a named hierarchy of modules which is very useful\n",
    "when accessing sub-components directly. NNsight reflect the same hierarchy and can be similarly printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1754938975508,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "bYtnbJHvlGZV",
    "outputId": "92d5d455-63c5-45fd-c207-de170892bd49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tiny_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djC5kyJWLuUH"
   },
   "source": [
    "Before we actually get to using the model we just created, let's talk about\n",
    "Python contexts.\n",
    "\n",
    "Python contexts define a scope using the `with` statement and are often used to\n",
    "create some object, or initiate some logic, that you later want to destroy or\n",
    "conclude.\n",
    "\n",
    "The most common application is opening files as in the following example:\n",
    "\n",
    "```python\n",
    "with open('myfile.txt', 'r') as file:\n",
    "  text = file.read()\n",
    "```\n",
    "\n",
    "Python uses the `with` keyword to enter a context-like object. This object\n",
    "defines logic to be run at the start of the `with` block, as well as logic to be\n",
    "run when exiting. When using `with` for a file, entering the context opens the\n",
    "file and exiting the context closes it. Being within the context means we can\n",
    "read from the file.\n",
    "\n",
    "Simple enough! Now we can discuss how `nnsight` uses\n",
    "contexts to enable intuitive access into the internals of a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNvuCOeyojcA"
   },
   "source": [
    "The main tool with `nnsight` is a context for tracing.\n",
    "\n",
    "We enter the tracing context by calling `model.trace(<input>)` on an `NNsight`\n",
    "model, which defines how we want to run the model. Inside the context, we will\n",
    "be able to customize how the neural network runs. The model is actually run upon\n",
    "exiting the tracing context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEXQ4auPSL-m"
   },
   "outputs": [],
   "source": [
    "# random input\n",
    "input = torch.rand((1, input_size))\n",
    "\n",
    "with tiny_model.trace(input) as tracer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZQsHinjqicJ"
   },
   "source": [
    "But where's the output? To get that, we'll have to learn how to request it from\n",
    "within the tracing context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMfBpYzDPMoB"
   },
   "source": [
    "## Getting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5_aFwFRv0ax"
   },
   "source": [
    "Earlier, we wrapped our little neural net with the `NNsight` class. This\n",
    "added a couple properties to each module in the model (including the root model\n",
    "itself). The two most important ones are `.input` and `.output`.\n",
    "\n",
    "```python\n",
    "model.input\n",
    "model.output\n",
    "```\n",
    "\n",
    "The names are self explanatory. They correspond to the inputs and outputs of\n",
    "their respective modules during a forward pass of the model. We can use these\n",
    "attributes inside the `with` block.\n",
    "\n",
    "However, it is important to understand that the model is not executed until the\n",
    "end of the tracing context. How can we access inputs and outputs before the\n",
    "model is run? The trick is deferred execution.\n",
    "\n",
    "`.input` and `.output` are Proxies for the eventual inputs and outputs of a\n",
    "module. In other words, when we access `model.output` what we are\n",
    "communicating to `nnsight` is, \"When you compute the output of `model`, please\n",
    "grab it for me and put the value into its corresponding Proxy object. Let's try it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1754938975517,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "cYe8-r9ptGaG",
    "outputId": "efa55f8b-75bf-42f6-e359-09e9d589d0cc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4206295698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiny_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input) as tracer:\n",
    "\n",
    "    output = tiny_model.output\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CGa5zSI6kkI"
   },
   "source": [
    "Oh no an error! \"name `output` is not defined.\"\n",
    "\n",
    "Why doesn't our `output` variable exist?\n",
    "\n",
    "Proxy objects will only have their value at the end of a context if we call\n",
    "`.save()` on them. This helps to reduce memory costs. Adding `.save()` fixes the\n",
    "error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1754938988094,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "y_bXRd5dvsBu",
    "outputId": "a625a05c-d80f-433d-ceb0-e81cc300166c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2872, -0.0245]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input) as tracer:\n",
    "\n",
    "    output = tiny_model.output.save()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5C0UZCwvrYn"
   },
   "source": [
    "Success! We now have the model output. We just completed out first\n",
    "intervention using `nnsight`.\n",
    "\n",
    "Each time we access a module's input or output, we create an _intervention_ in\n",
    "the neural network's forward pass. Collectively these requests form the\n",
    "_intervention graph_. We call the process of executing it alongside the model's\n",
    "normal computation graph, _interleaving_.\n",
    "\n",
    "<details>\n",
    "<summary>On Model output</summary>\n",
    "\n",
    "---\n",
    "\n",
    "If we don't need to access anything other than the model's final output (i.e., the model's predicted next token), we can\n",
    "call the tracing context with `trace=False` and not use it as a context. This could be useful for simple inference using NNsight.\n",
    "\n",
    "```python\n",
    "  output = model.trace(<inputs>, trace=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n",
    "\n",
    "Just like we saved the output of the model as a whole, we can save the output of\n",
    "any of its submodules. We use normal Python attribute syntax. We can discover\n",
    "how to access them by name by printing out the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1754938990601,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "0WcVUSP-0CJi",
    "outputId": "9111a768-ba71-4ba4-c39d-7a451c8723be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tiny_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXn9XRQqkkw1"
   },
   "source": [
    "Let's access the output of the first layer (which we've named `layer1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1754938991485,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "akWr-cNqy-9O",
    "outputId": "f6f7d0fa-2016-4e92-f71b-6a34bfbbd4c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input) as tracer:\n",
    "\n",
    "    l1_output = tiny_model.layer1.output.save()\n",
    "\n",
    "print(l1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85A-aP_03ht6"
   },
   "source": [
    "Let's do the same for the input of `layer2`.\n",
    "\n",
    "Because we aren't accessing the `tracer` object within these tracing contexts, we can also drop `as tracer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1754938992954,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "3EHEN38N3nXR",
    "outputId": "5e026e93-2bb5-4c33-9ae5-104176ea96e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "\n",
    "    l2_input = tiny_model.layer2.input.save()\n",
    "\n",
    "print(l2_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk-U8zi33Gi-"
   },
   "source": [
    "<details>\n",
    "  <summary>On module inputs</summary>\n",
    "\n",
    "---\n",
    "\n",
    "Notice how the value for `l2_input` is just a single tensor. By default, the `.input` attribute of a module will return the **first** tensor input to the module.\n",
    "\n",
    "We can also access the full input to a module by using the `.inputs` attribute, which will return the values in the form of:\n",
    "\n",
    "      tuple(tuple(args), dictionary(kwargs))\n",
    "\n",
    "Where the first index of the tuple is itself a tuple of all positional\n",
    "arguments, and the second index is a dictionary of the keyword arguments.\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5DE1ZJAkkw1"
   },
   "source": [
    "Until now we were saving the output of the model and its submodules within the `trace` context to then print it after exiting the context. We will continuing doing this in the rest of the tutorial since it's a good practice to save the computation results for later analysis.\n",
    "\n",
    "However, we can also log the outputs of the model and its submodules within the `trace` context using `print` statements. This is useful for debugging and understanding the model's behavior while saving memory.\n",
    "\n",
    "Let's see how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754938995321,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "oZzS86tzkkw1",
    "outputId": "3d288026-5b94-4325-ed0f-861071248277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 - out:  tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "  print(\"Layer 1 - out: \", tiny_model.layer1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7Xo_PHyPr4p"
   },
   "source": [
    "## Functions, Methods, and Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehSofWbx5DSx"
   },
   "source": [
    "Now that we can access activations, we also want to do some post-processing on\n",
    "it. Let's find out which dimension of layer1's output has the highest value.\n",
    "\n",
    "We could do this by calling `torch.argmax(...)` after the tracing context or we\n",
    "can just leverage the fact that `nnsight` handles Pytorch functions and methods within\n",
    "the tracing context, by creating a Proxy request for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1754938996413,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "c5XCiSZn2p3k",
    "outputId": "fd2a584b-5dbb-48a3-cd6c-6fdefff3a43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "\n",
    "    # Note we don't need to call .save() on the output,\n",
    "    # as we're only using its value within the tracing context.\n",
    "    l1_output = tiny_model.layer1.output\n",
    "\n",
    "    # We do need to save the argmax tensor however,\n",
    "    # as we're using it outside the tracing context.\n",
    "    l1_amax = torch.argmax(l1_output, dim=1).save()\n",
    "\n",
    "print(l1_amax[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGPUJvWq_bOp"
   },
   "source": [
    "We can chain together multiple operations on the model's intermediate outputs. Just remember to save everything at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754938998158,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "FIcOYeEjFuln",
    "outputId": "8f2f0de0-2219-4703-e573-84324f5a2604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5118)\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "\n",
    "    value = (tiny_model.layer1.output.sum() + tiny_model.layer2.output.sum()).save()\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0vNOJtJ2oFR"
   },
   "source": [
    "The code block above is saying to `nnsight`, \"Run the model with\n",
    "the given `input`. When the output of `tiny_model.layer1` is computed, take its sum. Then do\n",
    "the same for `tiny_model.layer2`. Now that both of those are computed, add them and make sure\n",
    "not to delete this value as I wish to use it outside of the tracing context.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJFCuZHtkkw2"
   },
   "source": [
    "We can apply any function we want during the trace context, even our own custom functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1754939000254,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "DF7MPuhckkw2",
    "outputId": "247467e2-75c4-42dd-a3c1-e8cb649857e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2491) tensor(0.2491)\n"
     ]
    }
   ],
   "source": [
    "# Take a tensor and return the sum of its elements\n",
    "def tensor_sum(tensor):\n",
    "    flat = tensor.flatten()\n",
    "    total = 0\n",
    "    for element in flat:\n",
    "        total += element.item()\n",
    "\n",
    "    return torch.tensor(total)\n",
    "\n",
    "with tiny_model.trace(input) as tracer:\n",
    "\n",
    "    # call on our custom function within the trace context\n",
    "    custom_sum = tensor_sum(tiny_model.layer1.output).save()\n",
    "    sum = tiny_model.layer1.output.sum()\n",
    "    sum.save()\n",
    "\n",
    "\n",
    "print(custom_sum, sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_5qH5gHPOT_"
   },
   "source": [
    "## Setting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgju-b_IOLlq"
   },
   "source": [
    "Getting and analyzing the activations from various points in a model can be\n",
    "really insightful, and a number of ML techniques do exactly that. However, often we not only want to view the computation of a model, but also to influence it.\n",
    "\n",
    "To demonstrate the effect of editing the flow of information through the model,\n",
    "let's set the first dimension of the first layer's output to 0. `NNsight` makes\n",
    "this really easy using the '=' operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1754939002735,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "-6y2wzJqOz3a",
    "outputId": "d73bb007-3edf-4c26-9855-0c3370985912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n",
      "After: tensor([[ 0.0000, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "\n",
    "    # Save the output before the edit to compare.\n",
    "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
    "    l1_output_before = tiny_model.layer1.output.clone().save()\n",
    "\n",
    "    # Access the 0th index of the hidden state dimension and set it to 0.\n",
    "    tiny_model.layer1.output[:, 0] = 0\n",
    "\n",
    "    # Save the output after to see our edit.\n",
    "    l1_output_after = tiny_model.layer1.output.save()\n",
    "\n",
    "print(\"Before:\", l1_output_before)\n",
    "print(\"After:\", l1_output_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZz9SMs3Y_iS"
   },
   "source": [
    "Seems our change was reflected. Now let's do the same for the last dimension:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "error",
     "timestamp": 1754939003419,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "qwlqvHFcxld2",
    "outputId": "7e40cff0-4206-4b2e-ea97-42d41921ed38"
   },
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipython-input-3404137504.py\", line 8, in <cell line: 0>\n    tiny_model.layer1.output[:, hidden_dims] = 0\n\nIndexError: index 10 is out of bounds for dimension 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3404137504.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtiny_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Save the output before the edit to compare.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Notice we apply .clone() before saving as the setting operation is in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml1_output_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiny_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nnsight/intervention/tracing/base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# Execute the traced code using the configured backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nnsight/intervention/backends/execution.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mwrap_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mGlobals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipython-input-3404137504.py\", line 8, in <cell line: 0>\n    tiny_model.layer1.output[:, hidden_dims] = 0\n\nIndexError: index 10 is out of bounds for dimension 1 with size 10"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "\n",
    "    # Save the output before the edit to compare.\n",
    "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
    "    l1_output_before = tiny_model.layer1.output.clone().save()\n",
    "\n",
    "    # Access the last index of the hidden state dimension and set it to 0.\n",
    "    tiny_model.layer1.output[:, hidden_dims] = 0\n",
    "\n",
    "    # Save the output after to see our edit.\n",
    "    l1_output_after = tiny_model.layer1.output.save()\n",
    "\n",
    "print(\"Before:\", l1_output_before)\n",
    "print(\"After:\", l1_output_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKaN1oi76djp"
   },
   "source": [
    "Oh no, we are getting an error! Ah of course, we needed to index at `hidden_dims - 1` not `hidden_dims`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo7oHC0yzI_p"
   },
   "source": [
    "The error messaging feature can be toggled using `nnsight.CONFIG.APP.DEBUG` which defaults to true.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Toggle Error Messaging\n",
    "</summary>\n",
    "\n",
    "Turn off debugging:\n",
    "```\n",
    "import nnsight\n",
    "\n",
    "nnsight.CONFIG.APP.DEBUG = False\n",
    "nnsight.CONFIG.save()\n",
    "```\n",
    "\n",
    "Turn on debugging:\n",
    "```\n",
    "import nnsight\n",
    "\n",
    "nnsight.CONFIG.APP.DEBUG = True\n",
    "nnsight.CONFIG.save()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCNR_Jkpxr8l"
   },
   "source": [
    "Now that we know more about NNsight's error messaging, let's try our setting operation again with the correct indexing and view the shape of the output\n",
    "before leaving the tracing context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1754939006092,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "Wf8ugJKB2mru",
    "outputId": "4aabb65b-3cff-4f99-e892-0ea69fe08684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output shape: torch.Size([1, 10])\n",
      "Before: tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n",
      "After: tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input):\n",
    "\n",
    "    # Save the output before the edit to compare.\n",
    "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
    "    l1_output_before = tiny_model.layer1.output.clone().save()\n",
    "\n",
    "    print(f\"Layer 1 output shape: {tiny_model.layer1.output.shape}\")\n",
    "\n",
    "    # Access the last index of the hidden state dimension and set it to 0.\n",
    "    tiny_model.layer1.output[:, hidden_dims - 1] = 0\n",
    "\n",
    "    # Save the output after to see our edit.\n",
    "    l1_output_after = tiny_model.layer1.output.save()\n",
    "\n",
    "print(\"Before:\", l1_output_before)\n",
    "print(\"After:\", l1_output_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s016CelFP8sx"
   },
   "source": [
    "## Gradients\n",
    "\n",
    "`NNsight` also lets us apply backpropagation and access gradients with respect to a\n",
    "loss. Like `.input` and `.output` on modules, `nnsight` exposes `.grad` on\n",
    "Proxies themselves (assuming they are proxies of tensors):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1754939008078,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "ZlAKiN44Xj_h",
    "outputId": "92abedc9-9b54-4e66-fc2f-b5b69f278653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output gradient: tensor([[ 5.0732e-01, -3.0065e-01, -4.2533e-01,  2.5249e-02,  1.6884e-01,\n",
      "         -1.1749e-02,  1.9957e-04,  9.8918e-02,  1.0680e-01,  7.1143e-02]])\n",
      "Layer 2 output gradient: tensor([[1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Now in NNsight 0.5\n",
    "with tiny_model.trace(input):\n",
    "  # 1) access l1 & l2 outputs so trace knows these are intermediate values we care about\n",
    "  l1_output = tiny_model.layer1.output\n",
    "  # 2) make sure gradient flows back to l1 (it will pass by l2)\n",
    "  l1_output.requires_grad = True\n",
    "  l2_output = tiny_model.layer2.output\n",
    "\n",
    "  # 3) access gradients within a backwards trace\n",
    "  with tiny_model.output.sum().backward():\n",
    "    # access .grad within backward context in REVERSE ORDER\n",
    "    layer2_output_grad = l2_output.grad.save()\n",
    "    layer1_output_grad = l1_output.grad.save()\n",
    "\n",
    "print(\"Layer 1 output gradient:\", layer1_output_grad)\n",
    "print(\"Layer 2 output gradient:\", layer2_output_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8q3ac9Zaq7U"
   },
   "source": [
    "Some important things to look for when tracing gradients:\n",
    "1. Register your intermediate values in advance\n",
    "<br>If you want the gradient of a layer's output, then first access that layer in the trace context before the `.backward()` trace call. For us, that looked like:\n",
    "```\n",
    "l1_output = tiny_model.layer1.output\n",
    "```\n",
    "2. Make sure the gradient has somewhere to flow\n",
    "<br>We set `l1_output.requires_grad` to `True` to make sure that the gradient flows to the earliest output we care about. Another option would be to do `tiny_model.input.requires_grad = True` at the beginning of the trace, but this is slightly less efficient, because we aren't collecting any gradients there.\n",
    "\n",
    "3. Call on modules in order within the trace\n",
    "<br>`nnsight` will ensure that your modules are called in the same order as the model's execution. This means we should do all of our operations on layer 1 before moving on to collecting any information from layer 2.\n",
    "\n",
    "4. Call on gradients in reverse order\n",
    "<br>Similarly, we want to follow the order of the backward pass, which starts at the final layer and works its way to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o0JaaYvWlHG"
   },
   "source": [
    "All of the features we learned previously, also apply to `.grad`. In other\n",
    "words, we can apply operations to and edit the gradients. Let's double the grad of `layer2`. Our intervention has downstream consequences - see how the gradient of `layer1` ends up doubled as well?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1754939010364,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "9bFoaJpOWlRb",
    "outputId": "996ffd72-f17c-4fa9-b5e4-a60a57e5f03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output gradient: tensor([[ 1.0146e+00, -6.0130e-01, -8.5066e-01,  5.0498e-02,  3.3768e-01,\n",
      "         -2.3498e-02,  3.9914e-04,  1.9784e-01,  2.1360e-01,  1.4229e-01]])\n",
      "Layer 2 output gradient: tensor([[2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Now in NNsight 0.5\n",
    "with tiny_model.trace(input):\n",
    "  # 1) access l1 & l2 outputs so trace knows these are intermediate values we care about\n",
    "  l1_output = tiny_model.layer1.output\n",
    "  # 2) make sure gradient flows back to l1 (it will pass by l2)\n",
    "  l1_output.requires_grad = True\n",
    "  l2_output = tiny_model.layer2.output\n",
    "\n",
    "  # 3) access gradients within a backwards trace\n",
    "  with tiny_model.output.sum().backward():\n",
    "    # access .grad within backward context in REVERSE ORDER\n",
    "    l2_output.grad = l2_output.grad * 2\n",
    "    layer2_output_grad = l2_output.grad.save()\n",
    "    layer1_output_grad = l1_output.grad.save()\n",
    "\n",
    "print(\"Layer 1 output gradient:\", layer1_output_grad)\n",
    "print(\"Layer 2 output gradient:\", layer2_output_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ6dAgzQkkw6"
   },
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZLrnIxAkkw6"
   },
   "source": [
    "If we are only interested in a model's intermediate computations, we can halt a forward pass run at any module level, reducing runtime and conserving compute resources. One examples where this could be particularly useful would if we are working with SAEs - we can train an SAE on one layer and then stop the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1754939016144,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "MwAeiZhGkkw6",
    "outputId": "bb2c8b30-890a-4bad-fb27-d37245926de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 - Output:  tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,\n",
      "         -0.0298,  0.0354]])\n"
     ]
    }
   ],
   "source": [
    "with tiny_model.trace(input) as tracer:\n",
    "   l1_out = tiny_model.layer1.output.save()\n",
    "   tracer.stop()\n",
    "\n",
    "# get the output of the first layer and stop tracing\n",
    "print(\"L1 - Output: \", l1_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvQ1nZgYQDG3"
   },
   "source": [
    "# 2️⃣ Bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miQgUY4NAmDQ"
   },
   "source": [
    "Now that we have the basics of `nnsight` under our belt, we can scale our model\n",
    "up and combine the techniques we've learned into more interesting experiments.\n",
    "\n",
    "The `NNsight` class is very bare bones. It wraps a pre-defined model and does no\n",
    "pre-processing on the inputs we enter. It's designed to be extended with more\n",
    "complex and powerful types of models, and we're excited to see what can be done\n",
    "to leverage its features!\n",
    "\n",
    "However, if you'd like to load a Language Model from HuggingFace with its tokenizer, the`LanguageModel` subclass greatly simplifies this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TJDblHiQpp1"
   },
   "source": [
    "## LanguageModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l9mZOY5HFH2"
   },
   "source": [
    "`LanguageModel` is a subclass of `NNsight`. While we could define and create a\n",
    "model to pass in directly, `LanguageModel` includes special support for\n",
    "Huggingface language models, including automatically loading models from a\n",
    "Huggingface ID, and loading the model together with the appropriate tokenizer.\n",
    "\n",
    "Here is how we can use `LanguageModel` to load `GPT-2`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1754939019271,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "1OD2z7d3HQJU",
    "outputId": "0739e038-3d2e-4ec3-b492-d936ede0e25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "llm = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFRa-NgoTIUK"
   },
   "source": [
    "When we initialize `LanguageModel`, we aren't yet loading the parameters of the\n",
    "model into memory. We are actually loading a 'meta' version of the model which\n",
    "doesn't take up any memory, but still allows us to view and trace actions on it.\n",
    "After exiting the first tracing context, the model is then fully loaded into\n",
    "memory. To load into memory on initialization, you can pass `dispatch=True` into\n",
    "`LanguageModel` like\n",
    "`LanguageModel('openai-community/gpt2', device_map=\"auto\", dispatch=True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw2VaAr_ezkj"
   },
   "source": [
    "<details>\n",
    "<summary>On Model Initialization</summary>\n",
    "\n",
    "---\n",
    "\n",
    "A few important things to note:\n",
    "\n",
    "Keyword arguments passed to the initialization of `LanguageModel` is forwarded\n",
    "to HuggingFace specific loading logic. In this case, `device_map` specifies\n",
    "which devices to use and its value `auto` indicates to evenly distribute it to\n",
    "all available GPUs (and CPU if no GPUs available). Other arguments can be found\n",
    "here:\n",
    "https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n",
    "\n",
    "Let's now apply some of the features that we used on the small model to `GPT-2`. Unlike `NNsight`, `LanguageModel` does define logic to pre-process\n",
    "inputs upon entering the tracing context. This makes interacting with the model\n",
    "simpler (i.e., you can send prompts to the model without having to directly access the tokenizer).\n",
    "\n",
    "In the following example, we ablate the value coming from the last layer's MLP\n",
    "module and decode the logits to see what token the model predicts without\n",
    "influence from that particular module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1754939022254,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "mLSapaMCgLNU",
    "outputId": "77a1c6f7-6bbe-41fc-9d32-7488d9968cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]],\n",
      "       device='cuda:0')\n",
      "Prediction:  London\n"
     ]
    }
   ],
   "source": [
    "with llm.trace(\"The Eiffel Tower is in the city of\"):\n",
    "    # Access the last layer using h[-1] as it's a ModuleList\n",
    "    # Access the first index of .output as that's where the hidden states are.\n",
    "    llm.transformer.h[-1].mlp.output[0][:] = 0\n",
    "\n",
    "    # # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.\n",
    "    token_ids = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"\\nToken IDs:\", token_ids)\n",
    "\n",
    "# Apply the tokenizer to decode the ids into words after the tracing context.\n",
    "print(\"Prediction:\", llm.tokenizer.decode(token_ids[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2-HiCQhkv-B"
   },
   "source": [
    "We just ran a little intervention on a much more complex model with many more\n",
    "parameters! However, we're missing an important piece of information: what the\n",
    "prediction would have looked like without our ablation.\n",
    "\n",
    "We could just run two tracing contexts and compare the outputs. However, this would require two forward passes through the model. `NNsight` can do\n",
    "better than that with batching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLjHnRyRPXjp"
   },
   "source": [
    "## Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gc-Zr6NmEOn"
   },
   "source": [
    "Batching is a way to process multiple inputs in one forward pass. To better understand how batching works, we're going to bring back the `Tracer` object that we dropped before.\n",
    "\n",
    "When we call `.trace(...)`, it's actually creating two different contexts behind the scenes. The first one is the tracing context that we've discussed previously, and the second one is the invoker context. The invoker context defines the values of the `.input` and `.output` Proxies.\n",
    "\n",
    "If we call `.trace(...)` with some input, the input is passed on to the invoker. As there is only one input, only one invoker context is created.\n",
    "\n",
    "If we call `.trace()` without an input, then we can call `tracer.invoke(input1)` to manually create the invoker context with an input, `input1`. We can also repeatedly call `tracer.invoke(...)` to create the invoker context for additional inputs. Every subsequent time we call\n",
    "`.invoke(...)`, interventions within its context will only refer to the input in that particular invoke statement.\n",
    "\n",
    "When exiting the tracing context, the inputs from all of the invokers will be batched together, and they will be executed in one forward pass! To test this out, let's do the same ablation experiment, but also add a 'control' output for comparison:\n",
    "\n",
    "<details>\n",
    "<summary>More on the invoker context</summary>\n",
    "\n",
    "---\n",
    "\n",
    "Note that when injecting data to only the relevant invoker interventions, `nnsight` tries, but can't guarantee, to narrow the data into the right\n",
    "batch indices. Thus, there are cases\n",
    "where all invokes will get all of the data. Specifically, if the input or output data is stored\n",
    "as an object that is not an arbitrary collection of tensors, it will be broadcasted to all invokes.\n",
    "\n",
    "Just like `.trace(...)` created a `Tracer` object, `.invoke(...)` creates an `Invoker` object. For `LanguageModel` models, the `Invoker` prepares the input by running a tokenizer on it.\n",
    "`Invoker` stores pre-processed inputs at `invoker.inputs`, which can be accessed to see information about our inputs.\n",
    "In a case where we pass a single input to `.trace(...)` directly, we can still access the invoker\n",
    "object at `tracer.invoker` without having to call `tracer.invoke(...)`.\n",
    "\n",
    "Keyword arguments given to `.invoke(..)` make their way to the input pre-processing.  \n",
    "`LanguageModel` has keyword arguments `max_length` and `truncation` used for tokenization which can be\n",
    "passed to the invoker. If we want to pass keyword arguments to the invoker for a single-input `.trace(...)`, we can pass `invoker_args` as a dictionary of invoker keyword arguments.\n",
    "\n",
    "Here is an example to demonstrate everything we've described:\n",
    "\n",
    "**This snippet**\n",
    "\n",
    "```\n",
    "with llm.trace(\"hello\", invoker_args={\"max_length\":10}) as tracer:\n",
    "  invoker = tracer.invoker\n",
    "\n",
    "```\n",
    "  **does the same as**\n",
    "  \n",
    "\n",
    "```\n",
    "with llm.trace() as tracer:\n",
    "  with tracer.invoke(\"hello\", max_length=10) as invoker:\n",
    "    invoker = invoker\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1754939025504,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "Kdcq4oCNmEua",
    "outputId": "ddfe328d-aa75-41ec-9671-ed68a2a85f68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token IDs: tensor([[ 198,   12,  417, 8765,  318,  257,  262, 3504, 7372, 6342]],\n",
      "       device='cuda:0')\n",
      "Modified token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]],\n",
      "       device='cuda:0')\n",
      "Original prediction:  Paris\n",
      "Modified prediction:  London\n"
     ]
    }
   ],
   "source": [
    "with llm.trace() as tracer:\n",
    "\n",
    "    with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
    "\n",
    "        # Ablate the last MLP for only this batch.\n",
    "        llm.transformer.h[-1].mlp.output[0][:] = 0\n",
    "\n",
    "        # Get the output for only the intervened on batch.\n",
    "        token_ids_intervention = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "    with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
    "\n",
    "        # Get the output for only the original batch.\n",
    "        token_ids_original = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "\n",
    "print(\"Original token IDs:\", token_ids_original)\n",
    "print(\"Modified token IDs:\", token_ids_intervention)\n",
    "\n",
    "print(\"Original prediction:\", llm.tokenizer.decode(token_ids_original[0][-1]))\n",
    "print(\"Modified prediction:\", llm.tokenizer.decode(token_ids_intervention[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BsybgsK2Bbr"
   },
   "source": [
    "Based on our control results, our ablation did end up affecting what the model predicted. That's pretty neat!\n",
    "\n",
    "Another cool thing with multiple invokes is that Proxies can interact between them.\n",
    "\n",
    "Here, we transfer the token embeddings from a real prompt into another placeholder prompt. Therefore the latter prompt produces the output of the former prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1754939028368,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "syKTI_KhpvCY",
    "outputId": "a72e2ca7-757e-406c-f40f-11382c48f16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original prediction shape torch.Size([])\n",
      "Original prediction:  _\n",
      "modified prediction shape torch.Size([])\n",
      "Modified prediction:  Paris\n"
     ]
    }
   ],
   "source": [
    "with llm.trace() as tracer:\n",
    "    barrier = tracer.barrier(2)\n",
    "    with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
    "        embeddings = llm.transformer.wte.output\n",
    "        # call barrier\n",
    "        barrier()\n",
    "\n",
    "    with tracer.invoke(\"_ _ _ _ _ _ _ _ _ _\"):\n",
    "        # tell model to wait for the output from the previous invoke with barrier\n",
    "        barrier()\n",
    "        llm.transformer.wte.output = embeddings\n",
    "        token_ids_intervention = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "    with tracer.invoke(\"_ _ _ _ _ _ _ _ _ _\"):\n",
    "        token_ids_original = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"original prediction shape\", token_ids_original[0][-1].shape)\n",
    "print(\"Original prediction:\", llm.tokenizer.decode(token_ids_original[0][-1]))\n",
    "\n",
    "print(\"modified prediction shape\", token_ids_intervention[0][-1].shape)\n",
    "print(\"Modified prediction:\", llm.tokenizer.decode(token_ids_intervention[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlichUS8TIUN"
   },
   "source": [
    "For larger batch sizes, you can also iteratate across multiple invoke contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bXvUGGqbwv1"
   },
   "source": [
    "## Multiple Token Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some HuggingFace models define methods to generate multiple outputs at a time. `LanguageModel` wraps that functionality to provide the same tracing features by using `.generate(...)` instead of `.trace(...)`. This calls the underlying model's `.generate` method. It passes the output through a `.generator` module that we've added onto the model, allowing us to get the generate output at `.generator.output`. You can control the number of new tokens generated by setting `max_new_tokens = N` within your call to `.generate()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvWA-CWqQtah"
   },
   "source": [
    "### Intervening on generated token iterations with `.all()` and `.iter[]`\n",
    "\n",
    "During model generation, the underlying model is called more than once, so the modules of said model produce more than one output. Which iteration should a given `module.output` refer to? That's where `.all` and `.iter` come in!\n",
    "\n",
    "If you want to access and intervene on module outputs across all iterations, you should use `.all()`. Simply create a `with tracer.all():` context and include your intervention code within the indented block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .all():\n",
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "layers = llm.transformer.h\n",
    "n_new_tokens = 50\n",
    "with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    hidden_states = list().save() # Initialize & .save() list\n",
    "\n",
    "    # Call .all() to apply intervention to each new token\n",
    "    with tracer.all():\n",
    "\n",
    "        # Apply intervention - set first layer output to zero\n",
    "        layers[0].output[0][:] = 0\n",
    "\n",
    "        # Append desired hidden state post-intervention\n",
    "        hidden_states.append(layers[-1].output) # no need to call .save\n",
    "\n",
    "print(\"Hidden state length: \",len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you want to intervene specific iterations of generation, you can use the `with tracer.iter[<slice>]:` context. Here, let's try intervening only on the generation iterations 2-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yy5Z9NE1GkaN"
   },
   "outputs": [],
   "source": [
    "# using .all():\n",
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "layers = llm.transformer.h\n",
    "n_new_tokens = 50\n",
    "with llm.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    hidden_states = list().save() # Initialize & .save() list\n",
    "\n",
    "    # Call .all() to apply intervention to each new token\n",
    "    with tracer.iter[2:5]:\n",
    "\n",
    "        # Apply intervention - set first layer output to zero\n",
    "        layers[0].output[0][:] = 0\n",
    "\n",
    "        # Append desired hidden state post-intervention\n",
    "        hidden_states.append(layers[-1].output) # no need to call .save\n",
    "\n",
    "print(\"Hidden state length: \",len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty-luCwCkkw7"
   },
   "source": [
    "## Model Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgrZVQLEkkw8"
   },
   "source": [
    "NNsight's model editing feature allows you to create persistently modified versions of a model with a use of `.edit()`. Unlike interventions in a tracing context, which are temporary, the **Editor** context enables you to make lasting changes to a model instance.\n",
    "\n",
    "This feature is useful for:\n",
    "* Creating modified model variants without altering the original\n",
    "* Applying changes that persist across multiple forward passes\n",
    "* Comparing interventions between original and edited models\n",
    "\n",
    "Let's explore how to use the **Editor** context to make a simple persistent change to a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1754939484713,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "-_oAprpwkkw8",
    "outputId": "77653653-580f-4cb1-8048-fb682ff08455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Prediction:   Rome\n",
      "Modified Prediction:   Paris\n"
     ]
    }
   ],
   "source": [
    "# we take the hidden states with the expected output \"Paris\"\n",
    "with llm.trace(\"The Eiffel Tower is located in the city of\") as tracer:\n",
    "    hs11 = llm.transformer.h[11].output[0][:, -1, :].save()\n",
    "\n",
    "# the edited model will now always predict \"Paris\" as the next token\n",
    "with llm.edit() as llm_edited:\n",
    "    llm.transformer.h[11].output[0][:, -1, :] = hs11\n",
    "\n",
    "# we demonstrate this by comparing the output of an unmodified model...\n",
    "with llm.trace(\"Vatican is located in the city of\") as tracer:\n",
    "    original_tokens = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "# ...with the output of the edited model\n",
    "with llm_edited.trace(\"Vatican is located in the city of\") as tracer:\n",
    "    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "\n",
    "print(\"\\nOriginal Prediction: \", llm.tokenizer.decode(original_tokens[0][-1]))\n",
    "print(\"Modified Prediction: \", llm.tokenizer.decode(modified_tokens[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8J6cVw0kkw8"
   },
   "source": [
    "Edits defined within an **Editor** context create a new, modified version of the model by default, preserving the original. This allows for safe experimentation with model changes. If you wish to modify the original model directly, you can set `inplace=True` when calling `.edit()`.\n",
    "\n",
    "Use this option cautiously, as in-place edits alter the base model for all the consequent model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1754939486973,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "b1atxBp8kkw8",
    "outputId": "87d210d4-0746-4b4f-ed33-930201ee2593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified In-place:   Paris\n"
     ]
    }
   ],
   "source": [
    "# we use the hidden state we saved above (hs11)\n",
    "with llm.edit(inplace=True) as llm_edited:\n",
    "    llm.transformer.h[11].output[0][:, -1, :] = hs11\n",
    "\n",
    "# we demonstrate this by comparing the output of an unmodified model...\n",
    "with llm.trace(\"Vatican is located in the city of\") as tracer:\n",
    "    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"Modified In-place: \", llm.tokenizer.decode(modified_tokens[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVeZO96jkkw8"
   },
   "source": [
    "If you've made in-place edits to your model and need to revert these changes, you can apply `.clear_edits()`. This method removes all edits applied to the model, effectively restoring it to its original state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1754939925024,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "_SHK4JzPkkw8",
    "outputId": "e5e4619f-9b97-481a-f923-e9845cd24901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edits cleared:   Rome\n"
     ]
    }
   ],
   "source": [
    "llm.clear_edits()\n",
    "\n",
    "with llm.trace(\"Vatican is located in the city of\"):\n",
    "    modified_tokens = llm.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"Edits cleared: \", llm.tokenizer.decode(modified_tokens[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpkX-LwBQZHo"
   },
   "source": [
    "# 3️⃣ I thought you said huge models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCZR65VmILEb"
   },
   "source": [
    "`NNsight` is only one part of our project to democratize access to AI internals. The other half is the National Deep Inference Fabric, or `NDIF`. `NDIF` hosts large models for shared access using `NNsight`, so you don't have to worry about any of the headaches of hosting large models yourself!\n",
    "\n",
    "The interaction between `NDIF` and `NNsight` is fairly straightforward. The\n",
    "**intervention graph** we create via the tracing context can be encoded into a\n",
    "custom json format and sent via an http request to the `NDIF` servers. `NDIF`\n",
    "then decodes the **intervention graph** and **interleaves** it alongside the\n",
    "specified model.\n",
    "\n",
    "To see which models are currently being hosted, check out the following status\n",
    "page: https://nnsight.net/status/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65Ks1LUvQaER"
   },
   "source": [
    "## Remote execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa-ZuVOFKS5k"
   },
   "source": [
    "In its current state, `NDIF` requires you to receive an API key. Therefore, to\n",
    "run the rest of this walkthrough, you need one of your own. To get one, simply\n",
    "register at https://login.ndif.us.\n",
    "\n",
    "With a valid API key, you then can configure `nnsight` as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax1NWoS9MJeZ"
   },
   "outputs": [],
   "source": [
    "from nnsight import CONFIG\n",
    "\n",
    "CONFIG.set_default_api_key(\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvXLOh65MXmF"
   },
   "source": [
    "If you're running in a local IDE, this only needs to be run once as it will save the API key as the default in a\n",
    ".config file along with your `nnsight` installation. You can also add your API key to Google Colab secrets.\n",
    "\n",
    "To amp things up a few levels, let's demonstrate using `nnsight`'s tracing\n",
    "context with `Llama-3.1-8b`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1N04sJPZnJt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Llama 3.1 8b is a gated model, so you need to apply for access on HuggingFace and include your token.\n",
    "os.environ['HF_TOKEN'] = \"YOUR_HUGGING_FACE_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "924a2c4032f64a3986ba784507f0e5ae",
      "b01fd466f1d049a68e77e1e21f279ce1",
      "667f40cef28a4e9799448824f04cd265",
      "d044fd5f7c4a4a158c48ca078fac1515",
      "41a3096d744b41859e00532cb9bd7dba",
      "14b22988f49c4439bc84a1a82891d0ed",
      "d0a4ba5c70704b5db507127585891437",
      "e6aeff134ee9414380b2f7ab0f9e65a9",
      "2c91c0b798a74158835c90b17887922f",
      "40c0cc5ab1fb430db1f264ddd8227a26",
      "d5c9902cb5d446dc9bfdcacc5a4c4634",
      "0096f4ae483840edb65d6a4136687217",
      "2bc735bb135b4cdfb7961bbf6841dfa6",
      "08e2dbf8eacd44108798732514f09a5f",
      "5ffcdc21e0e9495a95c87fdebc5f23e8",
      "4a113ee29b2c45cc8326af1b195bd30d",
      "77faedbfdffe4580abce2e78d8999a5b",
      "2615ba6b427f4eaba156a458f6315fd0",
      "96c80768cb624a44bbdd17316f7b110d",
      "2ca007d6d2954d09bc335be6986cf731"
     ]
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1754939725970,
     "user": {
      "displayName": "Amir Zur",
      "userId": "04168999637926303482"
     },
     "user_tz": 420
    },
    "id": "SaSx22uJjsiJ",
    "outputId": "a1aa9e30-e6c2-4945-d979-d7f1b517d9c7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924a2c4032f64a3986ba784507f0e5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqVzjNoyNGc7"
   },
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "# We'll never actually load the parameters locally, so no need to specify a device_map.\n",
    "llama = LanguageModel(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# All we need to specify using NDIF vs executing locally is remote=True.\n",
    "with llama.trace(\"The Eiffel Tower is in the city of\", remote=True) as runner:\n",
    "\n",
    "    hidden_states = llama.model.layers[-1].output.save()\n",
    "\n",
    "    output = llama.output.save()\n",
    "\n",
    "print(hidden_states)\n",
    "\n",
    "print(output[\"logits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IceVpnZcZ9F0"
   },
   "source": [
    "It really is as simple as `remote=True`. All of the techniques we went through\n",
    "in earlier sections work just the same when running locally or remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfRDZox5kkw8"
   },
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33XLOvtukkw8"
   },
   "source": [
    "NDIF uses a queue to handle concurrent requests from multiple users. To optimize the execution of our experiments we can use the `session` context to efficiently package multiple interventions together as one single request to the server.\n",
    "\n",
    "This offers the following benefits:\n",
    "1.   All interventions within a session will be executed one after another without additional wait in the NDIF queue\n",
    "2.   All intermediate outputs for each intervention are stored on the server and can be accessed by other interventions in the same session without moving the data back and forth between NDIF and the local machine\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KA6GROiTR3gf"
   },
   "outputs": [],
   "source": [
    "with llama.session(remote=True) as session:\n",
    "\n",
    "  with llama.trace(\"The Eiffel Tower is in the city of\") as t1:\n",
    "    # capture the hidden state from layer 32 at the last token\n",
    "    hs_31 = llama.model.layers[31].output[0][:, -1, :] # no .save()\n",
    "    t1_tokens_out = llama.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "  with llama.trace(\"Buckingham Palace is in the city of\") as t2:\n",
    "    llama.model.layers[1].output[0][:, -1, :] = hs_31[:]\n",
    "    t2_tokens_out = llama.lm_head.output.argmax(dim=-1).save()\n",
    "\n",
    "print(\"\\nT1 - Original Prediction: \", llama.tokenizer.decode(t1_tokens_out[0][-1]))\n",
    "print(\"T2 - Modified Prediction: \", llama.tokenizer.decode(t2_tokens_out[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZfiTNGgu-HI"
   },
   "source": [
    "# Next Steps\n",
    "Check out [nnsight.net/tutorials](https://nnsight.net/tutorials) for more walkthroughs implementating classic interpretability techniques using `nnsight`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3zRm-7VRRov"
   },
   "source": [
    "## Getting Involved!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnCc9xgjvxEP"
   },
   "source": [
    "Note that both `nnsight` and `NDIF` are in active development, so changes may be made and errors may arise during use. If you’re interested in following updates to `nnsight`, contributing, giving feedback, or finding collaborators, please join the [NDIF discord](https://discord.gg/6uFJmCSwW7). We’d love to hear about your work using nnsight!\n",
    "\n",
    "You can also follow us on [LinkedIn](https://www.linkedin.com/company/national-deep-inference-fabric/), Bluesky: [@ndif-team.bsky.social](https://bsky.app/profile/ndif-team.bsky.social), and X: [@ndif_team](https://x.com/ndif_team).\n",
    "\n",
    "💟\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rpkX-LwBQZHo",
    "lfRDZox5kkw8"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0096f4ae483840edb65d6a4136687217": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e2dbf8eacd44108798732514f09a5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14b22988f49c4439bc84a1a82891d0ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a113ee29b2c45cc8326af1b195bd30d",
      "placeholder": "​",
      "style": "IPY_MODEL_77faedbfdffe4580abce2e78d8999a5b",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "2615ba6b427f4eaba156a458f6315fd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96c80768cb624a44bbdd17316f7b110d",
      "placeholder": "​",
      "style": "IPY_MODEL_2ca007d6d2954d09bc335be6986cf731",
      "value": "Connecting..."
     }
    },
    "2bc735bb135b4cdfb7961bbf6841dfa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c91c0b798a74158835c90b17887922f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ca007d6d2954d09bc335be6986cf731": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40c0cc5ab1fb430db1f264ddd8227a26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41a3096d744b41859e00532cb9bd7dba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_08e2dbf8eacd44108798732514f09a5f",
      "style": "IPY_MODEL_5ffcdc21e0e9495a95c87fdebc5f23e8",
      "tooltip": ""
     }
    },
    "4a113ee29b2c45cc8326af1b195bd30d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ffcdc21e0e9495a95c87fdebc5f23e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "667f40cef28a4e9799448824f04cd265": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_40c0cc5ab1fb430db1f264ddd8227a26",
      "placeholder": "​",
      "style": "IPY_MODEL_d5c9902cb5d446dc9bfdcacc5a4c4634",
      "value": ""
     }
    },
    "77faedbfdffe4580abce2e78d8999a5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "924a2c4032f64a3986ba784507f0e5ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_d0a4ba5c70704b5db507127585891437"
     }
    },
    "96c80768cb624a44bbdd17316f7b110d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b01fd466f1d049a68e77e1e21f279ce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6aeff134ee9414380b2f7ab0f9e65a9",
      "placeholder": "​",
      "style": "IPY_MODEL_2c91c0b798a74158835c90b17887922f",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "d044fd5f7c4a4a158c48ca078fac1515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_0096f4ae483840edb65d6a4136687217",
      "style": "IPY_MODEL_2bc735bb135b4cdfb7961bbf6841dfa6",
      "value": true
     }
    },
    "d0a4ba5c70704b5db507127585891437": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "d5c9902cb5d446dc9bfdcacc5a4c4634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6aeff134ee9414380b2f7ab0f9e65a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
