
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" data-theme="dark">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Walkthrough &#8212; nnsight</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "dark";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "dark";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/nbsphinx-code-cells.css?v=2aa19091" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../_static/documentation_options.js?v=187304be"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/tutorials/get_started/walkthrough';</script>
    <script src="../../../../_static/js/custom.js?v=f64c75aa"></script>
    <script src="../../../../_static/js/code.js?v=34343d0c"></script>
    <link rel="icon" href="../../../../_static/icon.ico"/>
    <link rel="author" title="About these documents" href="../../../../about/" />
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" />
    <link rel="next" title="Access LLMs with NDIF and NNsight" href="../start_remote_access/" />
    <link rel="prev" title="Main Tutorials" href="../../../../walkthroughs/" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
<link href="../../../../_static/css/custom.css?v=1758727866" rel="stylesheet" type="text/css" />
<link href="../../../../_static/css/home.css?v=1758727866" rel="stylesheet" type="text/css" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="dark">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../../">
  
  
  
  
  
    
    
    
    <img src="../../../../_static/nnsight_logo.svg" class="logo__image only-dark" alt="nnsight - Home"/>
    <img src="../../../../_static/nnsight_logo.svg" class="logo__image only-light pst-js-only" alt="nnsight - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../start/">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../documentation/">
    Documentation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../features/">
    Features
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../../../tutorials/">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../about/">
    About
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item"><script>
    fetch("https://ndif.dev/ping")
        .then((response) => {
            if (response.status == 200) {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#66800b', 'important');
                    });
                });
            }
            else {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#af3029', 'important');
                    });
                });
                var statusIcon = document.querySelector('.ndif .fa-circle-check');
                if (statusIcon) {
                    // not here
                    statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark'); 
                }
            }
        })
        .catch((response) => {
            Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                    spanElement.style.setProperty('color', '#af3029', 'important');
                });
            });
            var statusIcon = document.querySelector('.ndif .fa-circle-check');
            if (statusIcon) {
                statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark');
            }
        })
</script></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="/status" title="Status: Unknown" class="ndif" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-circle-check fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Status: Unknown</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ndif-team/nnsight" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.ndif.us/" title="Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://forms.gle/1Y6myaXYzSh3oHf56" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../start/">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../documentation/">
    Documentation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../features/">
    Features
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../../../tutorials/">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../../about/">
    About
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><script>
    fetch("https://ndif.dev/ping")
        .then((response) => {
            if (response.status == 200) {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#66800b', 'important');
                    });
                });
            }
            else {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#af3029', 'important');
                    });
                });
                var statusIcon = document.querySelector('.ndif .fa-circle-check');
                if (statusIcon) {
                    // not here
                    statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark'); 
                }
            }
        })
        .catch((response) => {
            Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                    spanElement.style.setProperty('color', '#af3029', 'important');
                });
            });
            var statusIcon = document.querySelector('.ndif .fa-circle-check');
            if (statusIcon) {
                statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark');
            }
        })
</script></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="/status" title="Status: Unknown" class="ndif" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-circle-check fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Status: Unknown</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ndif-team/nnsight" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.ndif.us/" title="Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://forms.gle/1Y6myaXYzSh3oHf56" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../../../walkthroughs/">Main Tutorials</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Walkthrough</a></li>




<li class="toctree-l2"><a class="reference internal" href="../start_remote_access/">Access LLMs with NDIF and NNsight</a></li>






<li class="toctree-l2"><a class="reference internal" href="../chat_templates/">Chat Templates</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../probing/logit_lens/">Logit Lens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probing/diffusion_lens/">Diffusion Lens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../steering/dict_learning/">Dictionary Learning</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../steering/LoRA_tutorial/">LoRA for Sentiment Analysis</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../causal_mediation_analysis/causal_models_intro/">Causal Models Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../causal_mediation_analysis/causal_mediation_analysis_i/">Causal Mediation Analysis I</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../causal_mediation_analysis/causal_mediation_analysis_ii/">Causal Mediation Analysis II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../causal_mediation_analysis/activation_patching/">Activation Patching</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../causal_mediation_analysis/attribution_patching/">Attribution Patching</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../causal_mediation_analysis/DAS/">DAS</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../applied_tutorials/">Mini Papers</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../mini-papers/csordas_llm_depth/">Do Language Models Use Their Depth Efficiently?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mini-papers/marks_geometry_of_truth/">The Geometry of Truth</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mini-papers/todd_function_vectors/">Function Vectors</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../../mini-papers/huang_demystifying_memorization/">Demystifying Verbatim Memorization in Large Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mini-papers/feucht_dual_route_induction/">The Dual-Route Model of Induction</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../tutorials/" class="nav-link">Tutorials</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../../walkthroughs/" class="nav-link">Main Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Walkthrough</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="Walkthrough">
<h1>Walkthrough<a class="headerlink" href="#Walkthrough" title="Link to this heading">#</a></h1>
<section id="The-API-for-a-transparent-science-on-black-box-AI">
<h2>The API for a transparent science on black-box AI<a class="headerlink" href="#The-API-for-a-transparent-science-on-black-box-AI" title="Link to this heading">#</a></h2>
<p>In this era of large-scale deep learning, the most interesting AI models are massive black boxes that are hard to run. Ordinary commercial inference service APIs let us interact with huge models, but they do not let us access model internals.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library is different: it provides full access to all neural network internals. When using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> together with a remote service like the <a class="reference external" href="https://www.ndif.us">National Deep Inference Fabric</a> (NDIF), it is possible to run complex experiments on huge open models easily with fully transparent access.</p>
<p>Through NDIF and NNsight, our team wants to enable entire labs and independent researchers alike, as we believe a large, passionate, and collaborative community will produce the next big insights on this profoundly important field.</p>
</section>
</section>
<section id="1️⃣-First,-let's-start-small">
<h1>1️⃣ First, let’s start small<a class="headerlink" href="#1️⃣-First,-let's-start-small" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/drive/1qKY0fvNL-jtUKoD1gfItYfXl92PlxNoP">Run an interactive version of this walkthrough in Google Colab</a></p>
<section id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Link to this heading">#</a></h2>
<p>Install NNsight:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pip install nnsight
</pre></div>
</div>
</section>
<section id="Tracing-Context">
<h2>Tracing Context<a class="headerlink" href="#Tracing-Context" title="Link to this heading">#</a></h2>
<p>To demonstrate the core functionality and syntax of nnsight, we’ll define and use a tiny two layer neural network.</p>
<p>Our little model here is composed of two submodules – linear layers <code class="docutils literal notranslate"><span class="pre">layer1</span></code> and <code class="docutils literal notranslate"><span class="pre">layer2</span></code>. We specify the sizes of each of these modules and create some complementary example input.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">hidden_dims</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><span class="s2">&quot;layer1&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">)),</span>
            <span class="p">(</span><span class="s2">&quot;layer2&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)),</span>
        <span class="p">]</span>
    <span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The core object of the NNsight package is <code class="docutils literal notranslate"><span class="pre">NNsight</span></code>. This wraps around a given PyTorch model to enable investigation of its internal parameters.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nnsight</span><span class="w"> </span><span class="kn">import</span> <span class="n">NNsight</span>

<span class="n">tiny_model</span> <span class="o">=</span> <span class="n">NNsight</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Printing a PyTorch model shows a named hierarchy of modules which is very useful when accessing sub-components directly. NNsight reflect the same hierarchy and can be similarly printed.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tiny_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Sequential(
  (layer1): Linear(in_features=5, out_features=10, bias=True)
  (layer2): Linear(in_features=10, out_features=2, bias=True)
)
</pre></div></div>
</div>
<p>Before we actually get to using the model we just created, let’s talk about Python contexts.</p>
<p>Python contexts define a scope using the <code class="docutils literal notranslate"><span class="pre">with</span></code> statement and are often used to create some object, or initiate some logic, that you later want to destroy or conclude.</p>
<p>The most common application is opening files as in the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;myfile.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
  <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
<p>Python uses the <code class="docutils literal notranslate"><span class="pre">with</span></code> keyword to enter a context-like object. This object defines logic to be run at the start of the <code class="docutils literal notranslate"><span class="pre">with</span></code> block, as well as logic to be run when exiting. When using <code class="docutils literal notranslate"><span class="pre">with</span></code> for a file, entering the context opens the file and exiting the context closes it. Being within the context means we can read from the file.</p>
<p>Simple enough! Now we can discuss how <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> uses contexts to enable intuitive access into the internals of a neural network.</p>
<p>The main tool with <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> is a context for tracing.</p>
<p>We enter the tracing context by calling <code class="docutils literal notranslate"><span class="pre">model.trace(&lt;input&gt;)</span></code> on an <code class="docutils literal notranslate"><span class="pre">NNsight</span></code> model, which defines how we want to run the model. Inside the context, we will be able to customize how the neural network runs. The model is actually run upon exiting the tracing context.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># random input</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>

<span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
<p>But where’s the output? To get that, we’ll have to learn how to request it from within the tracing context.</p>
</section>
<section id="Getting">
<h2>Getting<a class="headerlink" href="#Getting" title="Link to this heading">#</a></h2>
<p>Earlier, we wrapped our little neural net with the <code class="docutils literal notranslate"><span class="pre">NNsight</span></code> class. This added a couple properties to each module in the model (including the root model itself). The two most important ones are <code class="docutils literal notranslate"><span class="pre">.input</span></code> and <code class="docutils literal notranslate"><span class="pre">.output</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">input</span>
<span class="n">model</span><span class="o">.</span><span class="n">output</span>
</pre></div>
</div>
<p>The names are self explanatory. They correspond to the inputs and outputs of their respective modules during a forward pass of the model. We can use these attributes inside the <code class="docutils literal notranslate"><span class="pre">with</span></code> block.</p>
<p>However, it is important to understand that the model is not executed until the end of the tracing context. How can we access inputs and outputs before the model is run? The trick is deferred execution.</p>
<p><code class="docutils literal notranslate"><span class="pre">.input</span></code> and <code class="docutils literal notranslate"><span class="pre">.output</span></code> are Proxies for the eventual inputs and outputs of a module. In other words, when we access <code class="docutils literal notranslate"><span class="pre">model.output</span></code> what we are communicating to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> is, “When you compute the output of <code class="docutils literal notranslate"><span class="pre">model</span></code>, please grab it for me and put the value into its corresponding Proxy object. Let’s try it:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">output</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipython-input-4206295698.py</span> in <span class="ansi-cyan-fg">&lt;cell line: 0&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     output <span class="ansi-blue-fg">=</span> tiny_model<span class="ansi-blue-fg">.</span>output
<span class="ansi-green-intense-fg ansi-bold">      4</span>
<span class="ansi-green-fg">----&gt; 5</span><span class="ansi-red-fg"> </span>print<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;output&#39; is not defined
</pre></div></div>
</div>
<p>Oh no an error! “name <code class="docutils literal notranslate"><span class="pre">output</span></code> is not defined.”</p>
<p>Why doesn’t our <code class="docutils literal notranslate"><span class="pre">output</span></code> variable exist?</p>
<p>Proxy objects will only have their value at the end of a context if we call <code class="docutils literal notranslate"><span class="pre">.save()</span></code> on them. This helps to reduce memory costs. Adding <code class="docutils literal notranslate"><span class="pre">.save()</span></code> fixes the error:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ 0.2872, -0.0245]])
</pre></div></div>
</div>
<p>Success! We now have the model output. We just completed out first intervention using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>.</p>
<p>Each time we access a module’s input or output, we create an <em>intervention</em> in the neural network’s forward pass. Collectively these requests form the <em>intervention graph</em>. We call the process of executing it alongside the model’s normal computation graph, <em>interleaving</em>.</p>
<details><summary><p>On Model output</p>
</summary><hr class="docutils" />
<p>If we don’t need to access anything other than the model’s final output (i.e., the model’s predicted next token), we can call the tracing context with <code class="docutils literal notranslate"><span class="pre">trace=False</span></code> and not use it as a context. This could be useful for simple inference using NNsight.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="o">&lt;</span><span class="n">inputs</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
</details><p>Just like we saved the output of the model as a whole, we can save the output of any of its submodules. We use normal Python attribute syntax. We can discover how to access them by name by printing out the model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tiny_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Sequential(
  (layer1): Linear(in_features=5, out_features=10, bias=True)
  (layer2): Linear(in_features=10, out_features=2, bias=True)
)
</pre></div></div>
</div>
<p>Let’s access the output of the first layer (which we’ve named <code class="docutils literal notranslate"><span class="pre">layer1</span></code>):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>

    <span class="n">l1_output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">l1_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
</pre></div></div>
</div>
<p>Let’s do the same for the input of <code class="docutils literal notranslate"><span class="pre">layer2</span></code>.</p>
<p>Because we aren’t accessing the <code class="docutils literal notranslate"><span class="pre">tracer</span></code> object within these tracing contexts, we can also drop <code class="docutils literal notranslate"><span class="pre">as</span> <span class="pre">tracer</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

    <span class="n">l2_input</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">l2_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
</pre></div></div>
</div>
<details><summary><p>On module inputs</p>
</summary><hr class="docutils" />
<p>Notice how the value for <code class="docutils literal notranslate"><span class="pre">l2_input</span></code> is just a single tensor. By default, the <code class="docutils literal notranslate"><span class="pre">.input</span></code> attribute of a module will return the <strong>first</strong> tensor input to the module.</p>
<p>We can also access the full input to a module by using the <code class="docutils literal notranslate"><span class="pre">.inputs</span></code> attribute, which will return the values in the form of:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tuple(tuple(args), dictionary(kwargs))
</pre></div>
</div>
<p>Where the first index of the tuple is itself a tuple of all positional arguments, and the second index is a dictionary of the keyword arguments.</p>
<hr class="docutils" />
</details><p>Until now we were saving the output of the model and its submodules within the <code class="docutils literal notranslate"><span class="pre">trace</span></code> context to then print it after exiting the context. We will continuing doing this in the rest of the tutorial since it’s a good practice to save the computation results for later analysis.</p>
<p>However, we can also log the outputs of the model and its submodules within the <code class="docutils literal notranslate"><span class="pre">trace</span></code> context using <code class="docutils literal notranslate"><span class="pre">print</span></code> statements. This is useful for debugging and understanding the model’s behavior while saving memory.</p>
<p>Let’s see how to do this:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 1 - out: &quot;</span><span class="p">,</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 1 - out:  tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
</pre></div></div>
</div>
</section>
<section id="Functions,-Methods,-and-Operations">
<h2>Functions, Methods, and Operations<a class="headerlink" href="#Functions,-Methods,-and-Operations" title="Link to this heading">#</a></h2>
<p>Now that we can access activations, we also want to do some post-processing on it. Let’s find out which dimension of layer1’s output has the highest value.</p>
<p>We could do this by calling <code class="docutils literal notranslate"><span class="pre">torch.argmax(...)</span></code> after the tracing context or we can just leverage the fact that <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> handles Pytorch functions and methods within the tracing context, by creating a Proxy request for it:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

    <span class="c1"># Note we don&#39;t need to call .save() on the output,</span>
    <span class="c1"># as we&#39;re only using its value within the tracing context.</span>
    <span class="n">l1_output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span>

    <span class="c1"># We do need to save the argmax tensor however,</span>
    <span class="c1"># as we&#39;re using it outside the tracing context.</span>
    <span class="n">l1_amax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">l1_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">l1_amax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(2)
</pre></div></div>
</div>
<p>We can chain together multiple operations on the model’s intermediate outputs. Just remember to save everything at the end!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

    <span class="n">value</span> <span class="o">=</span> <span class="p">(</span><span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(0.5118)
</pre></div></div>
</div>
<p>The code block above is saying to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, “Run the model with the given <code class="docutils literal notranslate"><span class="pre">input</span></code>. When the output of <code class="docutils literal notranslate"><span class="pre">tiny_model.layer1</span></code> is computed, take its sum. Then do the same for <code class="docutils literal notranslate"><span class="pre">tiny_model.layer2</span></code>. Now that both of those are computed, add them and make sure not to delete this value as I wish to use it outside of the tracing context.”</p>
<p>We can apply any function we want during the trace context, even our own custom functions!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Take a tensor and return the sum of its elements</span>
<span class="k">def</span><span class="w"> </span><span class="nf">tensor_sum</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">flat</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">element</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>

    <span class="c1"># call on our custom function within the trace context</span>
    <span class="n">custom_sum</span> <span class="o">=</span> <span class="n">tensor_sum</span><span class="p">(</span><span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="nb">sum</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="nb">sum</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="n">custom_sum</span><span class="p">,</span> <span class="nb">sum</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor(0.2491) tensor(0.2491)
</pre></div></div>
</div>
</section>
<section id="Setting">
<h2>Setting<a class="headerlink" href="#Setting" title="Link to this heading">#</a></h2>
<p>Getting and analyzing the activations from various points in a model can be really insightful, and a number of ML techniques do exactly that. However, often we not only want to view the computation of a model, but also to influence it.</p>
<p>To demonstrate the effect of editing the flow of information through the model, let’s set the first dimension of the first layer’s output to 0. <code class="docutils literal notranslate"><span class="pre">NNsight</span></code> makes this really easy using the ‘=’ operator:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

    <span class="c1"># Save the output before the edit to compare.</span>
    <span class="c1"># Notice we apply .clone() before saving as the setting operation is in-place.</span>
    <span class="n">l1_output_before</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="c1"># Access the 0th index of the hidden state dimension and set it to 0.</span>
    <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Save the output after to see our edit.</span>
    <span class="n">l1_output_after</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before:&quot;</span><span class="p">,</span> <span class="n">l1_output_before</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After:&quot;</span><span class="p">,</span> <span class="n">l1_output_after</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Before: tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
After: tensor([[ 0.0000, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
</pre></div></div>
</div>
<p>Seems our change was reflected. Now let’s do the same for the last dimension:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

    <span class="c1"># Save the output before the edit to compare.</span>
    <span class="c1"># Notice we apply .clone() before saving as the setting operation is in-place.</span>
    <span class="n">l1_output_before</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="c1"># Access the last index of the hidden state dimension and set it to 0.</span>
    <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="n">hidden_dims</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Save the output after to see our edit.</span>
    <span class="n">l1_output_after</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before:&quot;</span><span class="p">,</span> <span class="n">l1_output_before</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After:&quot;</span><span class="p">,</span> <span class="n">l1_output_after</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NNsightException</span>                          Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipython-input-3404137504.py</span> in <span class="ansi-cyan-fg">&lt;cell line: 0&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">with</span> tiny_model<span class="ansi-blue-fg">.</span>trace<span class="ansi-blue-fg">(</span>input<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     <span class="ansi-red-fg"># Save the output before the edit to compare.</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     <span class="ansi-red-fg"># Notice we apply .clone() before saving as the setting operation is in-place.</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>     l1_output_before <span class="ansi-blue-fg">=</span> tiny_model<span class="ansi-blue-fg">.</span>layer1<span class="ansi-blue-fg">.</span>output<span class="ansi-blue-fg">.</span>clone<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>save<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/usr/local/lib/python3.11/dist-packages/nnsight/intervention/tracing/base.py</span> in <span class="ansi-cyan-fg">__exit__</span><span class="ansi-blue-fg">(self, exc_type, exc_val, exc_tb)</span>
<span class="ansi-green-intense-fg ansi-bold">    431</span>
<span class="ansi-green-intense-fg ansi-bold">    432</span>             <span class="ansi-red-fg"># Execute the traced code using the configured backend</span>
<span class="ansi-green-fg">--&gt; 433</span><span class="ansi-red-fg">             </span>self<span class="ansi-blue-fg">.</span>backend<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    434</span>
<span class="ansi-green-intense-fg ansi-bold">    435</span>             <span class="ansi-green-fg">return</span> <span class="ansi-green-fg">True</span>

<span class="ansi-green-fg">/usr/local/lib/python3.11/dist-packages/nnsight/intervention/backends/execution.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, tracer)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span>         <span class="ansi-green-fg">except</span> Exception <span class="ansi-green-fg">as</span> e<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     23</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">raise</span> wrap_exception<span class="ansi-blue-fg">(</span>e<span class="ansi-blue-fg">,</span> tracer<span class="ansi-blue-fg">.</span>info<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">from</span> <span class="ansi-green-fg">None</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>         <span class="ansi-green-fg">finally</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             Globals<span class="ansi-blue-fg">.</span>exit<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NNsightException</span>:

Traceback (most recent call last):
  File &#34;/tmp/ipython-input-3404137504.py&#34;, line 8, in &lt;cell line: 0&gt;
    tiny_model.layer1.output[:, hidden_dims] = 0

IndexError: index 10 is out of bounds for dimension 1 with size 10
</pre></div></div>
</div>
<p>Oh no, we are getting an error! Ah of course, we needed to index at <code class="docutils literal notranslate"><span class="pre">hidden_dims</span> <span class="pre">-</span> <span class="pre">1</span></code> not <code class="docutils literal notranslate"><span class="pre">hidden_dims</span></code>.</p>
<p>The error messaging feature can be toggled using <code class="docutils literal notranslate"><span class="pre">nnsight.CONFIG.APP.DEBUG</span></code> which defaults to true.</p>
<details><summary><p>Toggle Error Messaging</p>
</summary><p>Turn off debugging:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import nnsight

nnsight.CONFIG.APP.DEBUG = False
nnsight.CONFIG.save()
</pre></div>
</div>
<p>Turn on debugging:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import nnsight

nnsight.CONFIG.APP.DEBUG = True
nnsight.CONFIG.save()
</pre></div>
</div>
</details><p>Now that we know more about NNsight’s error messaging, let’s try our setting operation again with the correct indexing and view the shape of the output before leaving the tracing context:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>

    <span class="c1"># Save the output before the edit to compare.</span>
    <span class="c1"># Notice we apply .clone() before saving as the setting operation is in-place.</span>
    <span class="n">l1_output_before</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer 1 output shape: </span><span class="si">{</span><span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Access the last index of the hidden state dimension and set it to 0.</span>
    <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="n">hidden_dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Save the output after to see our edit.</span>
    <span class="n">l1_output_after</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before:&quot;</span><span class="p">,</span> <span class="n">l1_output_before</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After:&quot;</span><span class="p">,</span> <span class="n">l1_output_after</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 1 output shape: torch.Size([1, 10])
Before: tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
After: tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0000]])
</pre></div></div>
</div>
</section>
<section id="Gradients">
<h2>Gradients<a class="headerlink" href="#Gradients" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">NNsight</span></code> also lets us apply backpropagation and access gradients with respect to a loss. Like <code class="docutils literal notranslate"><span class="pre">.input</span></code> and <code class="docutils literal notranslate"><span class="pre">.output</span></code> on modules, <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> exposes <code class="docutils literal notranslate"><span class="pre">.grad</span></code> on Proxies themselves (assuming they are proxies of tensors):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now in NNsight 0.5</span>
<span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
  <span class="c1"># 1) access l1 &amp; l2 outputs so trace knows these are intermediate values we care about</span>
  <span class="n">l1_output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span>
  <span class="c1"># 2) make sure gradient flows back to l1 (it will pass by l2)</span>
  <span class="n">l1_output</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">l2_output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">output</span>

  <span class="c1"># 3) access gradients within a backwards trace</span>
  <span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">():</span>
    <span class="c1"># access .grad within backward context in REVERSE ORDER</span>
    <span class="n">layer2_output_grad</span> <span class="o">=</span> <span class="n">l2_output</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="n">layer1_output_grad</span> <span class="o">=</span> <span class="n">l1_output</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 1 output gradient:&quot;</span><span class="p">,</span> <span class="n">layer1_output_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 2 output gradient:&quot;</span><span class="p">,</span> <span class="n">layer2_output_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 1 output gradient: tensor([[ 5.0732e-01, -3.0065e-01, -4.2533e-01,  2.5249e-02,  1.6884e-01,
         -1.1749e-02,  1.9957e-04,  9.8918e-02,  1.0680e-01,  7.1143e-02]])
Layer 2 output gradient: tensor([[1., 1.]])
</pre></div></div>
</div>
<p>Some important things to look for when tracing gradients:</p>
<ol class="arabic simple">
<li><p>Register your intermediate values in advance If you want the gradient of a layer’s output, then first access that layer in the trace context before the <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> trace call. For us, that looked like:</p></li>
</ol>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>l1_output = tiny_model.layer1.output
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Make sure the gradient has somewhere to flow We set <code class="docutils literal notranslate"><span class="pre">l1_output.requires_grad</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> to make sure that the gradient flows to the earliest output we care about. Another option would be to do <code class="docutils literal notranslate"><span class="pre">tiny_model.input.requires_grad</span> <span class="pre">=</span> <span class="pre">True</span></code> at the beginning of the trace, but this is slightly less efficient, because we aren’t collecting any gradients there.</p></li>
<li><p>Call on modules in order within the trace <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> will ensure that your modules are called in the same order as the model’s execution. This means we should do all of our operations on layer 1 before moving on to collecting any information from layer 2.</p></li>
<li><p>Call on gradients in reverse order Similarly, we want to follow the order of the backward pass, which starts at the final layer and works its way to the input.</p></li>
</ol>
<p>All of the features we learned previously, also apply to <code class="docutils literal notranslate"><span class="pre">.grad</span></code>. In other words, we can apply operations to and edit the gradients. Let’s double the grad of <code class="docutils literal notranslate"><span class="pre">layer2</span></code>. Our intervention has downstream consequences - see how the gradient of <code class="docutils literal notranslate"><span class="pre">layer1</span></code> ends up doubled as well?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now in NNsight 0.5</span>
<span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
  <span class="c1"># 1) access l1 &amp; l2 outputs so trace knows these are intermediate values we care about</span>
  <span class="n">l1_output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span>
  <span class="c1"># 2) make sure gradient flows back to l1 (it will pass by l2)</span>
  <span class="n">l1_output</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">l2_output</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer2</span><span class="o">.</span><span class="n">output</span>

  <span class="c1"># 3) access gradients within a backwards trace</span>
  <span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">():</span>
    <span class="c1"># access .grad within backward context in REVERSE ORDER</span>
    <span class="n">l2_output</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">l2_output</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">layer2_output_grad</span> <span class="o">=</span> <span class="n">l2_output</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="n">layer1_output_grad</span> <span class="o">=</span> <span class="n">l1_output</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 1 output gradient:&quot;</span><span class="p">,</span> <span class="n">layer1_output_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 2 output gradient:&quot;</span><span class="p">,</span> <span class="n">layer2_output_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Layer 1 output gradient: tensor([[ 1.0146e+00, -6.0130e-01, -8.5066e-01,  5.0498e-02,  3.3768e-01,
         -2.3498e-02,  3.9914e-04,  1.9784e-01,  2.1360e-01,  1.4229e-01]])
Layer 2 output gradient: tensor([[2., 2.]])
</pre></div></div>
</div>
</section>
<section id="Early-Stopping">
<h2>Early Stopping<a class="headerlink" href="#Early-Stopping" title="Link to this heading">#</a></h2>
<p>If we are only interested in a model’s intermediate computations, we can halt a forward pass run at any module level, reducing runtime and conserving compute resources. One examples where this could be particularly useful would if we are working with SAEs - we can train an SAE on one layer and then stop the execution.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
   <span class="n">l1_out</span> <span class="o">=</span> <span class="n">tiny_model</span><span class="o">.</span><span class="n">layer1</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
   <span class="n">tracer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

<span class="c1"># get the output of the first layer and stop tracing</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L1 - Output: &quot;</span><span class="p">,</span> <span class="n">l1_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
L1 - Output:  tensor([[ 0.2185, -0.3810,  0.9104,  0.4726,  0.3635,  0.1594, -0.9225, -0.5772,
         -0.0298,  0.0354]])
</pre></div></div>
</div>
</section>
</section>
<section id="2️⃣-Bigger">
<h1>2️⃣ Bigger<a class="headerlink" href="#2️⃣-Bigger" title="Link to this heading">#</a></h1>
<p>Now that we have the basics of <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> under our belt, we can scale our model up and combine the techniques we’ve learned into more interesting experiments.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">NNsight</span></code> class is very bare bones. It wraps a pre-defined model and does no pre-processing on the inputs we enter. It’s designed to be extended with more complex and powerful types of models, and we’re excited to see what can be done to leverage its features!</p>
<p>However, if you’d like to load a Language Model from HuggingFace with its tokenizer, the<code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> subclass greatly simplifies this process.</p>
<section id="LanguageModel">
<h2>LanguageModel<a class="headerlink" href="#LanguageModel" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> is a subclass of <code class="docutils literal notranslate"><span class="pre">NNsight</span></code>. While we could define and create a model to pass in directly, <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> includes special support for Huggingface language models, including automatically loading models from a Huggingface ID, and loading the model together with the appropriate tokenizer.</p>
<p>Here is how we can use <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> to load <code class="docutils literal notranslate"><span class="pre">GPT-2</span></code>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nnsight</span><span class="w"> </span><span class="kn">import</span> <span class="n">LanguageModel</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">dispatch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
  (generator): Generator(
    (streamer): Streamer()
  )
)
</pre></div></div>
</div>
<p>When we initialize <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code>, we aren’t yet loading the parameters of the model into memory. We are actually loading a ‘meta’ version of the model which doesn’t take up any memory, but still allows us to view and trace actions on it. After exiting the first tracing context, the model is then fully loaded into memory. To load into memory on initialization, you can pass <code class="docutils literal notranslate"><span class="pre">dispatch=True</span></code> into <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> like
<code class="docutils literal notranslate"><span class="pre">LanguageModel('openai-community/gpt2',</span> <span class="pre">device_map=&quot;auto&quot;,</span> <span class="pre">dispatch=True)</span></code>.</p>
<details><summary><p>On Model Initialization</p>
</summary><hr class="docutils" />
<p>A few important things to note:</p>
<p>Keyword arguments passed to the initialization of <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> is forwarded to HuggingFace specific loading logic. In this case, <code class="docutils literal notranslate"><span class="pre">device_map</span></code> specifies which devices to use and its value <code class="docutils literal notranslate"><span class="pre">auto</span></code> indicates to evenly distribute it to all available GPUs (and CPU if no GPUs available). Other arguments can be found here: <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM">https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM</a></p>
<hr class="docutils" />
</details><p>Let’s now apply some of the features that we used on the small model to <code class="docutils literal notranslate"><span class="pre">GPT-2</span></code>. Unlike <code class="docutils literal notranslate"><span class="pre">NNsight</span></code>, <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> does define logic to pre-process inputs upon entering the tracing context. This makes interacting with the model simpler (i.e., you can send prompts to the model without having to directly access the tokenizer).</p>
<p>In the following example, we ablate the value coming from the last layer’s MLP module and decode the logits to see what token the model predicts without influence from that particular module:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span><span class="p">):</span>
    <span class="c1"># Access the last layer using h[-1] as it&#39;s a ModuleList</span>
    <span class="c1"># Access the first index of .output as that&#39;s where the hidden states are.</span>
    <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Token IDs:&quot;</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">)</span>

<span class="c1"># Apply the tokenizer to decode the ids into words after the tracing context.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction:&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]],
       device=&#39;cuda:0&#39;)
Prediction:  London
</pre></div></div>
</div>
<p>We just ran a little intervention on a much more complex model with many more parameters! However, we’re missing an important piece of information: what the prediction would have looked like without our ablation.</p>
<p>We could just run two tracing contexts and compare the outputs. However, this would require two forward passes through the model. <code class="docutils literal notranslate"><span class="pre">NNsight</span></code> can do better than that with batching.</p>
</section>
<section id="Batching">
<h2>Batching<a class="headerlink" href="#Batching" title="Link to this heading">#</a></h2>
<p>Batching is a way to process multiple inputs in one forward pass. To better understand how batching works, we’re going to bring back the <code class="docutils literal notranslate"><span class="pre">Tracer</span></code> object that we dropped before.</p>
<p>When we call <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code>, it’s actually creating two different contexts behind the scenes. The first one is the tracing context that we’ve discussed previously, and the second one is the invoker context. The invoker context defines the values of the <code class="docutils literal notranslate"><span class="pre">.input</span></code> and <code class="docutils literal notranslate"><span class="pre">.output</span></code> Proxies.</p>
<p>If we call <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code> with some input, the input is passed on to the invoker. As there is only one input, only one invoker context is created.</p>
<p>If we call <code class="docutils literal notranslate"><span class="pre">.trace()</span></code> without an input, then we can call <code class="docutils literal notranslate"><span class="pre">tracer.invoke(input1)</span></code> to manually create the invoker context with an input, <code class="docutils literal notranslate"><span class="pre">input1</span></code>. We can also repeatedly call <code class="docutils literal notranslate"><span class="pre">tracer.invoke(...)</span></code> to create the invoker context for additional inputs. Every subsequent time we call <code class="docutils literal notranslate"><span class="pre">.invoke(...)</span></code>, interventions within its context will only refer to the input in that particular invoke statement.</p>
<p>When exiting the tracing context, the inputs from all of the invokers will be batched together, and they will be executed in one forward pass! To test this out, let’s do the same ablation experiment, but also add a ‘control’ output for comparison:</p>
<details><summary><p>More on the invoker context</p>
</summary><hr class="docutils" />
<p>Note that when injecting data to only the relevant invoker interventions, <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> tries, but can’t guarantee, to narrow the data into the right batch indices. Thus, there are cases where all invokes will get all of the data. Specifically, if the input or output data is stored as an object that is not an arbitrary collection of tensors, it will be broadcasted to all invokes.</p>
<p>Just like <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code> created a <code class="docutils literal notranslate"><span class="pre">Tracer</span></code> object, <code class="docutils literal notranslate"><span class="pre">.invoke(...)</span></code> creates an <code class="docutils literal notranslate"><span class="pre">Invoker</span></code> object. For <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> models, the <code class="docutils literal notranslate"><span class="pre">Invoker</span></code> prepares the input by running a tokenizer on it. <code class="docutils literal notranslate"><span class="pre">Invoker</span></code> stores pre-processed inputs at <code class="docutils literal notranslate"><span class="pre">invoker.inputs</span></code>, which can be accessed to see information about our inputs. In a case where we pass a single input to <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code> directly, we can still access the invoker object at <code class="docutils literal notranslate"><span class="pre">tracer.invoker</span></code> without having to call <code class="docutils literal notranslate"><span class="pre">tracer.invoke(...)</span></code>.</p>
<div class="line-block">
<div class="line">Keyword arguments given to <code class="docutils literal notranslate"><span class="pre">.invoke(..)</span></code> make their way to the input pre-processing.</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> has keyword arguments <code class="docutils literal notranslate"><span class="pre">max_length</span></code> and <code class="docutils literal notranslate"><span class="pre">truncation</span></code> used for tokenization which can be passed to the invoker. If we want to pass keyword arguments to the invoker for a single-input <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code>, we can pass <code class="docutils literal notranslate"><span class="pre">invoker_args</span></code> as a dictionary of invoker keyword arguments.</div>
</div>
<p>Here is an example to demonstrate everything we’ve described:</p>
<p><strong>This snippet</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>with llm.trace(&quot;hello&quot;, invoker_args={&quot;max_length&quot;:10}) as tracer:
  invoker = tracer.invoker
</pre></div>
</div>
<p><strong>does the same as</strong></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>with llm.trace() as tracer:
  with tracer.invoke(&quot;hello&quot;, max_length=10) as invoker:
    invoker = invoker
</pre></div>
</div>
<hr class="docutils" />
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">()</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span><span class="p">):</span>

        <span class="c1"># Ablate the last MLP for only this batch.</span>
        <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Get the output for only the intervened on batch.</span>
        <span class="n">token_ids_intervention</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span><span class="p">):</span>

        <span class="c1"># Get the output for only the original batch.</span>
        <span class="n">token_ids_original</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original token IDs:&quot;</span><span class="p">,</span> <span class="n">token_ids_original</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modified token IDs:&quot;</span><span class="p">,</span> <span class="n">token_ids_intervention</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original prediction:&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids_original</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modified prediction:&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids_intervention</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
You&#39;re using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Original token IDs: tensor([[ 198,   12,  417, 8765,  318,  257,  262, 3504, 7372, 6342]],
       device=&#39;cuda:0&#39;)
Modified token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]],
       device=&#39;cuda:0&#39;)
Original prediction:  Paris
Modified prediction:  London
</pre></div></div>
</div>
<p>Based on our control results, our ablation did end up affecting what the model predicted. That’s pretty neat!</p>
<p>Another cool thing with multiple invokes is that Proxies can interact between them.</p>
<p>Here, we transfer the token embeddings from a real prompt into another placeholder prompt. Therefore the latter prompt produces the output of the former prompt:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">()</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">barrier</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wte</span><span class="o">.</span><span class="n">output</span>
        <span class="c1"># call barrier</span>
        <span class="n">barrier</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;_ _ _ _ _ _ _ _ _ _&quot;</span><span class="p">):</span>
        <span class="c1"># tell model to wait for the output from the previous invoke with barrier</span>
        <span class="n">barrier</span><span class="p">()</span>
        <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">wte</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">embeddings</span>
        <span class="n">token_ids_intervention</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;_ _ _ _ _ _ _ _ _ _&quot;</span><span class="p">):</span>
        <span class="n">token_ids_original</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original prediction shape&quot;</span><span class="p">,</span> <span class="n">token_ids_original</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original prediction:&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids_original</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;modified prediction shape&quot;</span><span class="p">,</span> <span class="n">token_ids_intervention</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modified prediction:&quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids_intervention</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
original prediction shape torch.Size([])
Original prediction:  _
modified prediction shape torch.Size([])
Modified prediction:  Paris
</pre></div></div>
</div>
<p>For larger batch sizes, you can also iteratate across multiple invoke contexts.</p>
</section>
<section id="Multiple-Token-Generation">
<h2>Multiple Token Generation<a class="headerlink" href="#Multiple-Token-Generation" title="Link to this heading">#</a></h2>
<p>Some HuggingFace models define methods to generate multiple outputs at a time. <code class="docutils literal notranslate"><span class="pre">LanguageModel</span></code> wraps that functionality to provide the same tracing features by using <code class="docutils literal notranslate"><span class="pre">.generate(...)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code>. This calls the underlying model’s <code class="docutils literal notranslate"><span class="pre">.generate</span></code> method. It passes the output through a <code class="docutils literal notranslate"><span class="pre">.generator</span></code> module that we’ve added onto the model, allowing us to get the generate output at <code class="docutils literal notranslate"><span class="pre">.generator.output</span></code>. You can control the number of new tokens generated by setting
<code class="docutils literal notranslate"><span class="pre">max_new_tokens</span> <span class="pre">=</span> <span class="pre">N</span></code> within your call to <code class="docutils literal notranslate"><span class="pre">.generate()</span></code>.</p>
<section id="Intervening-on-generated-token-iterations-with-.all()-and-.iter[]">
<h3>Intervening on generated token iterations with <code class="docutils literal notranslate"><span class="pre">.all()</span></code> and <code class="docutils literal notranslate"><span class="pre">.iter[]</span></code><a class="headerlink" href="#Intervening-on-generated-token-iterations-with-.all()-and-.iter[]" title="Link to this heading">#</a></h3>
<p>During model generation, the underlying model is called more than once, so the modules of said model produce more than one output. Which iteration should a given <code class="docutils literal notranslate"><span class="pre">module.output</span></code> refer to? That’s where <code class="docutils literal notranslate"><span class="pre">.all</span></code> and <code class="docutils literal notranslate"><span class="pre">.iter</span></code> come in!</p>
<p>If you want to access and intervene on module outputs across all iterations, you should use <code class="docutils literal notranslate"><span class="pre">.all()</span></code>. Simply create a <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">tracer.all():</span></code> context and include your intervention code within the indented block.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># using .all():</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s1">&#39;The Eiffel Tower is in the city of&#39;</span>
<span class="n">layers</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span>
<span class="n">n_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">n_new_tokens</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span> <span class="c1"># Initialize &amp; .save() list</span>

    <span class="c1"># Call .all() to apply intervention to each new token</span>
    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>

        <span class="c1"># Apply intervention - set first layer output to zero</span>
        <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Append desired hidden state post-intervention</span>
        <span class="n">hidden_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span> <span class="c1"># no need to call .save</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hidden state length: &quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Alternatively, if you want to intervene specific iterations of generation, you can use the <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">tracer.iter[&lt;slice&gt;]:</span></code> context. Here, let’s try intervening only on the generation iterations 2-5.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># using .all():</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s1">&#39;The Eiffel Tower is in the city of&#39;</span>
<span class="n">layers</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span>
<span class="n">n_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">n_new_tokens</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span> <span class="c1"># Initialize &amp; .save() list</span>

    <span class="c1"># Call .all() to apply intervention to each new token</span>
    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">iter</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]:</span>

        <span class="c1"># Apply intervention - set first layer output to zero</span>
        <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Append desired hidden state post-intervention</span>
        <span class="n">hidden_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">)</span> <span class="c1"># no need to call .save</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hidden state length: &quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="Model-Editing">
<h2>Model Editing<a class="headerlink" href="#Model-Editing" title="Link to this heading">#</a></h2>
<p>NNsight’s model editing feature allows you to create persistently modified versions of a model with a use of <code class="docutils literal notranslate"><span class="pre">.edit()</span></code>. Unlike interventions in a tracing context, which are temporary, the <strong>Editor</strong> context enables you to make lasting changes to a model instance.</p>
<p>This feature is useful for:</p>
<ul class="simple">
<li><p>Creating modified model variants without altering the original</p></li>
<li><p>Applying changes that persist across multiple forward passes</p></li>
<li><p>Comparing interventions between original and edited models</p></li>
</ul>
<p>Let’s explore how to use the <strong>Editor</strong> context to make a simple persistent change to a model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we take the hidden states with the expected output &quot;Paris&quot;</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is located in the city of&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">hs11</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># the edited model will now always predict &quot;Paris&quot; as the next token</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">edit</span><span class="p">()</span> <span class="k">as</span> <span class="n">llm_edited</span><span class="p">:</span>
    <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">hs11</span>

<span class="c1"># we demonstrate this by comparing the output of an unmodified model...</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;Vatican is located in the city of&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">original_tokens</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># ...with the output of the edited model</span>
<span class="k">with</span> <span class="n">llm_edited</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;Vatican is located in the city of&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">modified_tokens</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Original Prediction: &quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">original_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modified Prediction: &quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">modified_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Original Prediction:   Rome
Modified Prediction:   Paris
</pre></div></div>
</div>
<p>Edits defined within an <strong>Editor</strong> context create a new, modified version of the model by default, preserving the original. This allows for safe experimentation with model changes. If you wish to modify the original model directly, you can set <code class="docutils literal notranslate"><span class="pre">inplace=True</span></code> when calling <code class="docutils literal notranslate"><span class="pre">.edit()</span></code>.</p>
<p>Use this option cautiously, as in-place edits alter the base model for all the consequent model calls.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we use the hidden state we saved above (hs11)</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">edit</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">llm_edited</span><span class="p">:</span>
    <span class="n">llm</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">hs11</span>

<span class="c1"># we demonstrate this by comparing the output of an unmodified model...</span>
<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;Vatican is located in the city of&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="n">modified_tokens</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Modified In-place: &quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">modified_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Modified In-place:   Paris
</pre></div></div>
</div>
<p>If you’ve made in-place edits to your model and need to revert these changes, you can apply <code class="docutils literal notranslate"><span class="pre">.clear_edits()</span></code>. This method removes all edits applied to the model, effectively restoring it to its original state.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span><span class="o">.</span><span class="n">clear_edits</span><span class="p">()</span>

<span class="k">with</span> <span class="n">llm</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;Vatican is located in the city of&quot;</span><span class="p">):</span>
    <span class="n">modified_tokens</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Edits cleared: &quot;</span><span class="p">,</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">modified_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Edits cleared:   Rome
</pre></div></div>
</div>
</section>
</section>
<section id="3️⃣-I-thought-you-said-huge-models?">
<h1>3️⃣ I thought you said huge models?<a class="headerlink" href="#3️⃣-I-thought-you-said-huge-models?" title="Link to this heading">#</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">NNsight</span></code> is only one part of our project to democratize access to AI internals. The other half is the National Deep Inference Fabric, or <code class="docutils literal notranslate"><span class="pre">NDIF</span></code>. <code class="docutils literal notranslate"><span class="pre">NDIF</span></code> hosts large models for shared access using <code class="docutils literal notranslate"><span class="pre">NNsight</span></code>, so you don’t have to worry about any of the headaches of hosting large models yourself!</p>
<p>The interaction between <code class="docutils literal notranslate"><span class="pre">NDIF</span></code> and <code class="docutils literal notranslate"><span class="pre">NNsight</span></code> is fairly straightforward. The <strong>intervention graph</strong> we create via the tracing context can be encoded into a custom json format and sent via an http request to the <code class="docutils literal notranslate"><span class="pre">NDIF</span></code> servers. <code class="docutils literal notranslate"><span class="pre">NDIF</span></code> then decodes the <strong>intervention graph</strong> and <strong>interleaves</strong> it alongside the specified model.</p>
<p>To see which models are currently being hosted, check out the following status page: <a class="reference external" href="https://nnsight.net/status/">https://nnsight.net/status/</a></p>
<section id="Remote-execution">
<h2>Remote execution<a class="headerlink" href="#Remote-execution" title="Link to this heading">#</a></h2>
<p>In its current state, <code class="docutils literal notranslate"><span class="pre">NDIF</span></code> requires you to receive an API key. Therefore, to run the rest of this walkthrough, you need one of your own. To get one, simply register at <a class="reference external" href="https://login.ndif.us">https://login.ndif.us</a>.</p>
<p>With a valid API key, you then can configure <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nnsight</span><span class="w"> </span><span class="kn">import</span> <span class="n">CONFIG</span>

<span class="n">CONFIG</span><span class="o">.</span><span class="n">set_default_api_key</span><span class="p">(</span><span class="s2">&quot;YOUR_API_KEY&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>If you’re running in a local IDE, this only needs to be run once as it will save the API key as the default in a .config file along with your <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> installation. You can also add your API key to Google Colab secrets.</p>
<p>To amp things up a few levels, let’s demonstrate using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>’s tracing context with <code class="docutils literal notranslate"><span class="pre">Llama-3.1-8b</span></code>!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="c1"># Llama 3.1 8b is a gated model, so you need to apply for access on HuggingFace and include your token.</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;HF_TOKEN&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;YOUR_HUGGING_FACE_TOKEN&quot;</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "924a2c4032f64a3986ba784507f0e5ae", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you&#39;ve just configured.
WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you&#39;ve just configured.
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nnsight</span><span class="w"> </span><span class="kn">import</span> <span class="n">LanguageModel</span>

<span class="c1"># We&#39;ll never actually load the parameters locally, so no need to specify a device_map.</span>
<span class="n">llama</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3.1-8B&quot;</span><span class="p">)</span>
<span class="c1"># All we need to specify using NDIF vs executing locally is remote=True.</span>
<span class="k">with</span> <span class="n">llama</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">runner</span><span class="p">:</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">llama</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">llama</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>It really is as simple as <code class="docutils literal notranslate"><span class="pre">remote=True</span></code>. All of the techniques we went through in earlier sections work just the same when running locally or remotely.</p>
</section>
<section id="Sessions">
<h2>Sessions<a class="headerlink" href="#Sessions" title="Link to this heading">#</a></h2>
<p>NDIF uses a queue to handle concurrent requests from multiple users. To optimize the execution of our experiments we can use the <code class="docutils literal notranslate"><span class="pre">session</span></code> context to efficiently package multiple interventions together as one single request to the server.</p>
<p>This offers the following benefits:</p>
<ol class="arabic simple">
<li><p>All interventions within a session will be executed one after another without additional wait in the NDIF queue</p></li>
<li><p>All intermediate outputs for each intervention are stored on the server and can be accessed by other interventions in the same session without moving the data back and forth between NDIF and the local machine</p></li>
</ol>
<p>Let’s take a look:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">llama</span><span class="o">.</span><span class="n">session</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>

  <span class="k">with</span> <span class="n">llama</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">t1</span><span class="p">:</span>
    <span class="c1"># capture the hidden state from layer 32 at the last token</span>
    <span class="n">hs_31</span> <span class="o">=</span> <span class="n">llama</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">31</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># no .save()</span>
    <span class="n">t1_tokens_out</span> <span class="o">=</span> <span class="n">llama</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

  <span class="k">with</span> <span class="n">llama</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s2">&quot;Buckingham Palace is in the city of&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">t2</span><span class="p">:</span>
    <span class="n">llama</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">hs_31</span><span class="p">[:]</span>
    <span class="n">t2_tokens_out</span> <span class="o">=</span> <span class="n">llama</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">T1 - Original Prediction: &quot;</span><span class="p">,</span> <span class="n">llama</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t1_tokens_out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;T2 - Modified Prediction: &quot;</span><span class="p">,</span> <span class="n">llama</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">t2_tokens_out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="Next-Steps">
<h1>Next Steps<a class="headerlink" href="#Next-Steps" title="Link to this heading">#</a></h1>
<p>Check out <a class="reference external" href="https://nnsight.net/tutorials">nnsight.net/tutorials</a> for more walkthroughs implementating classic interpretability techniques using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>.</p>
<section id="Getting-Involved!">
<h2>Getting Involved!<a class="headerlink" href="#Getting-Involved!" title="Link to this heading">#</a></h2>
<p>Note that both <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> and <code class="docutils literal notranslate"><span class="pre">NDIF</span></code> are in active development, so changes may be made and errors may arise during use. If you’re interested in following updates to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, contributing, giving feedback, or finding collaborators, please join the <a class="reference external" href="https://discord.gg/6uFJmCSwW7">NDIF discord</a>. We’d love to hear about your work using nnsight!</p>
<p>You can also follow us on <a class="reference external" href="https://www.linkedin.com/company/national-deep-inference-fabric/">LinkedIn</a>, Bluesky: <a class="reference external" href="https://bsky.app/profile/ndif-team.bsky.social">&#64;ndif-team.bsky.social</a>, and X: <a class="reference external" href="https://x.com/ndif_team">&#64;ndif_team</a>.</p>
<p>💟</p>
<script type="application/vnd.jupyter.widget-state+json">
{"0096f4ae483840edb65d6a4136687217": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "08e2dbf8eacd44108798732514f09a5f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "14b22988f49c4439bc84a1a82891d0ed": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4a113ee29b2c45cc8326af1b195bd30d", "placeholder": "\u200b", "style": "IPY_MODEL_77faedbfdffe4580abce2e78d8999a5b", "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}}, "2615ba6b427f4eaba156a458f6315fd0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "LabelModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "LabelModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "LabelView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_96c80768cb624a44bbdd17316f7b110d", "placeholder": "\u200b", "style": "IPY_MODEL_2ca007d6d2954d09bc335be6986cf731", "value": "Connecting..."}}, "2bc735bb135b4cdfb7961bbf6841dfa6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2c91c0b798a74158835c90b17887922f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2ca007d6d2954d09bc335be6986cf731": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "40c0cc5ab1fb430db1f264ddd8227a26": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "41a3096d744b41859e00532cb9bd7dba": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ButtonModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Login", "disabled": false, "icon": "", "layout": "IPY_MODEL_08e2dbf8eacd44108798732514f09a5f", "style": "IPY_MODEL_5ffcdc21e0e9495a95c87fdebc5f23e8", "tooltip": ""}}, "4a113ee29b2c45cc8326af1b195bd30d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5ffcdc21e0e9495a95c87fdebc5f23e8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ButtonStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "667f40cef28a4e9799448824f04cd265": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "PasswordModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "PasswordModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "PasswordView", "continuous_update": true, "description": "Token:", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_40c0cc5ab1fb430db1f264ddd8227a26", "placeholder": "\u200b", "style": "IPY_MODEL_d5c9902cb5d446dc9bfdcacc5a4c4634", "value": ""}}, "77faedbfdffe4580abce2e78d8999a5b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "924a2c4032f64a3986ba784507f0e5ae": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "VBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": [], "layout": "IPY_MODEL_d0a4ba5c70704b5db507127585891437"}}, "96c80768cb624a44bbdd17316f7b110d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b01fd466f1d049a68e77e1e21f279ce1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_e6aeff134ee9414380b2f7ab0f9e65a9", "placeholder": "\u200b", "style": "IPY_MODEL_2c91c0b798a74158835c90b17887922f", "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}}, "d044fd5f7c4a4a158c48ca078fac1515": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "CheckboxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "CheckboxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "CheckboxView", "description": "Add token as git credential?", "description_tooltip": null, "disabled": false, "indent": true, "layout": "IPY_MODEL_0096f4ae483840edb65d6a4136687217", "style": "IPY_MODEL_2bc735bb135b4cdfb7961bbf6841dfa6", "value": true}}, "d0a4ba5c70704b5db507127585891437": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": "center", "align_self": null, "border": null, "bottom": null, "display": "flex", "flex": null, "flex_flow": "column", "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "50%"}}, "d5c9902cb5d446dc9bfdcacc5a4c4634": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e6aeff134ee9414380b2f7ab0f9e65a9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}
</script></section>
</section>


                </article>
              
              
              
              
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Walkthrough</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#The-API-for-a-transparent-science-on-black-box-AI">The API for a transparent science on black-box AI</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#1️⃣-First,-let's-start-small">1️⃣ First, let’s start small</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Tracing-Context">Tracing Context</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Getting">Getting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Functions,-Methods,-and-Operations">Functions, Methods, and Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Setting">Setting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Gradients">Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Early-Stopping">Early Stopping</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#2️⃣-Bigger">2️⃣ Bigger</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#LanguageModel">LanguageModel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Batching">Batching</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multiple-Token-Generation">Multiple Token Generation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Intervening-on-generated-token-iterations-with-.all()-and-.iter[]">Intervening on generated token iterations with <code class="docutils literal notranslate"><span class="pre">.all()</span></code> and <code class="docutils literal notranslate"><span class="pre">.iter[]</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Model-Editing">Model Editing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#3️⃣-I-thought-you-said-huge-models?">3️⃣ I thought you said huge models?</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Remote-execution">Remote execution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Sessions">Sessions</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Next-Steps">Next Steps</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Getting-Involved!">Getting Involved!</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025 NDIF.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>