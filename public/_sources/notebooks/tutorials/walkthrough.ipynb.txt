{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Walkthrough\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Scfr942GwO4"
      },
      "source": [
        "An interactive version of this walkthrough can be found\n",
        "[here](https://colab.research.google.com/github/JadenFiotto-Kaufman/nnsight/blob/dev/NNsight_v0_2.ipynb)\n",
        "\n",
        "In this era of large-scale deep learning, the most interesting AI models are\n",
        "massive black boxes that are hard to run. Ordinary commercial inference service\n",
        "APIs let you interact with huge models, but they do not let you access model\n",
        "internals.\n",
        "\n",
        "The nnsight library is different: it gives you full access to all the neural\n",
        "network internals. When used together with a remote service like the\n",
        "[National Deep Inference Facility](https://thevisible.net/docs/NDIF-proposal.pdf)\n",
        "(NDIF), it lets you run complex experiments on huge open source models easily,\n",
        "with fully transparent access.\n",
        "\n",
        "Our team wants to enable entire labs and independent researchers alike, as we\n",
        "believe a large, passionate, and collaborative community will produce the next\n",
        "big insights on a profoundly important field.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1OemD2VGyZx"
      },
      "source": [
        "# 1 First, let's start small\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLMmhrAKTNM"
      },
      "source": [
        "## The Tracing Context\n",
        "\n",
        "To demonstrate the core functionality and syntax of nnsight, we'll define and\n",
        "use a tiny two layer neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0RJijD0eXwf"
      },
      "outputs": [],
      "source": [
        "# Install nnsight\n",
        "!pip install nnsight\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgydw7i3HmIH"
      },
      "source": [
        "Our little model here is composed of four sub-modules, two linear layers\n",
        "('layer1', 'layer2'). We specify the sizes of each of these modules, and create\n",
        "some complementary example input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pX2Wg8Ceo6N"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(\n",
        "    OrderedDict(\n",
        "        [\n",
        "            (\"layer1\", torch.nn.Linear(input_size, hidden_dims)),\n",
        "            (\"layer2\", torch.nn.Linear(hidden_dims, output_size)),\n",
        "        ]\n",
        "    )\n",
        ").requires_grad_(False)\n",
        "\n",
        "input = torch.rand((1, input_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPa2h2pJIwl"
      },
      "source": [
        "The core object of the nnsight package is `NNsight`. This wraps around a given\n",
        "pytorch model to enable the capabilities nnsight provides.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H9R_ynTJI5y"
      },
      "outputs": [],
      "source": [
        "from nnsight import NNsight\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NfISlQ_Ilvp"
      },
      "source": [
        "Printing a Pytorch model shows a named hierarchy of modules which is very useful\n",
        "when accessing sub-components directly. NNsight models work the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYtnbJHvlGZV",
        "outputId": "5eb7c572-451c-43c6-956f-af224ae1867c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djC5kyJWLuUH"
      },
      "source": [
        "Before we actually get to using the model we just created, let's talk about\n",
        "Python contexts.\n",
        "\n",
        "Python contexts define a scope using the `with` statement and are often used to\n",
        "create some object, or initiate some logic, that you later want to destroy or\n",
        "conclude.\n",
        "\n",
        "The most common application is opening files like the following example:\n",
        "\n",
        "```python\n",
        "with open('myfile.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "```\n",
        "\n",
        "Python uses the `with` keyword to enter a context-like object. This object\n",
        "defines logic to be run at the start of the `with` block, as well as logic to be\n",
        "run when exiting. When using `with` for a file, entering the context opens the\n",
        "file and exiting the context closes it. Being within the context means we can\n",
        "read from the file. Simple enough! Now we can discuss how `nnsight` uses\n",
        "contexts to enable intuitive access into the internals of a neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNvuCOeyojcA"
      },
      "source": [
        "The main tool with `nnsight` is a context for tracing.\n",
        "\n",
        "We enter the tracing context by calling `model.trace(<input>)` on an `NNsight`\n",
        "model, which defines how we want to run the model. Inside the context, we will\n",
        "be able to customize how the neural network runs. The model is actually run upon\n",
        "exiting the tracing context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEXQ4auPSL-m"
      },
      "outputs": [],
      "source": [
        "with model.trace(input) as tracer:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQsHinjqicJ"
      },
      "source": [
        "But where's the output? To get that, we'll have to learn how to request it from\n",
        "within the tracing context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMfBpYzDPMoB"
      },
      "source": [
        "## Getting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_aFwFRv0ax"
      },
      "source": [
        "Earlier, when we wrapped our little neural net with the `NNsight` class. This\n",
        "added a couple properties to each module in the model (including the root model\n",
        "itself). The two most important ones are `.input` and `.output`.\n",
        "\n",
        "```python\n",
        "model.input\n",
        "model.output\n",
        "```\n",
        "\n",
        "The names are self explanatory. They correspond to the inputs and outputs of\n",
        "their respective modules during a forward pass of the model. We can use these\n",
        "attributes inside the `with` block.\n",
        "\n",
        "However, it is important to understand that the model is not executed until the\n",
        "end of the tracing context. How can we access inputs and outputs before the\n",
        "model is run? The trick is deferred execution.\n",
        "\n",
        "`.input` and `.output` are Proxies for the eventual inputs and outputs of a\n",
        "module. In other words, when you access `model.output` what you are\n",
        "communicating to `nnsight` is, \"When you compute the output of `model`, please\n",
        "grab it for me and put the value into its corresponding Proxy object's `.value`\n",
        "attribute.\" Let's try it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "cYe8-r9ptGaG",
        "outputId": "adf25efe-d2cf-4f65-9eaf-15b155762f4b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Accessing Proxy value before it's been set.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c7e0c74b12fa>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nnsight/tracing/Proxy.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accessing Proxy value before it's been set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Accessing Proxy value before it's been set."
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "    output = model.output\n",
        "\n",
        "print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBz5qfwuR-6F"
      },
      "source": [
        "Oh no an error! \"Accessing Proxy value before it's been set.\"\n",
        "\n",
        "Why doesn't our `output` have a `value`?\n",
        "\n",
        "Proxy objects will only have their value at the end of a context if we call\n",
        "`.save()` on them. This helps to reduce memory costs. Adding `.save()` fixes the\n",
        "error:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_bXRd5dvsBu",
        "outputId": "a8e8985c-2661-46a8-975b-d43f67cfc162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1473, -0.1518]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "    output = model.output.save()\n",
        "\n",
        "print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5C0UZCwvrYn"
      },
      "source": [
        "Success! We now have the model output. You just completed your first\n",
        "intervention using `nnsight`.\n",
        "\n",
        "Each time you access a module's input or output, you create an _intervention_ in\n",
        "the neural network's forward pass. Collectively these requests form the\n",
        "_intervention graph_. We call the process of executing it alongside the model's\n",
        "normal computation graph, _interleaving_.\n",
        "\n",
        "<details>\n",
        "<summary>On Model output</summary>\n",
        "\n",
        "---\n",
        "\n",
        "If you don't need to access anything other than the final model output, you can\n",
        "call the tracing context with `trace=False` and not use it as a context:\n",
        "\n",
        "```python\n",
        "output = model.trace(<inputs>, trace=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "Just like we saved the output of the model as a whole, we can save the output of\n",
        "any of its submodules. We use normal Python attribute syntax. We can discover\n",
        "how to access them by name by printing out the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WcVUSP-0CJi",
        "outputId": "7fd9d6ac-152d-4431-9a4d-0b60321638eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWr-cNqy-9O",
        "outputId": "68b9e071-c609-4b70-906d-2f2cd93b4495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0458,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968, -0.8834]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "    l1_output = model.layer1.output.save()\n",
        "\n",
        "print(l1_output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85A-aP_03ht6"
      },
      "source": [
        "Let's do the same for the input of layer2. While we're at it, let's also drop\n",
        "the `as tracer`, as we won't be needing the tracer object itself for a few\n",
        "sections:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EHEN38N3nXR",
        "outputId": "5afd17a7-0bcc-4d1a-c131-7038bde9455d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((tensor([[ 0.0458,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968, -0.8834]]),), {})\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    l2_input = model.layer2.input.save()\n",
        "\n",
        "print(l2_input.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk-U8zi33Gi-"
      },
      "source": [
        "<details>\n",
        "  <summary>On module inputs</summary>\n",
        "\n",
        "---\n",
        "\n",
        "Notice how the value for `l2_input`, was not just a single tensor. The\n",
        "type/shape of values from `.input` is in the form of:\n",
        "\n",
        "      tuple(tuple(args), dictionary(kwargs))\n",
        "\n",
        "Where the first index of the tuple is itself a tuple of all positional\n",
        "arguments, and the second index is a dictionary of the keyword arguments.\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "Now that we can access activations, we also want to do some post-processing on\n",
        "it. Let's find out which dimension of layer1's output has the highest value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Xo_PHyPr4p"
      },
      "source": [
        "## Functions, Methods, and Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehSofWbx5DSx"
      },
      "source": [
        "We could do this by calling `torch.argmax(...)` after the tracing context or we\n",
        "can just leverage the fact that `nnsight` handles functions and methods within\n",
        "the tracing context, by creating a Proxy request for it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5XCiSZn2p3k",
        "outputId": "97d34dee-7bb5-4735-8c53-6aac6377af9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # Note we don't need to call .save() on the output,\n",
        "    # as we're only using its value within the tracing context.\n",
        "    l1_output = model.layer1.output\n",
        "\n",
        "    l1_amax = torch.argmax(l1_output, dim=1).save()\n",
        "\n",
        "print(l1_amax[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGPUJvWq_bOp"
      },
      "source": [
        "Nice! That worked seamlessly, but hold on, how come we didn't need to call\n",
        "`.value[0]` on the result? In previous sections, we were just being explicit to\n",
        "get an understanding of Proxies and their value. In practice, however, `nnsight`\n",
        "knows that when outside of the tracing context we only care about the actual\n",
        "value, and so printing, indexing, and applying functions all immediately return\n",
        "and reflect the data in `.value`. So for the rest of the tutorial we won't use\n",
        "it.\n",
        "\n",
        "The same principles work for methods and operations as well:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIcOYeEjFuln",
        "outputId": "d836abd3-b713-44c5-aedc-95bf85abf11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.3416)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    value = (model.layer1.output.sum() + model.layer2.output.sum()).save()\n",
        "\n",
        "print(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0vNOJtJ2oFR"
      },
      "source": [
        "By default, torch functions, methods and all operators work with `nnsight`. We\n",
        "also enable the use of the `einops` library.\n",
        "\n",
        "So to recap, the above code block is saying to `nnsight`, \"Run the model with\n",
        "the given `input`. When the output of layer1 is computed, take its sum. Then do\n",
        "the same for layer2. Now that both of those are computed, add them and make sure\n",
        "not to delete this value as I wish to use it outside of the tracing context.\"\n",
        "\n",
        "Getting and analyzing the activations from various points in a model can be\n",
        "really insightful, and a number of ML techniques do exactly that. However, often\n",
        "times we not only want to view the computation of a model, but influence it as\n",
        "well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_5qH5gHPOT_"
      },
      "source": [
        "## Setting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgju-b_IOLlq"
      },
      "source": [
        "To demonstrate the effect of editing the flow of information through the model,\n",
        "let's set the first dimension of the first layer's output to 0. `NNsight` makes\n",
        "this really easy using '=' operator:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6y2wzJqOz3a",
        "outputId": "6274e5c5-c5fe-4e07-87c4-a119cfd09742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: tensor([[ 0.0458,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968, -0.8834]])\n",
            "After: tensor([[ 0.0000,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968, -0.8834]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # Save the output before the edit to compare.\n",
        "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "    l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "    # Access the 0th index of the hidden state dimension and set it to 0.\n",
        "    model.layer1.output[:, 0] = 0\n",
        "\n",
        "    # Save the output after to see our edit.\n",
        "    l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZz9SMs3Y_iS"
      },
      "source": [
        "Seems our change was reflected. Now the same for the last dimension:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "qwlqvHFcxld2",
        "outputId": "28ae7175-5af0-4bd0-f9f0-21680a4fd748"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 10 is out of bounds for dimension 1 with size 10",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a1e18ebd4137>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Save the output before the edit to compare.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Notice we apply .clone() before saving as the setting operation is in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0ml1_output_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nnsight/contexts/Runner.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-a1e18ebd4137>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Access the last index of the hidden state dimension and set it to 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Save the output after to see our edit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nnsight/tracing/Proxy.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSelf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         self.node.graph.add(\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nnsight/tracing/Graph.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                     value = target(\n\u001b[0m\u001b[1;32m    147\u001b[0m                         \u001b[0;34m*\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_proxy_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                         \u001b[0;34m**\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_proxy_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 1 with size 10"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # Save the output before the edit to compare.\n",
        "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "    l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "    # Access the last index of the hidden state dimension and set it to 0.\n",
        "    model.layer1.output[:, hidden_dims] = 0\n",
        "\n",
        "    # Save the output after to see our edit.\n",
        "    l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCNR_Jkpxr8l"
      },
      "source": [
        "Ah of course, we needed to index at `hidden_dims - 1` not `hidden_dims`. How did\n",
        "`nnsight` know there was this indexing error before leaving the tracing context?\n",
        "\n",
        "Earlier when discussing contexts in Python, we learned some logic happens upon\n",
        "entering, and some logic happens upon exiting. We know the model is actually run\n",
        "on exit, but what happens on enter? Our input IS actually run though the model,\n",
        "however under its own \"fake\" context. This means the input makes its way through\n",
        "all of the model operations, allowing `nnsight` to record the shapes and data\n",
        "types of module inputs and outputs! The operations are never executed using\n",
        "tensors with real values so it doesn't incur any memory costs. Then, when\n",
        "creating proxy requests like the setting one above, `nnsight` also attempts to\n",
        "execute the request on the \"fake\" values we recorded. Hence, it lets us know if\n",
        "our request is feasible before even running the model.\n",
        "\n",
        "<details>\n",
        "<summary>On scanning</summary>\n",
        "\n",
        "---\n",
        "\n",
        "\"Scanning\" is what we call running \"fake\" inputs throught the model to collect\n",
        "information like shapes and types. \"Validating\" is what we call trying to\n",
        "execute your intervention proxies with \"fake\" inputs to see if they work. If you\n",
        "are doing anything in a loop where efficiency is important, you should turn off\n",
        "scanning and validating. You can turn off validating in `.trace(...)` like\n",
        "`.trace(..., validate=False)`. You can turn off scanning in `Tracer.invoke(...)`\n",
        "([see the Batching section](#batching-id)) like `Tracer.invoke(..., scan=False)`\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "Let's try again with the correct indexing, and view the shape of the output\n",
        "before leaving the tracing context:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf8ugJKB2mru",
        "outputId": "4eed6c19-cb8c-4deb-e0e1-25aa80959c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer1 output shape: torch.Size([1, 10])\n",
            "Before: tensor([[ 0.0458,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968, -0.8834]])\n",
            "After: tensor([[ 0.0458,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968,  0.0000]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # Save the output before the edit to compare.\n",
        "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "    l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "    print(f\"layer1 output shape: {model.layer1.output.shape}\")\n",
        "\n",
        "    # Access the last index of the hidden state dimension and set it to 0.\n",
        "    model.layer1.output[:, hidden_dims - 1] = 0\n",
        "\n",
        "    # Save the output after to see our edit.\n",
        "    l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DH7PeL13-WU"
      },
      "source": [
        "We can also just replace proxy inputs and outputs with tensors of the same shape\n",
        "and type. Let's use the shape information we have at our disposal to add noise\n",
        "to the output, and replace it with this new noised tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP5wRmz_4YE7",
        "outputId": "4c30dc1d-232d-4900-bfdd-0c93a9b23ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: tensor([[ 0.0458,  0.5267,  0.7119,  0.4046,  0.2460,  0.7998,  0.4485, -0.2506,\n",
            "          0.2968, -0.8834]])\n",
            "After: tensor([[ 0.0581,  0.5168,  0.6561,  0.4083,  0.2617,  0.7800,  0.4080, -0.2213,\n",
            "          0.3394, -0.9187]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # Save the output before the edit to compare.\n",
        "    # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "    l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "    # Create random noise with variance of .001\n",
        "    noise = (0.001**0.5) * torch.randn(l1_output_before.shape)\n",
        "\n",
        "    # Add to original value and replace.\n",
        "    model.layer1.output = l1_output_before + noise\n",
        "\n",
        "    # Save the output after to see our edit.\n",
        "    l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s016CelFP8sx"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "`NNsight` can also let you apply backprop and access gradients with respect to a\n",
        "loss. Like `.input` and `.output` on modules, `nnsight` also exposes `.grad` on\n",
        "Proxies themselves (assuming they are proxies of tensors):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-cYUNNyRDn1",
        "outputId": "d03dafa1-ef1e-40f1-de28-4dc2f77fe4db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 output gradient: tensor([[ 0.4545, -0.0596, -0.2059,  0.4643, -0.4211, -0.2813,  0.2126,  0.5016,\n",
            "         -0.0126, -0.1564]])\n",
            "Layer 2 output gradient: tensor([[1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # We need to explicitly have the tensor require grad\n",
        "    # as the model we defined earlier turned off requiring grad.\n",
        "    model.layer1.output.requires_grad = True\n",
        "\n",
        "    # We call .grad on a tensor Proxy to communicate we want to store its gradient.\n",
        "    # We need to call .save() of course as .grad is its own Proxy.\n",
        "    layer1_output_grad = model.layer1.output.grad.save()\n",
        "    layer2_output_grad = model.layer2.output.grad.save()\n",
        "\n",
        "    # Need a loss to propagate through the later modules in order to have a grad.\n",
        "    loss = model.output.sum()\n",
        "    loss.backward()\n",
        "\n",
        "print(\"Layer 1 output gradient:\", layer1_output_grad)\n",
        "print(\"Layer 2 output gradient:\", layer2_output_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o0JaaYvWlHG"
      },
      "source": [
        "All of the features we learned previously, also apply to `.grad`. In other\n",
        "words, we can apply operations to and edit the gradients. Let's zero the grad of\n",
        "`layer1` and double the grad of `layer2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bFoaJpOWlRb",
        "outputId": "1c026534-9ffe-40e7-d016-e4565ddde8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 output gradient: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "Layer 2 output gradient: tensor([[2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "    # We need to explicitly have the tensor require grad\n",
        "    # as the model we defined earlier turned off requiring grad.\n",
        "    model.layer1.output.requires_grad = True\n",
        "\n",
        "    model.layer1.output.grad[:] = 0\n",
        "    model.layer2.output.grad = model.layer2.output.grad.clone() * 2\n",
        "\n",
        "    layer1_output_grad = model.layer1.output.grad.save()\n",
        "    layer2_output_grad = model.layer2.output.grad.save()\n",
        "\n",
        "    # Need a loss to propagate through the later modules in order to have a grad.\n",
        "    loss = model.output.sum()\n",
        "    loss.backward()\n",
        "\n",
        "print(\"Layer 1 output gradient:\", layer1_output_grad)\n",
        "print(\"Layer 2 output gradient:\", layer2_output_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvQ1nZgYQDG3"
      },
      "source": [
        "# 2 Bigger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miQgUY4NAmDQ"
      },
      "source": [
        "Now that we have the basics of `nnsight` under our belt, we can scale our model\n",
        "up and combine the techniques we've learned into more interesting experiments.\n",
        "\n",
        "The `NNsight` class is very bare bones. It wraps a pre-defined model and does no\n",
        "pre-processing on the inputs we enter. It's designed to be extended with more\n",
        "complex and powerful types of models and we're excited to see what can be done\n",
        "to leverage its features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJDblHiQpp1"
      },
      "source": [
        "## LanguageModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l9mZOY5HFH2"
      },
      "source": [
        "`LanguageModel` is a subclass of `NNsight`. While we could define and create a\n",
        "model to pass in directly, `LanguageModel` includes special support for\n",
        "Huggingface language models, including automatically loading models from a\n",
        "Huggingface ID, and loading the model together with the appropriate tokenizer.\n",
        "\n",
        "Here is how you can use `LanguageModel` to load `GPT-2`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769,
          "referenced_widgets": [
            "0a676b8f2dc34fed9731f19a03e4bfa1",
            "74358c341daa44faa41a92b0eb073615",
            "c40e17a994ed44d79ef54421f4772858",
            "431ffcb0dbb1419dbbb237a29842b905",
            "c28c185a508e459b95edb29afedd9c79",
            "7a0f3d2deeb14bbba7f0df208133a7e2",
            "94cdcf1fb0914dfe99c3c615e68b7ec8",
            "f3e24aa6619e4185a3e2f53f2e473e45",
            "b7be20bd738940658f710309729d8ccd",
            "562a287082eb4882af864062ae13c91b",
            "11e99924bdf84739b3cfc7d676b797e5",
            "60fe026e908741209637604edfeb0563",
            "e7a73cfc3fa04425afc08368532ef0be",
            "d839da1c31a34659988e3420ba520725",
            "96a3b8c765374a5e98eaf713dd5e85d8",
            "b49ce0d00e5b4df097b3a9fc4c0bd8e7",
            "b2748df9aff540df954f002310606a56",
            "f4caf2bc58704464b6ac7bf6da31b71d",
            "3731a5747724403e86ef1abaf4fb389b",
            "688bf676958a4050b73cc9597f5110de",
            "8776d04bfd3748b69e10708466e9c132",
            "56490c20aef04fe4a63ba7c990e71e77",
            "b8ad433cc3dd4a8689cb06655158d545",
            "e51620404d244a4e806eeaf3b96a474f",
            "e4568f72971749ac92cc5577cb9c4f9c",
            "1a1ada8b122d4502949c4c75861a82bf",
            "18d4c2302f424bc18f9215f2d5b83393",
            "f28f557210b7402b8b2d0fd83f038dc1",
            "bc8190c700e949ebb6f874c8f8055161",
            "ceebda1d07b545f5b605551c08e73be8",
            "6aa96b80e3b746b48f5c13bcafe1134e",
            "8de63414803f41c29e75242756cfd67f",
            "f693dfed91df492c986afbd4ff925f89",
            "cef2516aabc24f6d852079984afa8090",
            "95d303d83d3348a0adfdf6b5cb94c41c",
            "9373fcd054014760b23e3acc928da0a5",
            "7e6b41ec535648b190bfc655d9e430dd",
            "d0198fa98e5f412380f2ba3e9448aef3",
            "eb0c489b913744a28304e5334bb66532",
            "c80603555c634e338f16c0021f752420",
            "0fddebfb42e847e08890d09f30161984",
            "d07a0a49a2924092affc0e07bf0b5ef0",
            "beb4231b49024d8a9bb7ead19647d99d",
            "26d09aa072d64af0bf4df9a89309093a"
          ]
        },
        "id": "1OD2z7d3HQJU",
        "outputId": "7a53864c-6965-4c25-9668-4b0d56db9f6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a676b8f2dc34fed9731f19a03e4bfa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60fe026e908741209637604edfeb0563",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8ad433cc3dd4a8689cb06655158d545",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cef2516aabc24f6d852079984afa8090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): WrapperModule()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2VaAr_ezkj"
      },
      "source": [
        "<details>\n",
        "<summary>On Model Initialization</summary>\n",
        "\n",
        "---\n",
        "\n",
        "A few important things to note:\n",
        "\n",
        "Keyword arguments passed to the initialization of `LanguageModel` is forwarded\n",
        "to HuggingFace specific loading logic. In this case, `device_map` specifies\n",
        "which devices to use and its value `auto` indicates to evenly distribute it to\n",
        "all available GPUs (and cpu if no GPUs available). Other arguments can be found\n",
        "here:\n",
        "https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM\n",
        "\n",
        "When we initialize `LanguageModel`, we aren't yet loading the parameters of the\n",
        "model into memory. We are actually loading a 'meta' version of the model which\n",
        "doesn't take up any memory, but still allows us to view and trace actions on it.\n",
        "After exiting the first tracing context, the model is then fully loaded into\n",
        "memory. To load into memory on initialization, you can pass `dispatch=True` into\n",
        "`LanguageModel` like\n",
        "`LanguageModel('openai-community/gpt2', device_map=\"auto\", dispatch=True)`.\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "Let's put together some of the features we applied to the small model, but now\n",
        "on `GPT-2`. Unlike `NNsight`, `LanguageModel` does define logic to pre-process\n",
        "inputs upon entering the tracing context. This makes interacting with the model\n",
        "simpler without having to directly access the tokenizer.\n",
        "\n",
        "In the following example, we ablate the value coming from the last layer's MLP\n",
        "module and decode the logits to see what token the model predicts without\n",
        "influence from that particular module:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLSapaMCgLNU",
        "outputId": "192b8998-ecca-4f25-ecf2-5196c0d2d672"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]])\n",
            "Prediction:  London\n"
          ]
        }
      ],
      "source": [
        "with model.trace(\"The Eiffel Tower is in the city of\"):\n",
        "\n",
        "    # Access the last layer using h[-1] as it's a ModuleList\n",
        "    # Access the first index of .output as that's where the hidden states are.\n",
        "    model.transformer.h[-1].mlp.output[0][:] = 0\n",
        "\n",
        "    # Logits come out of model.lm_head and we apply argmax to get the predicted token ids.\n",
        "    token_ids = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Apply the tokenizer to decode the ids into words after the tracing context.\n",
        "print(\"Prediction:\", model.tokenizer.decode(token_ids[0][-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2-HiCQhkv-B"
      },
      "source": [
        "You just ran a little intervention on a much more complex model with a lot more\n",
        "parameters! An important piece of information we're missing though is what the\n",
        "prediction would look like without our ablation.\n",
        "\n",
        "Of course we could just run two tracing contexts and compare the outputs. This,\n",
        "however, would require two forward passes through the model. `NNsight` can do\n",
        "better than that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLjHnRyRPXjp"
      },
      "source": [
        "<a name=\"batching-id\"></a>\n",
        "\n",
        "## Batching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gc-Zr6NmEOn"
      },
      "source": [
        "It's time to bring back the `Tracer` object we dropped before. See, when you\n",
        "call `.trace(...)` with some input, it's actually creating two different\n",
        "contexts behind the scenes. The second one is the invoker context. Being within\n",
        "this context just means that `.input` and `.output` should refer only to the\n",
        "input you've given invoke. Calling `.trace(...)` with some input just means\n",
        "there's only one input and therefore only one invoker context.\n",
        "\n",
        "We can call `.trace()` without input and call `Tracer.invoke(...)` to manually\n",
        "create the invoker context with our input. Now every subsequent time we call\n",
        "`.invoke(...)`, new interventions will only refer to the input in that\n",
        "particular invoke. When exiting the tracing context, the inputs from all of the\n",
        "invokers will be batched together, and they will be executed in one forward\n",
        "pass! So let's do the ablation experiment, and compute a 'control' output to\n",
        "compare to:\n",
        "\n",
        "<details>\n",
        "<summary>On the invoker context</summary>\n",
        "\n",
        "---\n",
        "\n",
        "Note that when injecting data to only the relevant invoker interventions,\n",
        "`nnsight` tries, but can't guarantee, that it can narrow the data into the right\n",
        "batch idxs (in the case of an object as input or output). So there are cases\n",
        "where all invokes will get all of the data.\n",
        "\n",
        "Just like `.trace(...)` created a `Tracer` object, `.invoke(...)` creates an\n",
        "`Invoker` object. The `Invoker` object has post-processed inputs at\n",
        "`invoker.inputs`, which can be useful for seeing information about your input.\n",
        "If you are using `.trace(...)` with inputs, you can still access the invoker\n",
        "object at `tracer._invoker`.\n",
        "\n",
        "Keyword arguments given to `.invoke(..)` make its way to the input\n",
        "pre-processing. For example in `LanguageModel`, the keyword arguments are used\n",
        "to tokenize like `max_length` and `truncation`. If you need to pass in keyword\n",
        "arguments directly to one input `.trace(...)`, you can pass an `invoker_args`\n",
        "keyword argument that should be a dictionary of keyword arguments for the\n",
        "invoker. `.trace(..., invoker_args={...})`\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdcq4oCNmEua",
        "outputId": "e6d33c07-d22a-41af-b84a-91c9681c7524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original token IDs: tensor([[ 198,   12,  417, 8765,  318,  257,  262, 3504, 7372, 6342]])\n",
            "Intervention token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]])\n",
            "Original prediction:  Paris\n",
            "Intervention prediction:  London\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "    with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
        "\n",
        "        # Ablate the last MLP for only this batch.\n",
        "        model.transformer.h[-1].mlp.output[0][:] = 0\n",
        "\n",
        "        # Get the output for only the intervened on batch.\n",
        "        token_ids_intervention = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "    with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
        "\n",
        "        # Get the output for only the original batch.\n",
        "        token_ids_original = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Original token IDs:\", token_ids_original)\n",
        "print(\"Intervention token IDs:\", token_ids_intervention)\n",
        "\n",
        "print(\"Original prediction:\", model.tokenizer.decode(token_ids_original[0][-1]))\n",
        "print(\"Intervention prediction:\", model.tokenizer.decode(token_ids_intervention[0][-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BsybgsK2Bbr"
      },
      "source": [
        "So it did end up affecting what the model predicted. That's pretty neat!\n",
        "\n",
        "Another cool thing with multiple invokes is that the Proxies can interact\n",
        "between them. Here we transfer the word token embeddings from a real prompt into\n",
        "another placeholder prompt. Therefore the latter prompt produces the output of\n",
        "the former prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syKTI_KhpvCY",
        "outputId": "b2238c5c-d2d8-47bf-b73a-730f1e03cf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original prediction:  _\n",
            "Intervention prediction:  Paris\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "    with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
        "\n",
        "        embeddings = model.transformer.wte.output\n",
        "\n",
        "    with tracer.invoke(\"_ _ _ _ _ _ _ _ _ _\"):\n",
        "\n",
        "        model.transformer.wte.output = embeddings\n",
        "\n",
        "        token_ids_intervention = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "    with tracer.invoke(\"_ _ _ _ _ _ _ _ _ _\"):\n",
        "\n",
        "        token_ids_original = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Original prediction:\", model.tokenizer.decode(token_ids_original[0][-1]))\n",
        "print(\"Intervention prediction:\", model.tokenizer.decode(token_ids_intervention[0][-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWA-CWqQtah"
      },
      "source": [
        "## .next()\n",
        "\n",
        "Some HuggingFace models define methods to generate multiple outputs at a time.\n",
        "`LanguageModel` wraps that functionality to provide the same tracing features by\n",
        "using `.generate(...)` instead of `.trace(...)`. This calls the underlying\n",
        "model's `.generate` method. It passes the output through a `model.generator`\n",
        "module that we've added onto the model, allowing you to get the generate output\n",
        "at `model.generator.output`.\n",
        "\n",
        "In a case like this, the underlying model is called more than once; the modules\n",
        "of said model produce more than one output. Which iteration should a given\n",
        "`module.output` refer to? That's where `Module.next()` comes in.\n",
        "\n",
        "Each module has a call idx associated with it and `.next()` simply increments\n",
        "that attribute. At the time of execution, data is injected into the intervention\n",
        "graph only at the iteration that matches the call idx.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy5Z9NE1GkaN",
        "outputId": "0b4c1bd3-a6f6-4ba1-9cb8-6019c222311f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction 1:   Paris\n",
            "Prediction 2:  ,\n",
            "Prediction 3:   and\n",
            "All token ids:  tensor([[ 464,  412,  733,  417, 8765,  318,  287,  262, 1748,  286, 6342,   11,\n",
            "          290]])\n",
            "All prediction:  ['The Eiffel Tower is in the city of Paris, and']\n"
          ]
        }
      ],
      "source": [
        "with model.generate(\"The Eiffel Tower is in the city of\", max_new_tokens=3):\n",
        "\n",
        "    token_ids_1 = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "    token_ids_2 = model.lm_head.next().output.argmax(dim=-1).save()\n",
        "\n",
        "    token_ids_3 = model.lm_head.next().output.argmax(dim=-1).save()\n",
        "\n",
        "    output = model.generator.output.save()\n",
        "\n",
        "print(\"Prediction 1: \", model.tokenizer.decode(token_ids_1[0][-1]))\n",
        "print(\"Prediction 2: \", model.tokenizer.decode(token_ids_2[0][-1]))\n",
        "print(\"Prediction 3: \", model.tokenizer.decode(token_ids_3[0][-1]))\n",
        "\n",
        "print(\"All token ids: \", output)\n",
        "\n",
        "print(\"All prediction: \", model.tokenizer.batch_decode(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpkX-LwBQZHo"
      },
      "source": [
        "# 3 I thought you said huge models?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCZR65VmILEb"
      },
      "source": [
        "`NNsight` is only one part of our project to democratize access to AI internals.\n",
        "The other half is `NDIF` (National Deep Inference Facility).\n",
        "\n",
        "The interaction between the two is fairly straightforward. The\n",
        "`intervention graph` we create via the tracing context can be encoded into a\n",
        "custom json format and sent via an http request to the `NDIF` servers. `NDIF`\n",
        "then decodes the `intervention graph` and `interleaves` it alongside the\n",
        "specified model.\n",
        "\n",
        "To see which models are currently being hosted, check out the following status\n",
        "page: https://nnsight.net/status/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Ks1LUvQaER"
      },
      "source": [
        "## Remote execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa-ZuVOFKS5k"
      },
      "source": [
        "In its current state, `NDIF` requires you to receive an API key. Therefore, to\n",
        "run the rest of this colab, you would need one of your own. To get one, simply\n",
        "join the [NDIF discord](https://discord.gg/6uFJmCSwW7) and introduce yourself on\n",
        "the `#introductions` channel. Then DM either @JadenFK or @caden and we'll create\n",
        "one for you.\n",
        "\n",
        "Once you have one, to register your api key with `nnsight`, do the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax1NWoS9MJeZ"
      },
      "outputs": [],
      "source": [
        "from nnsight import CONFIG\n",
        "\n",
        "CONFIG.set_default_api_key(\"<your api key here>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvXLOh65MXmF"
      },
      "source": [
        "This only needs to be run once as it will save this api key as the default in a\n",
        "config file along with the `nnsight` installation.\n",
        "\n",
        "To amp things up a few levels, let's demonstrate using `nnsight`'s tracing\n",
        "context with one of the larger open source language models, `Llama-2-70b`!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1N04sJPZnJt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# llama2 70b is a gated model and you need access via your huggingface token\n",
        "os.environ['HF_TOKEN'] = \"<your huggingface token>\"\n",
        "\n",
        "# llama response object requires the version of transformers from github\n",
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqVzjNoyNGc7"
      },
      "outputs": [],
      "source": [
        "# We'll never actually load the parameters so no need to specify a device_map.\n",
        "model = LanguageModel(\"meta-llama/Llama-2-70b-hf\")\n",
        "\n",
        "# All we need to specify using NDIF vs executing locally is remote=True.\n",
        "with model.trace(\"The Eiffel Tower is in the city of\", remote=True) as runner:\n",
        "\n",
        "    hidden_states = model.model.layers[-1].output.save()\n",
        "\n",
        "    output = model.output.save()\n",
        "\n",
        "print(hidden_states)\n",
        "\n",
        "print(output[\"logits\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IceVpnZcZ9F0"
      },
      "source": [
        "It really is as simple as `remote=True`. All of the techniques we went through\n",
        "in earlier sections work just the same when running locally and remotely.\n",
        "\n",
        "Note that both `nnsight`, but especially `NDIF`, is in active development and\n",
        "therefore there may be caveats, changes, and errors to work through.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3zRm-7VRRov"
      },
      "source": [
        "# Getting Involved!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnCc9xgjvxEP"
      },
      "source": [
        "If you're interested in following updates to `nnsight`, contributing, giving\n",
        "feedback, or finding collaborators, please join the\n",
        "[NDIF discord](https://discord.gg/6uFJmCSwW7)!\n",
        "\n",
        "The [Mech Interp discord](https://discord.gg/km2RQBzaUn) is also a fantastic\n",
        "place to discuss all things mech interp with a really cool community.\n",
        "\n",
        "Our website [nnsight.net](https://nnsight.net/), has a bunch more tutorials\n",
        "detailing more complex interpretability techniques using `nnsight`. If you want\n",
        "to share any of the work you do using `nnsight`, let others know on either of\n",
        "the discords above and we might turn it into a tutorial on our website.\n",
        "\n",
        "💟\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a676b8f2dc34fed9731f19a03e4bfa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74358c341daa44faa41a92b0eb073615",
              "IPY_MODEL_c40e17a994ed44d79ef54421f4772858",
              "IPY_MODEL_431ffcb0dbb1419dbbb237a29842b905"
            ],
            "layout": "IPY_MODEL_c28c185a508e459b95edb29afedd9c79"
          }
        },
        "0fddebfb42e847e08890d09f30161984": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11e99924bdf84739b3cfc7d676b797e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18d4c2302f424bc18f9215f2d5b83393": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a1ada8b122d4502949c4c75861a82bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de63414803f41c29e75242756cfd67f",
            "placeholder": "​",
            "style": "IPY_MODEL_f693dfed91df492c986afbd4ff925f89",
            "value": " 456k/456k [00:00&lt;00:00, 1.87MB/s]"
          }
        },
        "26d09aa072d64af0bf4df9a89309093a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3731a5747724403e86ef1abaf4fb389b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "431ffcb0dbb1419dbbb237a29842b905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562a287082eb4882af864062ae13c91b",
            "placeholder": "​",
            "style": "IPY_MODEL_11e99924bdf84739b3cfc7d676b797e5",
            "value": " 665/665 [00:00&lt;00:00, 12.8kB/s]"
          }
        },
        "562a287082eb4882af864062ae13c91b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56490c20aef04fe4a63ba7c990e71e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60fe026e908741209637604edfeb0563": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7a73cfc3fa04425afc08368532ef0be",
              "IPY_MODEL_d839da1c31a34659988e3420ba520725",
              "IPY_MODEL_96a3b8c765374a5e98eaf713dd5e85d8"
            ],
            "layout": "IPY_MODEL_b49ce0d00e5b4df097b3a9fc4c0bd8e7"
          }
        },
        "688bf676958a4050b73cc9597f5110de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6aa96b80e3b746b48f5c13bcafe1134e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74358c341daa44faa41a92b0eb073615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a0f3d2deeb14bbba7f0df208133a7e2",
            "placeholder": "​",
            "style": "IPY_MODEL_94cdcf1fb0914dfe99c3c615e68b7ec8",
            "value": "config.json: 100%"
          }
        },
        "7a0f3d2deeb14bbba7f0df208133a7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6b41ec535648b190bfc655d9e430dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb4231b49024d8a9bb7ead19647d99d",
            "placeholder": "​",
            "style": "IPY_MODEL_26d09aa072d64af0bf4df9a89309093a",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.16MB/s]"
          }
        },
        "8776d04bfd3748b69e10708466e9c132": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de63414803f41c29e75242756cfd67f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9373fcd054014760b23e3acc928da0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fddebfb42e847e08890d09f30161984",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d07a0a49a2924092affc0e07bf0b5ef0",
            "value": 1355256
          }
        },
        "94cdcf1fb0914dfe99c3c615e68b7ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95d303d83d3348a0adfdf6b5cb94c41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb0c489b913744a28304e5334bb66532",
            "placeholder": "​",
            "style": "IPY_MODEL_c80603555c634e338f16c0021f752420",
            "value": "tokenizer.json: 100%"
          }
        },
        "96a3b8c765374a5e98eaf713dd5e85d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8776d04bfd3748b69e10708466e9c132",
            "placeholder": "​",
            "style": "IPY_MODEL_56490c20aef04fe4a63ba7c990e71e77",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 3.16MB/s]"
          }
        },
        "b2748df9aff540df954f002310606a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b49ce0d00e5b4df097b3a9fc4c0bd8e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7be20bd738940658f710309729d8ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8ad433cc3dd4a8689cb06655158d545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e51620404d244a4e806eeaf3b96a474f",
              "IPY_MODEL_e4568f72971749ac92cc5577cb9c4f9c",
              "IPY_MODEL_1a1ada8b122d4502949c4c75861a82bf"
            ],
            "layout": "IPY_MODEL_18d4c2302f424bc18f9215f2d5b83393"
          }
        },
        "bc8190c700e949ebb6f874c8f8055161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb4231b49024d8a9bb7ead19647d99d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28c185a508e459b95edb29afedd9c79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c40e17a994ed44d79ef54421f4772858": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e24aa6619e4185a3e2f53f2e473e45",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7be20bd738940658f710309729d8ccd",
            "value": 665
          }
        },
        "c80603555c634e338f16c0021f752420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceebda1d07b545f5b605551c08e73be8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef2516aabc24f6d852079984afa8090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95d303d83d3348a0adfdf6b5cb94c41c",
              "IPY_MODEL_9373fcd054014760b23e3acc928da0a5",
              "IPY_MODEL_7e6b41ec535648b190bfc655d9e430dd"
            ],
            "layout": "IPY_MODEL_d0198fa98e5f412380f2ba3e9448aef3"
          }
        },
        "d0198fa98e5f412380f2ba3e9448aef3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07a0a49a2924092affc0e07bf0b5ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d839da1c31a34659988e3420ba520725": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3731a5747724403e86ef1abaf4fb389b",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688bf676958a4050b73cc9597f5110de",
            "value": 1042301
          }
        },
        "e4568f72971749ac92cc5577cb9c4f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceebda1d07b545f5b605551c08e73be8",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6aa96b80e3b746b48f5c13bcafe1134e",
            "value": 456318
          }
        },
        "e51620404d244a4e806eeaf3b96a474f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f28f557210b7402b8b2d0fd83f038dc1",
            "placeholder": "​",
            "style": "IPY_MODEL_bc8190c700e949ebb6f874c8f8055161",
            "value": "merges.txt: 100%"
          }
        },
        "e7a73cfc3fa04425afc08368532ef0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2748df9aff540df954f002310606a56",
            "placeholder": "​",
            "style": "IPY_MODEL_f4caf2bc58704464b6ac7bf6da31b71d",
            "value": "vocab.json: 100%"
          }
        },
        "eb0c489b913744a28304e5334bb66532": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f28f557210b7402b8b2d0fd83f038dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e24aa6619e4185a3e2f53f2e473e45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4caf2bc58704464b6ac7bf6da31b71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f693dfed91df492c986afbd4ff925f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
