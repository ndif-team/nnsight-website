
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" data-theme="dark">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Function Vectors &#8212; nnsight</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "dark";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "dark";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/nbsphinx-code-cells.css?v=2aa19091" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=187304be"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/mini-papers/todd_function_vectors';</script>
    <script src="../../../_static/js/custom.js?v=1e4be224"></script>
    <script src="../../../_static/js/code.js?v=34343d0c"></script>
    <link rel="icon" href="../../../_static/icon.ico"/>
    <link rel="author" title="About these documents" href="../../../about/" />
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="About NNsight" href="../../../about/" />
    <link rel="prev" title="The Geometry of Truth" href="../marks_geometry_of_truth/" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
<link href="../../../_static/css/custom.css?v=1752681933" rel="stylesheet" type="text/css" />
<link href="../../../_static/css/home.css?v=1752681933" rel="stylesheet" type="text/css" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="dark">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../../">
  
  
  
  
  
    
    
    
    <img src="../../../_static/nnsight_logo.svg" class="logo__image only-dark" alt="nnsight - Home"/>
    <img src="../../../_static/nnsight_logo.svg" class="logo__image only-light pst-js-only" alt="nnsight - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../start/">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../documentation/">
    Documentation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../features/">
    Features
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../../tutorials/">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../about/">
    About
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item"><script>
    fetch("https://ndif.dev/ping")
        .then((response) => {
            if (response.status == 200) {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#66800b', 'important');
                    });
                });
            }
            else {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#af3029', 'important');
                    });
                });
                var statusIcon = document.querySelector('.ndif .fa-circle-check');
                if (statusIcon) {
                    // not here
                    statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark'); 
                }
            }
        })
        .catch((response) => {
            Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                    spanElement.style.setProperty('color', '#af3029', 'important');
                });
            });
            var statusIcon = document.querySelector('.ndif .fa-circle-check');
            if (statusIcon) {
                statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark');
            }
        })
</script></div>
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="/status" title="Status: Unknown" class="ndif" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-circle-check fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Status: Unknown</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ndif-team/nnsight" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.ndif.us/" title="Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://forms.gle/1Y6myaXYzSh3oHf56" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../start/">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../documentation/">
    Documentation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../features/">
    Features
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../../tutorials/">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../../about/">
    About
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><script>
    fetch("https://ndif.dev/ping")
        .then((response) => {
            if (response.status == 200) {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#66800b', 'important');
                    });
                });
            }
            else {
                Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                    Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                        spanElement.style.setProperty('color', '#af3029', 'important');
                    });
                });
                var statusIcon = document.querySelector('.ndif .fa-circle-check');
                if (statusIcon) {
                    // not here
                    statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark'); 
                }
            }
        })
        .catch((response) => {
            Array.from(document.getElementsByClassName("ndif")).forEach((ndifElement) => {
                Array.from(ndifElement.getElementsByTagName('span')).forEach((spanElement) => {
                    spanElement.style.setProperty('color', '#af3029', 'important');
                });
            });
            var statusIcon = document.querySelector('.ndif .fa-circle-check');
            if (statusIcon) {
                statusIcon.classList.replace('fa-circle-check', 'fa-circle-xmark');
            }
        })
</script></div>
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="/status" title="Status: Unknown" class="ndif" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-circle-check fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Status: Unknown</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ndif-team/nnsight" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.ndif.us/" title="Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://forms.gle/1Y6myaXYzSh3oHf56" title="Discord" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../walkthroughs/">Main Tutorials</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/get_started/walkthrough/">Walkthrough</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../tutorials/get_started/start_remote_access/">Access LLMs with NDIF and NNsight</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../tutorials/get_started/chat_templates/">Chat Templates</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../tutorials/probing/logit_lens/">Logit Lens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/probing/diffusion_lens/">Diffusion Lens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/steering/dict_learning/">Dictionary Learning</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../tutorials/steering/LoRA_tutorial/">LoRA for Sentiment Analysis</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../tutorials/causal_mediation_analysis/activation_patching/">Activation Patching</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../tutorials/causal_mediation_analysis/attribution_patching/">Attribution Patching</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../tutorials/causal_mediation_analysis/DAS/">DAS</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../../applied_tutorials/">Mini Papers</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../csordas_llm_depth/">Do Language Models Use Their Depth Efficiently?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../marks_geometry_of_truth/">The Geometry of Truth</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Function Vectors</a></li>






</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../tutorials/" class="nav-link">Tutorials</a></li>
    
    
    <li class="breadcrumb-item"><a href="../../../applied_tutorials/" class="nav-link">Mini Paper Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Function Vectors</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="Function-Vectors">
<h1>Function Vectors<a class="headerlink" href="#Function-Vectors" title="Link to this heading">#</a></h1>
<p><strong>ARENA Function Vectors &amp; Model Steering Tutorial</strong></p>
<p>This tutorial is adapted from the ARENA program material and serves as a fantastic introduction to running experiments in NNsight and working with function vectors and model steering. Thanks to Callum McDougall for writing this comprehensive tutorial and for allowing us to adapt the tutorial for NNsight users, and thanks to Eric Todd for writing the original function vector paper!</p>
<blockquote>
<div><p><strong>ARENA:</strong> <a class="reference external" href="https://arena-chapter1-transformer-interp.streamlit.app/22_📚_%5B1.4.2%5D_Function_Vectors_&amp;_Model_Steering">Streamlit Page</a></p>
<p><strong>Colab:</strong> <a class="reference external" href="https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/docs/source/notebooks/tutorials/function_vectors.ipynb">exercises</a> <strong>|</strong> <a class="reference external" href="https://colab.research.google.com/github/ndif-team/nnsight/blob/docs/function_vectors_solutions.ipynb">solutions</a></p>
</div></blockquote>
<p>You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.</p>
<p><img alt="83a03ed198a54a4b9550e62d2068ccba" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-14-2.png" style="width: 350px;" /></p>
</section>
<section id="Introduction">
<h1>Introduction<a class="headerlink" href="#Introduction" title="Link to this heading">#</a></h1>
<p>These exercises serve as an exploration of the following question: <strong>can we steer a model to produce different outputs / have a different behaviour, by intervening on the model’s forward pass using vectors found by non gradient descent-based methods?</strong></p>
<p>The majority of the exercises focus on <a class="reference external" href="https://functions.baulab.info/">function vectors</a>: vectors which are extracted from forward passes on in-context learning (ICL) tasks, and added to the residual stream in order to trigger the execution of this task from a zero-shot prompt. The diagram below illustrates this.</p>
<p><img alt="229e35b9d92c436db4bfc0840fb73a4b" class="no-scaled-link" src="https://functions.baulab.info/images/Paper/fv-demonstrations.png" style="width: 650px;" /></p>
<p>The exercises also take you through use of the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library, which is designed to support this kind of work (and other interpretability research) on very large language models - i.e. larger than models like GPT2-Small which you might be used to at this point in the course.</p>
<p>The final set of exercises look at Alex Turner et al’s work on <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">steering vectors</a>, which is conceptually related but has different aims and methodologies.</p>
<section id="Content-&amp;-Learning-Objectives">
<h2>Content &amp; Learning Objectives<a class="headerlink" href="#Content-&-Learning-Objectives" title="Link to this heading">#</a></h2>
<section id="1️⃣-Introduction-to-nnsight">
<h3>1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code><a class="headerlink" href="#1️⃣-Introduction-to-nnsight" title="Link to this heading">#</a></h3>
<p>In this section, you’ll learn the basics of how to use the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library: running forward passes on your model, and saving the internal states. You’ll also learn some basics of HuggingFace models which translate over into <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models (e.g. tokenization, and how to work with model output).</p>
<blockquote>
<div><p class="rubric" id="learning-objectives">Learning Objectives</p>
<ul class="simple">
<li><p>Learn the basics of the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library, and what it can be useful for</p></li>
<li><p>Learn some basics of HuggingFace models (e.g. tokenization, model output)</p></li>
<li><p>Use it to extract &amp; visualise GPT-J-6B’s internal activations</p></li>
</ul>
</div></blockquote>
</section>
<section id="2️⃣-Task-encoding-hidden-states">
<h3>2️⃣ Task-encoding hidden states<a class="headerlink" href="#2️⃣-Task-encoding-hidden-states" title="Link to this heading">#</a></h3>
<p>We’ll begin with the following question, posed by the Function Vectors paper:</p>
<blockquote>
<div><p><em>When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task :math:`T`, do any hidden states encode the task itself?</em></p>
</div></blockquote>
<p>We’ll prove that the answer is yes, by constructing a vector <span class="math notranslate nohighlight">\(h\)</span> from a set of ICL prompts for the <strong>antonym task</strong>, and intervening with our vector to make our model produce antonyms on zero-shot prompts.</p>
<p>This will require you to learn how to perform causal interventions with <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, not just save activations.</p>
<p>(Note - this section structurally follows section 2.1 of the function vectors paper).</p>
<blockquote>
<div><p class="rubric" id="learning-objectives-1">Learning Objectives</p>
<ul class="simple">
<li><p>Understand how <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> can be used to perform causal interventions, and perform some yourself</p></li>
<li><p>Reproduce the “h-vector results” from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts</p></li>
</ul>
</div></blockquote>
</section>
<section id="3️⃣-Function-Vectors">
<h3>3️⃣ Function Vectors<a class="headerlink" href="#3️⃣-Function-Vectors" title="Link to this heading">#</a></h3>
<p>In this section, we’ll replicate the crux of the paper’s results, by identifying a set of attention heads whose outputs have a large effect on the model’s ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.</p>
<p>We’ll also learn how to use <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation, and steer the model’s behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you’ll be able to steer the model to complete prompts like <code class="docutils literal notranslate"><span class="pre">&quot;When</span> <span class="pre">you</span> <span class="pre">think</span> <span class="pre">of</span> <span class="pre">Netherlands,</span> <span class="pre">you</span> <span class="pre">usually</span> <span class="pre">think</span> <span class="pre">of&quot;</span></code> by talking about Amsterdam.</p>
<p>(Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper).</p>
<blockquote>
<div><p class="rubric" id="learning-objectives-2">Learning Objectives</p>
<ul class="simple">
<li><p>Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task</p></li>
<li><p>Understand how to rearrange activations in a model during an <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> forward pass, to extract activations corresponding to a particular attention head</p></li>
<li><p>Learn how to use <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation</p></li>
</ul>
</div></blockquote>
</section>
<section id="4️⃣-Steering-Vectors-in-GPT2-XL">
<h3>4️⃣ Steering Vectors in GPT2-XL<a class="headerlink" href="#4️⃣-Steering-Vectors-in-GPT2-XL" title="Link to this heading">#</a></h3>
<p>Here, we discuss a different but related set of research: Alex Turner’s work on steering vectors. This also falls under the umbrella of “interventions in the residual stream using vectors found with forward pass (non-SGD) based methods in order to alter behaviour”, but it has a different setup, objectives, and approach.</p>
<blockquote>
<div><p class="rubric" id="learning-objectives-3">Learning Objectives</p>
<ul class="simple">
<li><p>Understand the goals &amp; main results from Alex Turner et al’s work on steering vectors</p></li>
<li><p>Reproduce the changes in behaviour described in their initial post</p></li>
</ul>
</div></blockquote>
</section>
<section id="☆-Bonus">
<h3>☆ Bonus<a class="headerlink" href="#☆-Bonus" title="Link to this heading">#</a></h3>
<p>Lastly, we discuss some possible extensions of function vectors &amp; steering vectors work, which is currently an exciting area of development (e.g. with a paper on steering Llama 2-13b coming out as recently as December 2023).</p>
</section>
</section>
<section id="Setup-code">
<h2>Setup code<a class="headerlink" href="#Setup-code" title="Link to this heading">#</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="n">IN_COLAB</span> <span class="o">=</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span>

<span class="n">chapter</span> <span class="o">=</span> <span class="s2">&quot;chapter1_transformer_interp&quot;</span>
<span class="n">repo</span> <span class="o">=</span> <span class="s2">&quot;ARENA_3.0&quot;</span>
<span class="n">branch</span> <span class="o">=</span> <span class="s2">&quot;main&quot;</span>

<span class="c1"># Install dependencies</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">nnsight</span>
<span class="k">except</span><span class="p">:</span>
    <span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">openai</span><span class="o">&gt;=</span><span class="mf">1.56.2</span> <span class="n">nnsight</span> <span class="n">einops</span> <span class="n">jaxtyping</span> <span class="n">plotly</span> <span class="n">transformer_lens</span><span class="o">==</span><span class="mf">2.11.0</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">callummcdougall</span><span class="o">/</span><span class="n">CircuitsVis</span><span class="o">.</span><span class="n">git</span><span class="c1">#subdirectory=python gradio typing-extensions</span>
    <span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">pydantic</span>

<span class="c1"># Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo</span>
<span class="n">root</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;/content&quot;</span>
    <span class="k">if</span> <span class="n">IN_COLAB</span>
    <span class="k">else</span> <span class="s2">&quot;/root&quot;</span>
    <span class="k">if</span> <span class="n">repo</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
    <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span><span class="o">.</span><span class="n">parents</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">repo</span><span class="p">))</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">Path</span><span class="p">(</span><span class="n">root</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">Path</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">root</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">chapter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">IN_COLAB</span><span class="p">:</span>
        <span class="err">!</span><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">unzip</span>
        <span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">jupyter</span> <span class="n">ipython</span> <span class="o">--</span><span class="n">upgrade</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">root</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">chapter</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">):</span>
        <span class="err">!</span><span class="n">wget</span> <span class="o">-</span><span class="n">P</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">callummcdougall</span><span class="o">/</span><span class="n">ARENA_3</span><span class="mf">.0</span><span class="o">/</span><span class="n">archive</span><span class="o">/</span><span class="n">refs</span><span class="o">/</span><span class="n">heads</span><span class="o">/</span><span class="p">{</span><span class="n">branch</span><span class="p">}</span><span class="o">.</span><span class="n">zip</span>
        <span class="err">!</span><span class="n">unzip</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">branch</span><span class="p">}</span><span class="o">.</span><span class="n">zip</span> <span class="s1">&#39;</span><span class="si">{repo}</span><span class="s1">-</span><span class="si">{branch}</span><span class="s1">/</span><span class="si">{chapter}</span><span class="s1">/exercises/*&#39;</span> <span class="o">-</span><span class="n">d</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span>
        <span class="err">!</span><span class="n">mv</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">repo</span><span class="p">}</span><span class="o">-</span><span class="p">{</span><span class="n">branch</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">chapter</span><span class="p">}</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">chapter</span><span class="p">}</span>
        <span class="err">!</span><span class="n">rm</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">branch</span><span class="p">}</span><span class="o">.</span><span class="n">zip</span>
        <span class="err">!</span><span class="n">rmdir</span> <span class="p">{</span><span class="n">root</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">repo</span><span class="p">}</span><span class="o">-</span><span class="p">{</span><span class="n">branch</span><span class="p">}</span>


<span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">root</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">chapter</span><span class="si">}</span><span class="s2">/exercises&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">root</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">chapter</span><span class="si">}</span><span class="s2">/exercises&quot;</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">root</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">chapter</span><span class="si">}</span><span class="s2">/exercises&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">circuitsvis</span>
<span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">plotly</span>
<span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">jaxtyping</span>
<span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">nnsight</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">circuitsvis</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">cv</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">einops</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">t</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jaxtyping</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nnsight</span><span class="w"> </span><span class="kn">import</span> <span class="n">CONFIG</span><span class="p">,</span> <span class="n">LanguageModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rich</span><span class="w"> </span><span class="kn">import</span> <span class="nb">print</span> <span class="k">as</span> <span class="n">rprint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rich.table</span><span class="w"> </span><span class="kn">import</span> <span class="n">Table</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="c1"># Hide some info logging messages from nnsight</span>
<span class="n">logging</span><span class="o">.</span><span class="n">disable</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">)</span>

<span class="n">t</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Make sure exercises are in the path</span>
<span class="n">chapter</span> <span class="o">=</span> <span class="s2">&quot;chapter1_transformer_interp&quot;</span>
<span class="n">section</span> <span class="o">=</span> <span class="s2">&quot;part42_function_vectors_and_model_steering&quot;</span>
<span class="n">root_dir</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span><span class="o">.</span><span class="n">parents</span> <span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="n">chapter</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">())</span>
<span class="n">exercises_dir</span> <span class="o">=</span> <span class="n">root_dir</span> <span class="o">/</span> <span class="n">chapter</span> <span class="o">/</span> <span class="s2">&quot;exercises&quot;</span>
<span class="n">section_dir</span> <span class="o">=</span> <span class="n">exercises_dir</span> <span class="o">/</span> <span class="n">section</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">part42_function_vectors_and_model_steering.solutions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">solutions</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">part42_function_vectors_and_model_steering.tests</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">plotly_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">imshow</span>

<span class="n">MAIN</span> <span class="o">=</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="id2">
<h1>1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code><a class="headerlink" href="#id2" title="Link to this heading">#</a></h1>
<blockquote>
<div><p class="rubric" id="id3">Learning Objectives</p>
<ul class="simple">
<li><p>Learn the basics of the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library, and what it can be useful for</p></li>
<li><p>Learn some basics of HuggingFace models (e.g. tokenization, model output)</p></li>
<li><p>Use it to extract &amp; visualise GPT-J-6B’s internal activations</p></li>
</ul>
</div></blockquote>
<section id="Remote-execution">
<h2>Remote execution<a class="headerlink" href="#Remote-execution" title="Link to this heading">#</a></h2>
<p>We’ll start by discussing <a class="reference external" href="(https://nnsight.net/notebooks/features/remote_execution/)">remote execution</a> - the ability <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> has to run models on an external server, which is one of the major benefits of the library as a research tool. This helps you bypass the memory &amp; computational limits you might be faced with on your own machine. For remote execution to work, you need 2 things:</p>
<ol class="arabic simple">
<li><p>An API key from the NDIF login page, which you can request <a class="reference external" href="https://login.ndif.us/">here</a></p></li>
<li><p>The model you’re working with being live - you can see all live models in the status page <a class="reference external" href="https://nnsight.net/status/">here</a></p></li>
</ol>
<p>Note that the status page sometimes takes ~5 minutes to load all live models - click the dropdown below to see an example of what the status page should look like once the models have loaded. If you can’t see the model you’re looking for in this list, then you should set <code class="docutils literal notranslate"><span class="pre">REMOTE=False</span></code> for these exercises, or else make a request on the NDIF Discord to get the model live.</p>
<details><summary><p>Example status page</p>
</summary><p><img alt="960a7122b0af4345ae21c2c5a9171427" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ndif-status.png" style="width: 650px;" /></p>
</details></section>
<section id="Important-syntax">
<h2>Important syntax<a class="headerlink" href="#Important-syntax" title="Link to this heading">#</a></h2>
<p>Here, we’ll discuss some important syntax for interacting with <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models. Since these models are extensions of HuggingFace models, some of this information (e.g. tokenization) applies to plain HuggingFace models as well as <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> models, and some of it (e.g. forward passes) is specific to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, i.e. it would work differently if you just had a standard HuggingFace model. Make sure to keep this distinction in mind, otherwise syntax can get confusing!</p>
<section id="Model-config">
<h3>Model config<a class="headerlink" href="#Model-config" title="Link to this heading">#</a></h3>
<p>Each model comes with a <code class="docutils literal notranslate"><span class="pre">model.config</span></code>, which contains lots of useful information about the model (e.g. number of heads and layers, size of hidden layers, etc.). You can access this with <code class="docutils literal notranslate"><span class="pre">model.config</span></code>. Run the code below to see this in action, and to define some useful variables for later.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s2">&quot;EleutherAI/gpt-j-6b&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span>

<span class="n">N_HEADS</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_head</span>
<span class="n">N_LAYERS</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span>
<span class="n">D_MODEL</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_embd</span>
<span class="n">D_HEAD</span> <span class="o">=</span> <span class="n">D_MODEL</span> <span class="o">//</span> <span class="n">N_HEADS</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of heads: </span><span class="si">{</span><span class="n">N_HEADS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of layers: </span><span class="si">{</span><span class="n">N_LAYERS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model dimension: </span><span class="si">{</span><span class="n">D_MODEL</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Head dimension: </span><span class="si">{</span><span class="n">D_HEAD</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entire config: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9b311d130b044025bfe4ba34b08c4b58", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ce647b9b168c46749bd5400f608d1f29", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1e9575b528de43f2a2282b66a3716bd1", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2b9b778defd54ebbb35c362fa1914cc2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6f231bf805014279b74b5c15cc403b0f", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "38fb4dd1106047afb5669a99732411ce", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d040f3c84b3f472f9085af62cd2a338e", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of heads: 16
Number of layers: 28
Model dimension: 4096
Head dimension: 256

Entire config:  GPTJConfig {
  &#34;activation_function&#34;: &#34;gelu_new&#34;,
  &#34;architectures&#34;: [
    &#34;GPTJForCausalLM&#34;
  ],
  &#34;attn_pdrop&#34;: 0.0,
  &#34;bos_token_id&#34;: 50256,
  &#34;embd_pdrop&#34;: 0.0,
  &#34;eos_token_id&#34;: 50256,
  &#34;gradient_checkpointing&#34;: false,
  &#34;initializer_range&#34;: 0.02,
  &#34;layer_norm_epsilon&#34;: 1e-05,
  &#34;model_type&#34;: &#34;gptj&#34;,
  &#34;n_embd&#34;: 4096,
  &#34;n_head&#34;: 16,
  &#34;n_inner&#34;: null,
  &#34;n_layer&#34;: 28,
  &#34;n_positions&#34;: 2048,
  &#34;resid_pdrop&#34;: 0.0,
  &#34;rotary&#34;: true,
  &#34;rotary_dim&#34;: 64,
  &#34;scale_attn_weights&#34;: true,
  &#34;summary_activation&#34;: null,
  &#34;summary_first_dropout&#34;: 0.1,
  &#34;summary_proj_to_labels&#34;: true,
  &#34;summary_type&#34;: &#34;cls_index&#34;,
  &#34;summary_use_proj&#34;: true,
  &#34;task_specific_params&#34;: {
    &#34;text-generation&#34;: {
      &#34;do_sample&#34;: true,
      &#34;max_length&#34;: 50,
      &#34;temperature&#34;: 1.0
    }
  },
  &#34;tie_word_embeddings&#34;: false,
  &#34;tokenizer_class&#34;: &#34;GPT2Tokenizer&#34;,
  &#34;torch_dtype&#34;: &#34;bfloat16&#34;,
  &#34;transformers_version&#34;: &#34;4.51.3&#34;,
  &#34;use_cache&#34;: true,
  &#34;vocab_size&#34;: 50400
}

</pre></div></div>
</div>
</section>
<section id="Tokenizers">
<h3>Tokenizers<a class="headerlink" href="#Tokenizers" title="Link to this heading">#</a></h3>
<p>A model comes with a tokenizer, accessable with <code class="docutils literal notranslate"><span class="pre">model.tokenizer</span></code> (just like TransformerLens). Unlike TransformerLens, we won’t be using utility functions like <code class="docutils literal notranslate"><span class="pre">model.to_str_tokens</span></code>, instead we’ll be using the tokenizer directly. Some important functions for today’s exercises are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> (i.e. just calling it on some input)</p>
<ul>
<li><p>This takes in a string (or list of strings) and returns the tokenized version.</p></li>
<li><p>It will return a dictionary, always containing <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> (i.e. the actual tokens) but also other things which are specific to the transformer model (e.g. <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> - see dropdown).</p></li>
<li><p>Other useful arguments for this function:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">return_tensors</span></code> - if this is <code class="docutils literal notranslate"><span class="pre">&quot;pt&quot;</span></code>, you’ll get results returned as PyTorch tensors, rather than lists (which is the default).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding</span></code> - if True (default is False), the tokenizer can accept sequences of variable length. The shorter sequences get padded at the beginning (see dropdown below for more).</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.decode</span></code></p>
<ul>
<li><p>This takes in tokens, and returns the decoded string.</p></li>
<li><p>If the input is an integer, it returns the corresponding string. If the input is a list / 1D array of integers, it returns all those strings concatenated (which can sometimes not be what you want).</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.batch_decode</span></code></p>
<ul>
<li><p>Equivalent to <code class="docutils literal notranslate"><span class="pre">tokenizer.decode</span></code>, but it doesn’t concatenate.</p></li>
<li><p>If the input is a list / 1D integer array, it returns a list of strings. If the input is 2D, it will concatenate within each list.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.tokenize</span></code></p>
<ul>
<li><p>Takes in a string, and returns a list of strings.</p></li>
</ul>
</li>
</ul>
<p>Run the code below to see some examples of these functions in action.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calling tokenizer returns a dictionary, containing input ids &amp; other data.</span>
<span class="c1"># If returned as a tensor, then by default it will have a batch dimension.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This must be Thursday&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">))</span>

<span class="c1"># Decoding a list of integers, into a concatenated string.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">40</span><span class="p">,</span> <span class="mi">1239</span><span class="p">,</span> <span class="mi">714</span><span class="p">,</span> <span class="mi">651</span><span class="p">,</span> <span class="mi">262</span><span class="p">,</span> <span class="mi">8181</span><span class="p">,</span> <span class="mi">286</span><span class="p">,</span> <span class="mi">48971</span><span class="p">,</span> <span class="mi">12545</span><span class="p">,</span> <span class="mi">13</span><span class="p">]))</span>

<span class="c1"># Using batch decode, on both 1D and 2D input.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">([</span><span class="mi">4711</span><span class="p">,</span> <span class="mi">2456</span><span class="p">,</span> <span class="mi">481</span><span class="p">,</span> <span class="mi">307</span><span class="p">,</span> <span class="mi">6626</span><span class="p">,</span> <span class="mi">510</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">([[</span><span class="mi">1212</span><span class="p">,</span> <span class="mi">6827</span><span class="p">,</span> <span class="mi">481</span><span class="p">,</span> <span class="mi">307</span><span class="p">,</span> <span class="mi">1978</span><span class="p">],</span> <span class="p">[</span><span class="mi">2396</span><span class="p">,</span> <span class="mi">481</span><span class="p">,</span> <span class="mi">428</span><span class="p">,</span> <span class="mi">530</span><span class="p">]]))</span>

<span class="c1"># Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;This sentence will be tokenized&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;input_ids&#39;: tensor([[1212, 1276,  307, 3635]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1]])}
I never could get the hang of Thursdays.
[&#39;These&#39;, &#39; words&#39;, &#39; will&#39;, &#39; be&#39;, &#39; split&#39;, &#39; up&#39;]
[&#39;This sentence will be together&#39;, &#39;So will this one&#39;]
[&#39;This&#39;, &#39;Ġsentence&#39;, &#39;Ġwill&#39;, &#39;Ġbe&#39;, &#39;Ġtoken&#39;, &#39;ized&#39;]
</pre></div></div>
</div>
<details><summary><p>Note on attention_mask (optional)</p>
</summary><p><code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>, which is a series of 1s and 0s. We mask attention at all 0-positions (i.e. we don’t allow these tokens to be attended to). This is useful when you have to do padding. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Hello world&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>will return:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span>
    <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">15496</span><span class="p">,</span>   <span class="mi">995</span><span class="p">],</span> <span class="p">[</span><span class="mi">50256</span><span class="p">,</span> <span class="mi">15496</span><span class="p">]])</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We can see how the shorter sequence has been padded at the beginning, and attention to this token will be masked.</p>
</details></section>
<section id="Model-outputs">
<h3>Model outputs<a class="headerlink" href="#Model-outputs" title="Link to this heading">#</a></h3>
<p>At a high level, there are 2 ways to run our model: using the <code class="docutils literal notranslate"><span class="pre">trace</span></code> method (a single forward pass) and the <code class="docutils literal notranslate"><span class="pre">generate</span></code> method (generating multiple tokens). We’ll focus on <code class="docutils literal notranslate"><span class="pre">trace</span></code> for now, and we’ll discuss <code class="docutils literal notranslate"><span class="pre">generate</span></code> when it comes to multi-token generation later.</p>
<p>The default behaviour of forward passes in normal HuggingFace models is to return an object containing logits (and optionally a bunch of other things). The default behaviour of <code class="docutils literal notranslate"><span class="pre">trace</span></code> in <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> is to not return anything, because anything that we choose to return is explicitly returned inside the context manager.</p>
<p>Below is the simplest example of code to run the model (and also access the internal states of the model). Run it and look at the output, then read the explanation below. Remember to obtain and set an API key first if you’re using remote execution!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">REMOTE</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">if</span> <span class="n">IN_COLAB</span><span class="p">:</span>
    <span class="c1"># include your HuggingFace Token and NNsight API key on Colab secrets</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">google.colab</span><span class="w"> </span><span class="kn">import</span> <span class="n">userdata</span>
    <span class="n">NDI</span>  <span class="n">F_API</span> <span class="o">=</span> <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;NDIF_API&#39;</span><span class="p">)</span>
    <span class="n">CONFIG</span><span class="o">.</span><span class="n">set_default_api_key</span><span class="p">(</span><span class="n">NDIF_API</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;The Eiffel Tower is in the city of&quot;</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
    <span class="c1"># Save the model&#39;s hidden states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="c1"># Save the model&#39;s logit output</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Get the model&#39;s logit output, and it&#39;s next token prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;logits.shape = </span><span class="si">{</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> = (vocab_size,)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted token ID =&quot;</span><span class="p">,</span> <span class="n">predicted_token_id</span> <span class="o">:=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted token = </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predicted_token_id</span><span class="p">)</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Print the shape of the model&#39;s residual stream</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">resid.shape = </span><span class="si">{</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> = (batch_size, seq_len, d_model)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d7be60815f534f098018a1b1c9e384e2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
logits.shape = torch.Size([50400]) = (vocab_size,)
Predicted token ID = 6342
Predicted token = &#39; Paris&#39;

resid.shape = torch.Size([1, 10, 4096]) = (batch_size, seq_len, d_model)
</pre></div></div>
</div>
<p>Lets go over this piece by piece.</p>
<p><strong>First, we create a context block</strong> by calling <code class="docutils literal notranslate"><span class="pre">.trace(...)</span></code> on the model object. This denotes that we wish to generate tokens given some prompts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
</pre></div>
</div>
<p>By default, running this will cause your model to be loaded &amp; run locally, but by passing <code class="docutils literal notranslate"><span class="pre">remote=REMOTE</span></code>, it causes the model to be run on the server instead. This is very useful when working with models too large to fit on your machine (or even models which can fit on your machine, but run slowly due to their size, however if you’re running this material on a sufficiently large GPU, you may prefer to set <code class="docutils literal notranslate"><span class="pre">REMOTE=False</span></code>). The input argument can take a variety of formats: strings, lists of
tokens, tensors of tokens, etc. Here, we’ve just used a string <code class="docutils literal notranslate"><span class="pre">prompt</span></code>.</p>
<p>The most interesting part of <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> is the ability to access the model’s internal states (like you might already have done with TransformerLens). Let’s now see how this works!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>On this line we’re saying: within our forward pass, access the last layer of the transformer <code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code>, access this layer’s output <code class="docutils literal notranslate"><span class="pre">.output</span></code> (which is a tuple of tensors), index the first tensor in this tuple <code class="docutils literal notranslate"><span class="pre">.output[0]</span></code>.</p>
<p>Let’s break down this line in a bit more detail:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model.transformer.h[-1]</span></code> is a module in our transformer.</p>
<ul>
<li><p>If you <code class="docutils literal notranslate"><span class="pre">print(model)</span></code>, you’ll see that it consists of <code class="docutils literal notranslate"><span class="pre">transformer</span></code> and <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> (for “language modelling head”). The <code class="docutils literal notranslate"><span class="pre">transformer</span></code> module is made up of embeddings &amp; dropout, a series of layers (called <code class="docutils literal notranslate"><span class="pre">.h</span></code>, for “hidden states”), and a final layernorm. So indexing <code class="docutils literal notranslate"><span class="pre">.h[-1]</span></code> gives you the final layer.</p></li>
<li><p>Note - it’s often useful to visit the documentation page for whatever model you’re working on, e.g. you can find GPT-J <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">here</a>. Not all models will have a nice uniform standardized architecture like you might be used to in TransformerLens!</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">.output[0]</span></code> gives you this module’s output, as a <strong>proxy</strong>.</p>
<ul>
<li><p>The output of a module is often a tuple (again, you can see on the <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">documentation page</a> what the output of each module is). In this case, it’s a tuple of 2 tensors, the first of which is the actual layer output (the thing we want).</p></li>
<li><p>Doing operations on a proxy still returns a proxy - this is why we can index into the <code class="docutils literal notranslate"><span class="pre">output</span></code> proxy tuple and get a proxy tensor!</p></li>
</ul>
</li>
</ul>
<details><summary><p>Optional exercise - we mentioned that .output returns a tuple of 2 tensors. Can you use the documentation page what the second tensor in this tuple is?</p>
</summary><p>The second output is also a tuple of tensors, of length 2. In the GPT-J source code, they are called <code class="docutils literal notranslate"><span class="pre">present</span></code>. They represent the keys and values which were calculated in this forward pass (as opposed to those that were calculated in an earlier forward pass, and cached by the model). Since we’re only generating one new token, these are just the full keys and values.</p>
</details><p>The next command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>can be understood in a very similar way. The only difference is that we’re accessing the output of <code class="docutils literal notranslate"><span class="pre">lm_head</span></code>, the language modelling head (i.e. the unembedding at the very end), and the output is just a tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">d_vocab)</span></code> rather than a tuple of tensors. Again, see the <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">documentation page</a> for this.</p>
<p>If you’ve worked with Hugging Face models then you might be used to getting logits directly from the model output, but here we generally extract logits from the model internals just like any other activation because this allows us to <strong>control exactly what we return.</strong> If we return lots of very large tensors, this can take quite a while to download from the server (remember that <code class="docutils literal notranslate"><span class="pre">d_vocab</span></code> is often very large for transformers, i.e. around 50k). See the “which objects to save” section below for
more discussion on this.</p>
</section>
<section id="Output-vs-input">
<h3>Output vs input<a class="headerlink" href="#Output-vs-input" title="Link to this heading">#</a></h3>
<p>You can also extract a module’s input using <code class="docutils literal notranslate"><span class="pre">.input</span></code> or <code class="docutils literal notranslate"><span class="pre">.inputs</span></code>. If a module’s forward method is called as <code class="docutils literal notranslate"><span class="pre">module.forward(*args,</span> <span class="pre">**kwargs)</span></code> then <code class="docutils literal notranslate"><span class="pre">.inputs</span></code> returns a tuple of <code class="docutils literal notranslate"><span class="pre">(tuple_of_args,</span> <span class="pre">dict_of_kwargs)</span></code>. Alternatively, <code class="docutils literal notranslate"><span class="pre">.input</span></code> is an alias for <code class="docutils literal notranslate"><span class="pre">.inputs[0][0]</span></code>, in other words it returns the first arg from the module’s forward method (which is usually the tensor we want).</p>
<p>Remember that if you’re not sure then you can debug with <code class="docutils literal notranslate"><span class="pre">print(module.input.shape)</span></code> - even if <code class="docutils literal notranslate"><span class="pre">.inputs</span></code> is a tuple of inputs, this will work to recursively print the shape of all the tensors in the tuple, rather than causing an error.</p>
</section>
<section id="Which-objects-to-save">
<h3>Which objects to save<a class="headerlink" href="#Which-objects-to-save" title="Link to this heading">#</a></h3>
<p>Note that we saved <code class="docutils literal notranslate"><span class="pre">logits</span></code> above, which is a vector of length 50k. In general, it’s best to save as small an object as possible, because this reduces the size of object you’ll have to download from the server. For example, if you only want the next token completions, just argmax the logits and then save the result! All basic tensor operations can be performed within your context manager.</p>
</section>
</section>
<section id="Scan-&amp;-Validate">
<h2>Scan &amp; Validate<a class="headerlink" href="#Scan-&-Validate" title="Link to this heading">#</a></h2>
<p>A really cool feature in nnsight is the scan &amp; validate mode, which allows you to efficiently debug without getting long uninterpretable error messages. For example, consider the code below, which tries to zero ablate one of the model’s output tensors. Can you figure out what’s wrong with it before running it?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">seq_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span>

<span class="k">try</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
        <span class="n">original_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="n">seq_len</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">modified_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Uninformative error message:</span><span class="se">\n</span><span class="s2">  </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Uninformative error message:
  NNsightError: index 10 is out of bounds for dimension 1 with size 10
</pre></div></div>
</div>
<p>If you guessed “we’re indexing a tensor along a dimension of size <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> with index <code class="docutils literal notranslate"><span class="pre">seq_len</span></code> which is an indexing error, you’d be correct! But the error message we get is pretty opaque. This is because of the way the objects in nnsight work: they’re not tensors, they’re tensor proxies, and can behave in funny ways sometimes.</p>
<p>If we want to debug, we should instead pass <code class="docutils literal notranslate"><span class="pre">scan=True</span></code> and <code class="docutils literal notranslate"><span class="pre">validate=True</span></code> into our <code class="docutils literal notranslate"><span class="pre">model.trace</span></code> call. <code class="docutils literal notranslate"><span class="pre">scan=True</span></code> means we run “fake inputs” through the model which incur no memory costs, and so can be done very quickly and cheaply to detect errors. <code class="docutils literal notranslate"><span class="pre">validate=True</span></code> will run tests during our forward pass that make our error messages more informative. When we use both, we get fast no-memory-cost operations with interpretable error messages!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">,</span> <span class="n">scan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">original_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">=}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="n">seq_len</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">modified_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Informative error message:</span><span class="se">\n</span><span class="s2">  </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model.transformer.h[-1].output.shape=(torch.Size([1, 10, 4096]), &lt;transformers.cache_utils.DynamicCache object at 0x7edbe8693e50&gt;)

Informative error message:
  IndexError: index 10 is out of bounds for dimension 1 with size 10
</pre></div></div>
</div>
<p>It’s possible to use <code class="docutils literal notranslate"><span class="pre">validate</span></code> without using <code class="docutils literal notranslate"><span class="pre">scan</span></code> (e.g. if you have any <code class="docutils literal notranslate"><span class="pre">assert</span> <span class="pre">proxy.shape</span> <span class="pre">==</span> <span class="pre">...</span></code> then you must use <code class="docutils literal notranslate"><span class="pre">validate=True</span></code>), although we generally recommend using both when debugging, and then neither when you’re finished debugging.</p>
<p>Also note that (as the example above shows) it’s useful to use <code class="docutils literal notranslate"><span class="pre">scan=True,</span> <span class="pre">validate=True</span></code> when printing tensor shapes, at the initial exploration phase, if you’re not exactly sure what the shape of a particular input / output will be. Even if your proxy objects are tuples of tensors, you can still call <code class="docutils literal notranslate"><span class="pre">.shape</span></code>, and it will return a tuple of the shapes of each tensor in the proxy!</p>
</section>
<section id="Putting-this-into-practice">
<h2>Putting this into practice<a class="headerlink" href="#Putting-this-into-practice" title="Link to this heading">#</a></h2>
<section id="Exercise---visualize-attention-heads">
<h3>Exercise - visualize attention heads<a class="headerlink" href="#Exercise---visualize-attention-heads" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴⚪⚪⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 10-20 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>We just covered a lot of content, so lets put it into practice. Your first task is to extract the attention patterns from the zeroth layer of the transformer, and visualize them using circuitsvis. As a reminder, the syntax for circuitsvis is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span>
    <span class="n">tokens</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">tokens</span></code> is a list of strings, and <code class="docutils literal notranslate"><span class="pre">attention</span></code> is a tensor of shape <code class="docutils literal notranslate"><span class="pre">(num_heads,</span> <span class="pre">num_tokens,</span> <span class="pre">num_tokens)</span></code>.</p>
<p>If you’re stuck, <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">here’s a link</a> to the source code for GPT-J. Look for how the attention patterns are calculated, within the <code class="docutils literal notranslate"><span class="pre">GPTJAttention</span></code> block.</p>
<p><em>Note - this model uses dropout on the attention probabilities, as you’ll probably notice from looking at the source code in the link above. This won’t affect the model’s behaviour because dropout is disabled in inference mode (and using the ``generate`` method always puts a model in inference mode). But it is still a layer which exists in the model, so you can access its input or output just like any other module.</em></p>
<details><summary><p>Aside - inference mode</p>
</summary><p>Dropout is one of the two main layers whose behaviour changes in inference mode (the other is BatchNorm).</p>
<p>If you want to run the model without inference mode, you can wrap your code in <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">model.trace(inference=False):</span></code>. However, you don’t need to worry about this for the purposes of these exercises.</p>
</details><p>If you’re stuck on how to reference the right module, see the following hint:</p>
<details><summary><p>Hint - what module you should get attention from</p>
</summary><p>You want to extract attention from <code class="docutils literal notranslate"><span class="pre">model.transformer.h[0].attn.attn_dropout.input</span></code>. If you used <code class="docutils literal notranslate"><span class="pre">.output</span></code>, it would give you the same values (although they might differ by a dummy batch dimension). Both of these will return a single tensor, because dropout layers take just one input and return just one output.</p>
</details><details><summary><p>Aside - GPT2 tokenizer uses special characters to represent space</p>
</summary><p>GPT2 tokenizer uses “Ġ” to represent prepended space. So [“My”, “ name”, “ is”, “ James”] will be tokenized as [“My”, “Ġname”, “Ġis”, “ĠJames”]. Make sure you replace “Ġ” with an actual space.</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># YOUR CODE HERE - extract and visualize attention</span>
</pre></div>
</div>
</div>
<details><summary><p>Solution (and explanation)</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
    <span class="n">attn_patterns</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn_dropout</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Get string tokens (replacing special character for spaces)</span>
<span class="n">str_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="n">str_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Ġ&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">str_tokens</span><span class="p">]</span>

<span class="c1"># Attention patterns (squeeze out the batch dimension)</span>
<span class="n">attn_patterns_value</span> <span class="o">=</span> <span class="n">attn_patterns</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Layer 0 Head Attention Patterns:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">attention_patterns</span><span class="p">(</span>
    <span class="n">tokens</span><span class="o">=</span><span class="n">str_tokens</span><span class="p">,</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">attn_patterns_value</span><span class="p">,</span>
<span class="p">))</span>
</pre></div>
</div>
<p>Explanation:</p>
<ul class="simple">
<li><p>Within the context managers:</p>
<ul>
<li><p>We access the attention patterns by taking the input to the <code class="docutils literal notranslate"><span class="pre">attn_dropout</span></code>.</p>
<ul>
<li><p>From the GPT-J source code, we can see that the attention weights are calculated by standard torch functions (and an unnamed <code class="docutils literal notranslate"><span class="pre">nn.Softmax</span></code> module) from the key and query vectors, and are then passed through the dropout layer before being used to calculate the attention layer output. So by accessing the input to the dropdown layer, we get the attention weights before dropout is applied.</p></li>
<li><p>Because of the previously discussed point about dropout not working in inference mode, we could also use the output of <code class="docutils literal notranslate"><span class="pre">attn_dropout</span></code>, and get the same values.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Outside of the context managers:</p>
<ul>
<li><p>We use the <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> method to tokenize the prompt.</p></li>
</ul>
</li>
</ul>
</details><p>As an optional bonus exercise, you can verify for yourself that these are the correct attention patterns, by calculating them from scratch using the key and query vectors. Using <code class="docutils literal notranslate"><span class="pre">model.transformer.h[0].attn.q_proj.output</span></code> will give you the query vectors, and <code class="docutils literal notranslate"><span class="pre">k_proj</span></code> for the key vectors. However, one thing to be wary of is that GPT-J uses <strong>rotary embeddings</strong>, which makes the computation of attention patterns from keys and queries a bit harder than it would otherwise be. See
<a class="reference external" href="https://blog.eleuther.ai/rotary-embeddings/">here</a> for an in-depth discussion of rotary embeddings, and <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bef36Bf9k7FYsCt1DpzCw6eV">here</a> for some rough intuitions.</p>
</section>
</section>
</section>
<section id="id4">
<h1>2️⃣ Task-encoding hidden states<a class="headerlink" href="#id4" title="Link to this heading">#</a></h1>
<blockquote>
<div><p class="rubric" id="id5">Learning Objectives</p>
<ul class="simple">
<li><p>Understand how <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> can be used to perform causal interventions, and perform some yourself</p></li>
<li><p>Reproduce the “h-vector results” from the function vectors paper; that the residual stream does contain a vector which encodes the task and can induce task behaviour on zero-shot prompts</p></li>
</ul>
</div></blockquote>
<p>We’ll begin with the following question, posed by the Function Vectors paper:</p>
<blockquote>
<div><p><em>When a transformer processes an ICL (in-context-learning) prompt with exemplars demonstrating task :math:`T`, do any hidden states encode the task itself?</em></p>
</div></blockquote>
<p>We’ll prove that the answer is yes, by constructing a vector <span class="math notranslate nohighlight">\(h\)</span> from a set of ICL prompts for the <strong>antonym task</strong>, and intervening with our vector to make our model produce antonyms on zero-shot prompts.</p>
<p>This will require you to learn how to perform causal interventions with <code class="docutils literal notranslate"><span class="pre">nnsight</span></code>, not just save activations.</p>
<p>Note - this section structurally follows section 2.1 of the function vectors paper.</p>
<section id="ICL-Task">
<h2>ICL Task<a class="headerlink" href="#ICL-Task" title="Link to this heading">#</a></h2>
<section id="Exercise-(optional)---generate-your-own-antonym-pairs">
<h3>Exercise (optional) - generate your own antonym pairs<a class="headerlink" href="#Exercise-(optional)---generate-your-own-antonym-pairs" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴🔴⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵⚪⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">If you choose to do this exercise, you should spend up to 10-30 minutes on it - depending on your familiarity with the OpenAI Python API.</span>
</pre></div>
</div>
</div></blockquote>
<p>We’ve provided you two options for the antonym dataset you’ll use in these exercises.</p>
<ol class="arabic simple">
<li><p>Firstly, we’ve provided you a list of word pairs, in the file <code class="docutils literal notranslate"><span class="pre">data/antonym_pairs.txt</span></code>.</p></li>
<li><p>Secondly, if you want to run experiments like the ones in this paper, it can be good practice to learn how to generate prompts from GPT-4 or other models (this is how we generated the data for this exercise).</p></li>
</ol>
<p>If you just want to use the provided list of words, skip this exercise and run the code below to load in the dataset from the text file. Alternatively, if you want to generate your own dataset, you can fill in the function <code class="docutils literal notranslate"><span class="pre">generate_dataset</span></code> below, which should query GPT-4 and get a list of antonym pairs.</p>
<p>See <a class="reference external" href="https://platform.openai.com/docs/guides/gpt/chat-completions-api">here</a> for a guide to using the chat completions API, if you haven’t already used it. Use the two dropdowns below (in order) for some guidance.</p>
<details><summary><p>Getting started #1</p>
</summary><p>Here is a recommended template:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">antonym_task</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">start_of_response</span><span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">antonym_task</span></code> explains the antonym task, and <code class="docutils literal notranslate"><span class="pre">start_of_respose</span></code> gives the model a prompt to start from (e.g. “Sure, here are some antonyms: …”), to guide its subsequent behaviour.</p>
</details><details><summary><p>Getting started #2</p>
</summary><p>Here is an template you might want to use for the actual request:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">example_antonyms</span> <span class="o">=</span> <span class="s2">&quot;old: young, top: bottom, awake: asleep, future: past, &quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Give me </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> examples of antonym pairs. They should be obvious, i.e. each word should be associated with a single correct antonym.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Sure! Here are </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> pairs of antonyms satisfying this specification: </span><span class="si">{</span><span class="n">example_antonyms</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the function argument. Note that we’ve provided a few example antonyms, and appended them to the start of GPT4’s completion. This is a classic trick to guide the rest of the output (in fact, it’s commonly used in adversarial attacks).</p>
</details><p>Note - it’s possible that not all the antonyms returned will be solvable by GPT-J. In this section, we won’t worry too much about this. When it comes to testing out our zero-shot intervention, we’ll make sure to only use cases where GPT-J can actually solve it.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_antonym_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates 100 pairs of antonyms, in the form of a list of 2-tuples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Please set your API key before running this function!&quot;</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">])</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Generate </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2"> pairs of antonyms in the form of a list of 2-tuples. For example, [[&#39;old&#39;, &#39;young&#39;], [&#39;top&#39;, bottom&#39;], [&#39;awake&#39;, &#39;asleep&#39;]...].&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Sure, here is a list of 100 antonyms: &quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>


<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">ANTONYM_PAIRS</span> <span class="o">=</span> <span class="n">generate_antonym_dataset</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="c1"># Save the word pairs in a text file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">section_dir</span> <span class="o">/</span> <span class="s2">&quot;data&quot;</span> <span class="o">/</span> <span class="s2">&quot;my_antonym_pairs.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="n">ANTONYM_PAIRS</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Load the word pairs from the text file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">section_dir</span> <span class="o">/</span> <span class="s2">&quot;data&quot;</span> <span class="o">/</span> <span class="s2">&quot;antonym_pairs.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">ANTONYM_PAIRS</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[&#39;old&#39;, &#39;young&#39;], [&#39;top&#39;, &#39;bottom&#39;], [&#39;awake&#39;, &#39;asleep&#39;], [&#39;future&#39;, &#39;past&#39;], [&#39;appear&#39;, &#39;disappear&#39;], [&#39;early&#39;, &#39;late&#39;], [&#39;empty&#39;, &#39;full&#39;], [&#39;innocent&#39;, &#39;guilty&#39;], [&#39;ancient&#39;, &#39;modern&#39;], [&#39;arrive&#39;, &#39;depart&#39;]]
</pre></div></div>
</div>
</section>
</section>
<section id="ICL-Dataset">
<h2>ICL Dataset<a class="headerlink" href="#ICL-Dataset" title="Link to this heading">#</a></h2>
<p>To handle this list of word pairs, we’ve given you some helpful classes.</p>
<p>Firstly, there’s the <code class="docutils literal notranslate"><span class="pre">ICLSequence</span></code> class, which takes in a list of word pairs and contains methods for constructing a prompt (and completion) from these words. Run the code below to see how it works.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ICLSequence</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class to store a single antonym sequence.</span>

<span class="sd">    Uses the default template &quot;Q: {x}\nA: {y}&quot; (with separate pairs split by &quot;\n\n&quot;).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_pairs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span> <span class="o">=</span> <span class="n">word_pairs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">word_pairs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the prompt, which contains all but the second element in the last word pair.&quot;&quot;&quot;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;Q: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\n</span><span class="s2">A: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">p</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">completion</span><span class="p">())]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">completion</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the second element in the last word pair (with padded space).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prints a readable string representation of the prompt &amp; completion (indep of template).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">)&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> -&gt;&quot;</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="p">)</span>


<span class="n">word_list</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;hot&quot;</span><span class="p">,</span> <span class="s2">&quot;cold&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;yes&quot;</span><span class="p">,</span> <span class="s2">&quot;no&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;out&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;up&quot;</span><span class="p">,</span> <span class="s2">&quot;down&quot;</span><span class="p">]]</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">ICLSequence</span><span class="p">(</span><span class="n">word_list</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tuple-representation of the sequence:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Actual prompt, which will be fed into the model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">prompt</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Tuple-representation of the sequence:
(hot, cold), (yes, no), (in, out), up -&gt;

Actual prompt, which will be fed into the model:
Q: hot
A: cold

Q: yes
A: no

Q: in
A: out

Q: up
A:
</pre></div></div>
</div>
<p>Secondly, we have the <code class="docutils literal notranslate"><span class="pre">ICLDataset</span></code> class. This is also fed a word pair list, and it has methods for generating batches of prompts and completions. It can generate both clean prompts (where each pair is actually an antonym pair) and corrupted prompts (where the answers for each pair are randomly chosen from the dataset).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ICLDataset</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency</span>
<span class="sd">    between the corrupted and clean datasets.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        word_pairs:</span>
<span class="sd">            list of ICL task, e.g. [[&quot;old&quot;, &quot;young&quot;], [&quot;top&quot;, &quot;bottom&quot;], ...] for the antonym task</span>
<span class="sd">        size:</span>
<span class="sd">            number of prompts to generate</span>
<span class="sd">        n_prepended:</span>
<span class="sd">            number of antonym pairs before the single-word ICL task</span>
<span class="sd">        bidirectional:</span>
<span class="sd">            if True, then we also consider the reversed antonym pairs</span>
<span class="sd">        corrupted:</span>
<span class="sd">            if True, then the second word in each pair is replaced with a random word</span>
<span class="sd">        seed:</span>
<span class="sd">            random seed, for consistency &amp; reproducibility</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">word_pairs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_prepended</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">corrupted</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="n">n_prepended</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">),</span> <span class="s2">&quot;Not enough antonym pairs in dataset to create prompt.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span> <span class="o">=</span> <span class="n">word_pairs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="n">word_pairs</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_pair</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_prepended</span> <span class="o">=</span> <span class="n">n_prepended</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corrupted</span> <span class="o">=</span> <span class="n">corrupted</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">completions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Generate the dataset (by choosing random word pairs, and constructing `ICLSequence` objects)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
            <span class="n">random_pairs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">),</span> <span class="n">n_prepended</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Randomize the order of each word pair (x, y). If not bidirectional, we always have x -&gt; y not y -&gt; x</span>
            <span class="n">random_orders</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_prepended</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">bidirectional</span><span class="p">):</span>
                <span class="n">random_orders</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">[</span><span class="n">pair</span><span class="p">][::</span><span class="n">order</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">order</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">random_pairs</span><span class="p">,</span> <span class="n">random_orders</span><span class="p">)]</span>
            <span class="c1"># If corrupted, then replace y with a random word in all (x, y) pairs except the last one</span>
            <span class="k">if</span> <span class="n">corrupted</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">word_pairs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_list</span><span class="p">)</span>
            <span class="n">seq</span> <span class="o">=</span> <span class="n">ICLSequence</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">prompt</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq</span><span class="o">.</span><span class="n">completion</span><span class="p">())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_corrupted_dataset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a corrupted version of the dataset (with same random seed).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">ICLDataset</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word_pairs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_prepended</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span><span class="p">,</span>
            <span class="n">corrupted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>You can see how this dataset works below. <strong>Note that the correct completions have a prepended space</strong>, because this is how the antonym prompts are structured - the answers are tokenized as <code class="docutils literal notranslate"><span class="pre">&quot;A:</span> <span class="pre">answer&quot;</span> <span class="pre">-&gt;</span> <span class="pre">[&quot;A&quot;,</span> <span class="pre">&quot;:&quot;,</span> <span class="pre">&quot;</span> <span class="pre">answer&quot;]</span></code>. Forgetting prepended spaces is a classic mistake when working with transformers!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">corrupted</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Correct completion&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">completion</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">))</span>

<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                               </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (right, left), (maximum, minimum), melt -&gt;           │ ' freeze'          │
│ (minimum, maximum), (old, new), punishment -&gt;        │ ' reward'          │
│ (arrogant, humble), (blunt, sharp), compulsory -&gt;    │ ' voluntary'       │
│ (inside, outside), (freeze, melt), full -&gt;           │ ' empty'           │
│ (reject, accept), (awake, asleep), dusk -&gt;           │ ' dawn'            │
│ (invisible, visible), (punishment, reward), heavy -&gt; │ ' light'           │
│ (victory, defeat), (forward, backward), young -&gt;     │ ' old'             │
│ (up, down), (compulsory, voluntary), right -&gt;        │ ' wrong'           │
│ (open, closed), (domestic, foreign), brave -&gt;        │ ' cowardly'        │
│ (under, over), (past, future), increase -&gt;           │ ' decrease'        │
└──────────────────────────────────────────────────────┴────────────────────┘
</pre></div>
</div>
<p>Compare this output to what it looks like when <code class="docutils literal notranslate"><span class="pre">corrupted=True</span></code>. Each of the pairs before the last one has their second element replaced with a random one (but the last pair is unchanged).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">corrupted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;Correct completion&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">completions</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completions</span><span class="p">))</span>

<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Prompt                                            </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (right, private), (maximum, destroy), melt -&gt;     │ ' freeze'          │
│ (minimum, increase), (old, sharp), punishment -&gt;  │ ' reward'          │
│ (arrogant, humble), (blunt, deep), compulsory -&gt;  │ ' voluntary'       │
│ (inside, voluntary), (freeze, exterior), full -&gt;  │ ' empty'           │
│ (reject, profit), (awake, start), dusk -&gt;         │ ' dawn'            │
│ (invisible, birth), (punishment, spend), heavy -&gt; │ ' light'           │
│ (victory, rich), (forward, honest), young -&gt;      │ ' old'             │
│ (up, lie), (compulsory, short), right -&gt;          │ ' wrong'           │
│ (open, soft), (domestic, anxious), brave -&gt;       │ ' cowardly'        │
│ (under, melt), (past, young), increase -&gt;         │ ' decrease'        │
└───────────────────────────────────────────────────┴────────────────────┘
</pre></div>
</div>
<details><summary><p>Aside - the rich library</p>
</summary><p>The <code class="docutils literal notranslate"><span class="pre">rich</span></code> library is a helpful little library to display outputs more clearly in a Python notebook or terminal. It’s not necessary for this workshop, but it’s a nice little tool to have in your toolbox.</p>
<p>The most important function is <code class="docutils literal notranslate"><span class="pre">rich.print</span></code> (usually imported as <code class="docutils literal notranslate"><span class="pre">rprint</span></code>). This can print basic strings, but it also supports the following syntax for printing colors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rprint</span><span class="p">(</span><span class="s2">&quot;[green]This is green text[/], this is default color&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="83fdd99f2b664611a487fab21c9449e2" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-1.png" style="width: 350px;" /></p>
<p>and for making text bold / underlined:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rprint</span><span class="p">(</span><span class="s2">&quot;[u dark_orange]This is underlined[/], and [b cyan]this is bold[/].&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="fed19808f685413cb01b3b1d9f3ae691" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-2.png" style="width: 350px;" /></p>
<p>It can also print tables:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">rich.table</span><span class="w"> </span><span class="kn">import</span> <span class="n">Table</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Col1&quot;</span><span class="p">,</span> <span class="s2">&quot;Col2&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Title&quot;</span><span class="p">)</span> <span class="c1"># title is optional</span>
<span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>

<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="2a8e8555c2904283bbb8b24ce6a9f06a" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rprint-3.png" style="width: 150px;" /></p>
<p>The text formatting (bold, underlined, colors, etc) is also supported within table cells.</p>
</details></section>
<section id="Task-encoding-vector">
<h2>Task-encoding vector<a class="headerlink" href="#Task-encoding-vector" title="Link to this heading">#</a></h2>
<section id="Exercise---forward-pass-on-antonym-dataset">
<h3>Exercise - forward pass on antonym dataset<a class="headerlink" href="#Exercise---forward-pass-on-antonym-dataset" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴⚪⚪⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 10-15 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>You should fill in the <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> function below. It should:</p>
<ul class="simple">
<li><p>Run a forward pass on the model with the dataset prompts (i.e. the <code class="docutils literal notranslate"><span class="pre">.prompts</span></code> attribute), using the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> syntax we’ve demonstrated previously,</p></li>
<li><p>Return a tuple of the model’s output (i.e. a list of its string-token completions, one for each prompt in the batch) and the residual stream value at the end of layer <code class="docutils literal notranslate"><span class="pre">layer</span></code> (e.g. if <code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">=</span> <span class="pre">-1</span></code>, this means the final value of the residual stream before we convert into logits).</p></li>
</ul>
<p><img alt="834609a664c44040bcf16077d848d4d3" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-1.png" style="width: 900px;" /></p>
<p>You should only return the residual stream values for the very last sequence position in each prompt, i.e. the last <code class="docutils literal notranslate"><span class="pre">-1</span></code> token (where the model makes the antonym prediction), and same for the completions.</p>
<details><summary><p>Help - I’m not sure how to run (and index into) a batch of inputs.</p>
</summary><p>If we pass a list of strings to the <code class="docutils literal notranslate"><span class="pre">generator.invoke</span></code> function, this will be tokenized with padding automatically.</p>
<p>The type of padding which is applied is <strong>left padding</strong>, meaning if you index at sequence position <code class="docutils literal notranslate"><span class="pre">-1</span></code>, this will get the final token in the prompt for all prompts in the list, even if the prompts have different lengths.</p>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_h</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Averages over the model&#39;s hidden representations on each of the prompts in `dataset` at layer `layer`, to produce</span>
<span class="sd">    a single vector `h`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset whose prompts `dataset.prompts` you&#39;re extracting the activations from (at the last seq pos)</span>
<span class="sd">        layer: int</span>
<span class="sd">            the layer you&#39;re extracting activations from</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions: list[str]</span>
<span class="sd">            list of the model&#39;s next-token predictions (i.e. the strings the model predicts to follow the last token)</span>
<span class="sd">        h: Tensor</span>
<span class="sd">            average hidden state tensor at final sequence position, of shape (d_model,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="n">tests</span><span class="o">.</span><span class="n">test_calculate_h</span><span class="p">(</span><span class="n">calculate_h</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ed7231cf77cb4816892d4146e740115f", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
All tests in `test_calculate_h` passed.
</pre></div></div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_h</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span> <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Averages over the model&#39;s hidden representations on each of the prompts in `dataset` at layer `layer`, to produce</span>
<span class="sd">    a single vector `h`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset whose prompts `dataset.prompts` you&#39;re extracting the activations from (at the last seq pos)</span>
<span class="sd">        layer: int</span>
<span class="sd">            the layer you&#39;re extracting activations from</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions: list[str]</span>
<span class="sd">            list of the model&#39;s next-token predictions (i.e. the strings the model predicts to follow the last token)</span>
<span class="sd">        h: Tensor</span>
<span class="sd">            average hidden state tensor at final sequence position, of shape (d_model,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">next_tok_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="n">completions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">next_tok_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">completions</span><span class="p">,</span> <span class="n">h</span>
</pre></div>
</div>
</details><p>We’ve provided you with a helper function, which displays the model’s output on the antonym dataset (and highlights the examples where the model’s prediction is correct). Note, we’re using the <code class="docutils literal notranslate"><span class="pre">repr</span></code> function, because a lot of the completions are line breaks, and this helps us see them more clearly!</p>
<p>If the antonyms dataset was constructed well, you should find that the model’s completion is correct most of the time, and most of its mistakes are either copying (e.g. predicting <code class="docutils literal notranslate"><span class="pre">wet</span> <span class="pre">-&gt;</span> <span class="pre">wet</span></code> rather than <code class="docutils literal notranslate"><span class="pre">wet</span> <span class="pre">-&gt;</span> <span class="pre">dry</span></code>) or understandable completions which shouldn’t really be considered mistakes (e.g. predicting <code class="docutils literal notranslate"><span class="pre">right</span> <span class="pre">-&gt;</span> <span class="pre">left</span></code> rather than <code class="docutils literal notranslate"><span class="pre">right</span> <span class="pre">-&gt;</span> <span class="pre">wrong</span></code>). If we were being rigorous, we’d want to filter this dataset to make sure it only contains examples where the model can correctly
perform the task - but for these exercises, we won’t worry about this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">display_model_completions_on_antonyms</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">completions</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">num_to_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span>
        <span class="s2">&quot;Prompt (tuple representation)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Model&#39;s completion</span><span class="se">\n</span><span class="s2">(green=correct)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Correct completion&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Model&#39;s antonym completions&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">completions</span><span class="p">),</span> <span class="n">num_to_display</span><span class="p">)):</span>
        <span class="c1"># Get model&#39;s completion, and correct completion</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion_first_token</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;Ġ&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Color code the completion based on whether it&#39;s correct</span>
        <span class="n">is_correct</span> <span class="o">=</span> <span class="n">completion</span> <span class="o">==</span> <span class="n">correct_completion_first_token</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[b green]</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">)</span><span class="si">}</span><span class="s2">[/]&quot;</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">)</span>

        <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="n">completion</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">))</span>

    <span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>


<span class="c1"># Get uncorrupted dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Getting it from layer 12, as in the description in section 2.1 of paper</span>
<span class="n">model_completions</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">calculate_h</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Displaying the output</span>
<span class="n">display_model_completions_on_antonyms</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">model_completions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "84a3ebc478ea435db31d0229cdfff4fa", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                    Model's antonym completions                                    </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">                                                       </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt (tuple representation)                         </span>┃<span style="font-weight: bold"> (green=correct)    </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ (right, left), (maximum, minimum), melt -&gt;            │ ' melt'            │ ' freeze'          │
│ (minimum, maximum), (old, new), punishment -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' reward'</span>          │ ' reward'          │
│ (arrogant, humble), (blunt, sharp), compulsory -&gt;     │ ' optional'        │ ' voluntary'       │
│ (inside, outside), (freeze, melt), full -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' empty'</span>           │ ' empty'           │
│ (reject, accept), (awake, asleep), dusk -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' dawn'</span>            │ ' dawn'            │
│ (invisible, visible), (punishment, reward), heavy -&gt;  │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' light'</span>           │ ' light'           │
│ (victory, defeat), (forward, backward), young -&gt;      │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' old'</span>             │ ' old'             │
│ (up, down), (compulsory, voluntary), right -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' wrong'</span>           │ ' wrong'           │
│ (open, closed), (domestic, foreign), brave -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' cowardly'</span>        │ ' cowardly'        │
│ (under, over), (past, future), increase -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' decrease'</span>        │ ' decrease'        │
│ (inside, outside), (melt, freeze), over -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' under'</span>           │ ' under'           │
│ (solid, liquid), (backward, forward), open -&gt;         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' closed'</span>          │ ' closed'          │
│ (optimist, pessimist), (invisible, visible), brave -&gt; │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' cowardly'</span>        │ ' cowardly'        │
│ (noisy, quiet), (sell, buy), north -&gt;                 │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' south'</span>           │ ' south'           │
│ (guilty, innocent), (birth, death), victory -&gt;        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' defeat'</span>          │ ' defeat'          │
│ (answer, question), (noisy, quiet), ancient -&gt;        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' modern'</span>          │ ' modern'          │
│ (on, off), (success, failure), flexible -&gt;            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' rigid'</span>           │ ' rigid'           │
│ (junior, senior), (arrive, depart), punishment -&gt;     │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' reward'</span>          │ ' reward'          │
│ (loose, tight), (learn, teach), new -&gt;                │ ' new'             │ ' old'             │
│ (introduce, remove), (deficiency, quality), wet -&gt;    │ ' wet'             │ ' dry'             │
└───────────────────────────────────────────────────────┴────────────────────┴────────────────────┘
</pre></div>
</div>
</section>
<section id="Using-multiple-invokes">
<h3>Using multiple invokes<a class="headerlink" href="#Using-multiple-invokes" title="Link to this heading">#</a></h3>
<p>Another cool feature of <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> is the ability to run multiple different batches through the model at once (or the same batch multiple times) in a way which leads to very clean syntax for doing causal interventions. Rather than doing something like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
    <span class="c1"># some causal interventions</span>
</pre></div>
</div>
<p>we can write a double-nested context manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># some causal interventions</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">other_inputs</span><span class="p">):</span>
        <span class="c1"># some other causal interventions</span>
</pre></div>
</div>
<p>Both inputs will be run together in parallel, and proxies defined within one <code class="docutils literal notranslate"><span class="pre">tracer.invoke</span></code> block can be used in another. A common use-case is to have clean and corrupted inputs, so we can patch from one to the other and get both outputs all in a single forward pass:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">clean_inputs</span><span class="p">):</span>
        <span class="c1"># extract clean activations</span>
        <span class="n">clean_activations</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">corrupted_inputs</span><span class="p">):</span>
        <span class="c1"># patch clean into corrupted</span>
        <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]</span> <span class="o">=</span> <span class="n">clean_activations</span>
</pre></div>
</div>
<p>You’ll do something like this in a later exercise. However for your first exercise (immediately below), you’ll only be intervening with vectors that are defined outside of your context manager.</p>
<p><strong>One important thing to watch out for</strong> - make sure you’re not using your proxy before it’s being defined! For example, if you were extracting <code class="docutils literal notranslate"><span class="pre">clean_activations</span></code> from <code class="docutils literal notranslate"><span class="pre">model.transformer.h[10]</span></code> but then intervening with it on <code class="docutils literal notranslate"><span class="pre">model.transformer.h[9]</span></code>, this couldn’t be done in parallel (you’d need to first extract the clean activations, <em>then</em> run the patched forward pass). Doing this should result in a warning message, but may pass silently in some cases - so you need to be extra
vigilant!</p>
</section>
<section id="Exercise---intervene-with-h">
<h3>Exercise - intervene with <span class="math notranslate nohighlight">\(h\)</span><a class="headerlink" href="#Exercise---intervene-with-h" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴⚪⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵🔵⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 10-15 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>You should fill in the function <code class="docutils literal notranslate"><span class="pre">intervene_with_h</span></code> below. This will involve:</p>
<ul class="simple">
<li><p>Run two forward passes (within the same context manager) on a zero-shot dataset:</p>
<ul>
<li><p>One with no intervention (i.e. the residual stream is unchanged),</p></li>
<li><p>One with an intervention using <code class="docutils literal notranslate"><span class="pre">h</span></code> (i.e. <code class="docutils literal notranslate"><span class="pre">h</span></code> is added to the residual stream at the layer it was taken from).</p></li>
</ul>
</li>
<li><p>Return the completions for no intervention and intervention cases respectively (see docstring).</p></li>
</ul>
<p>The diagram below shows how all of this should work, when combined with the <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> function.</p>
<p><img alt="13371017774143dcb57fe6078f01f542" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-2.png" style="width: 950px;" /></p>
<p>Hint - you can use <code class="docutils literal notranslate"><span class="pre">tokenizer.batch_decode</span></code> to turn a list of tokens into a list of strings.</p>
<details><summary><p>Help - I’m not sure how best to get both the no-intervention and intervention completions.</p>
</summary><p>You can use <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">tracer.invoke...</span></code> more than once within the same context manager, in order to add to your batch. This will eventually give you output of shape (2*N, seq_len), which can then be indexed and reshaped to get the completions in the no intervention &amp; intervention cases respectively.</p>
</details><details><summary><p>Help - I’m not sure how to intervene on the hidden state.</p>
</summary><p>First, you can define the tensor of hidden states (i.e. using <code class="docutils literal notranslate"><span class="pre">.output[0]</span></code>, like you’ve done before).</p>
<p>Then, you can add to this tensor directly (or add to some indexed version of it). You can use inplace operations (i.e. <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">+=</span> <span class="pre">h</span></code>) or redefining the tensor (i.e. <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">=</span> <span class="pre">tensor</span> <span class="pre">+</span> <span class="pre">h</span></code>); either work.</p>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">intervene_with_h</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">zero_shot_dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">remote</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">REMOTE</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the</span>
<span class="sd">    residual stream of a set of generated zero-shot prompts.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: the model we&#39;re using to generate completions</span>
<span class="sd">        zero_shot_dataset: the dataset of zero-shot prompts which we&#39;ll intervene on, using the `h`-vector</span>
<span class="sd">        h: the `h`-vector we&#39;ll be adding to the residual stream</span>
<span class="sd">        layer: the layer we&#39;ll be extracting the `h`-vector from</span>
<span class="sd">        remote: whether to run the forward pass on the remote server (used for running test code)</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions_zero_shot: list of string completions for the zero-shot prompts, without intervention</span>
<span class="sd">        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="n">tests</span><span class="o">.</span><span class="n">test_intervene_with_h</span><span class="p">(</span><span class="n">intervene_with_h</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">REMOTE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Running your `intervene_with_h` function...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "f301ae5316f84d739a66681ed5048a91", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Running `solutions.intervene_with_h` (so we can compare outputs) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "56ff235baaba49e686e5a5745854b00e", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Comparing the outputs...

All tests in `test_intervene_with_h` passed.
</pre></div></div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">intervene_with_h</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">zero_shot_dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">h</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">remote</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">REMOTE</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the</span>
<span class="sd">    residual stream of a set of generated zero-shot prompts.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: the model we&#39;re using to generate completions</span>
<span class="sd">        zero_shot_dataset: the dataset of zero-shot prompts which we&#39;ll intervene on, using the `h`-vector</span>
<span class="sd">        h: the `h`-vector we&#39;ll be adding to the residual stream</span>
<span class="sd">        layer: the layer we&#39;ll be extracting the `h`-vector from</span>
<span class="sd">        remote: whether to run the forward pass on the remote server (used for running test code)</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions_zero_shot: list of string completions for the zero-shot prompts, without intervention</span>
<span class="sd">        completions_intervention: list of string completions for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">remote</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
        <span class="c1"># First, run a forward pass where we don&#39;t intervene, just save token id completions</span>
        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">token_completions_zero_shot</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

        <span class="c1"># Next, run a forward pass on the zero-shot prompts where we do intervene</span>
        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="c1"># Add the h-vector to the residual stream, at the last sequence position</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>
            <span class="c1"># Also save completions</span>
            <span class="n">token_completions_intervention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="c1"># Decode to get the string tokens</span>
    <span class="n">completions_zero_shot</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">token_completions_zero_shot</span><span class="p">)</span>
    <span class="n">completions_intervention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">token_completions_intervention</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">completions_zero_shot</span><span class="p">,</span> <span class="n">completions_intervention</span>
</pre></div>
</div>
</details><p>Run the code below to calculate completions for the function.</p>
<p><strong>Note, it’s very important that we set a different random seed for the zero shot dataset, otherwise we’ll be intervening on examples which were actually in the dataset we used to compute :math:`h`!</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">zero_shot_dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Run previous function to get h-vector</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">calculate_h</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Run new function to intervene with h-vector</span>
<span class="n">completions_zero_shot</span><span class="p">,</span> <span class="n">completions_intervention</span> <span class="o">=</span> <span class="n">intervene_with_h</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Zero-shot completions: &quot;</span><span class="p">,</span> <span class="n">completions_zero_shot</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Completions with intervention: &quot;</span><span class="p">,</span> <span class="n">completions_intervention</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "9d2acab0a4194c5bafc413a3f82f708b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1ed7cba7417844fdb5a9603f470c0ec2", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Zero-shot completions:  [&#39; minimum&#39;, &#39; I&#39;, &#39; inside&#39;, &#39; reject&#39;, &#39; invisible&#39;, &#39; victory&#39;, &#39; up&#39;, &#39; open&#39;, &#39; under&#39;, &#39; inside&#39;, &#39; solid&#39;, &#39;\n&#39;, &#39; noisy&#39;, &#39; guilty&#39;, &#39; yes&#39;, &#39; I&#39;, &#39; senior&#39;, &#39; loose&#39;, &#39; introduce&#39;, &#39; innocent&#39;]
Completions with intervention:  [&#39; maximum&#39;, &#39; arrogant&#39;, &#39; outside&#39;, &#39; reject&#39;, &#39; visible&#39;, &#39; victory&#39;, &#39; down&#39;, &#39; closed&#39;, &#39; under&#39;, &#39; outside&#39;, &#39; solid&#39;, &#39; optim&#39;, &#39; noisy&#39;, &#39; guilty&#39;, &#39; answer&#39;, &#39; on&#39;, &#39; senior&#39;, &#39; tight&#39;, &#39; introduce&#39;, &#39; guilty&#39;]
</pre></div></div>
</div>
<p>Next, run the code below to visualise the completions in a table. You should see:</p>
<ul class="simple">
<li><p>~0% correct completions on the zero-shot prompt with no intervention, because the model usually just copies the first and only word in the prompt</p></li>
<li><p>~25% correct completions on the zero-shot prompt with intervention</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">display_model_completions_on_h_intervention</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">completions</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">completions_intervention</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">num_to_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span>
        <span class="s2">&quot;Prompt&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Model&#39;s completion</span><span class="se">\n</span><span class="s2">(no intervention)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Model&#39;s completion</span><span class="se">\n</span><span class="s2">(intervention)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Correct completion&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Model&#39;s antonym completions&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">completions</span><span class="p">),</span> <span class="n">num_to_display</span><span class="p">)):</span>
        <span class="n">completion_ni</span> <span class="o">=</span> <span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">completion_i</span> <span class="o">=</span> <span class="n">completions_intervention</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">correct_completion_first_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;Ġ&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="c1"># Color code the completion based on whether it&#39;s correct</span>
        <span class="n">is_correct</span> <span class="o">=</span> <span class="n">completion_i</span> <span class="o">==</span> <span class="n">correct_completion_first_token</span>
        <span class="n">completion_i</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[b green]</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion_i</span><span class="p">)</span><span class="si">}</span><span class="s2">[/]&quot;</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion_i</span><span class="p">)</span>

        <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion_ni</span><span class="p">),</span> <span class="n">completion_i</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">correct_completion</span><span class="p">))</span>

    <span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>


<span class="n">display_model_completions_on_h_intervention</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">completions_zero_shot</span><span class="p">,</span> <span class="n">completions_intervention</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                          Model's antonym completions                          </span>
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">              </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt       </span>┃<span style="font-weight: bold"> (no intervention)  </span>┃<span style="font-weight: bold"> (intervention)     </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ minimum -&gt;   │ ' minimum'         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' maximum'</span>         │ ' maximum'         │
│ arrogant -&gt;  │ ' I'               │ ' arrogant'        │ ' humble'          │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ reject -&gt;    │ ' reject'          │ ' reject'          │ ' accept'          │
│ invisible -&gt; │ ' invisible'       │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' visible'</span>         │ ' visible'         │
│ victory -&gt;   │ ' victory'         │ ' victory'         │ ' defeat'          │
│ up -&gt;        │ ' up'              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' down'</span>            │ ' down'            │
│ open -&gt;      │ ' open'            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' closed'</span>          │ ' closed'          │
│ under -&gt;     │ ' under'           │ ' under'           │ ' over'            │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ solid -&gt;     │ ' solid'           │ ' solid'           │ ' liquid'          │
│ optimist -&gt;  │ '\n'               │ ' optim'           │ ' pessimist'       │
│ noisy -&gt;     │ ' noisy'           │ ' noisy'           │ ' quiet'           │
│ guilty -&gt;    │ ' guilty'          │ ' guilty'          │ ' innocent'        │
│ answer -&gt;    │ ' yes'             │ ' answer'          │ ' question'        │
│ on -&gt;        │ ' I'               │ ' on'              │ ' off'             │
│ junior -&gt;    │ ' senior'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' senior'</span>          │ ' senior'          │
│ loose -&gt;     │ ' loose'           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' tight'</span>           │ ' tight'           │
│ introduce -&gt; │ ' introduce'       │ ' introduce'       │ ' remove'          │
│ innocent -&gt;  │ ' innocent'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' guilty'</span>          │ ' guilty'          │
└──────────────┴────────────────────┴────────────────────┴────────────────────┘
</pre></div>
</div>
</section>
<section id="Exercise---combine-the-last-two-functions">
<h3>Exercise - combine the last two functions<a class="headerlink" href="#Exercise---combine-the-last-two-functions" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴⚪⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 10-15 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>One great feature of the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library is its ability to parallelize forward passes and perform complex interventions within a single context manager.</p>
<p>In the code above, we had one function to extract the hidden states from the model, and another function where we intervened with those hidden states. But we can actually do both at once: we can compute <span class="math notranslate nohighlight">\(h\)</span> within our forward pass, and then intervene with it on a different forward pass (using our zero-shot prompts), all within the same <code class="docutils literal notranslate"><span class="pre">model.trace</span></code> context manager. In other words, <strong>we’ll be using ``with tracer.invoke…`` three times</strong> in this context manager.</p>
<p><img alt="7e1ac8b7eb1e4189aaa73f47fc6021dc" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/h-intervention-3.png" style="width: 1000px;" /></p>
<p>You should fill in the <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code> function below, to do this. Mostly, this should involve combining your <code class="docutils literal notranslate"><span class="pre">calculate_h</span></code> and <code class="docutils literal notranslate"><span class="pre">intervene_with_h</span></code> functions, and wrapping the forward passes in the same context manager (plus a bit of code rewriting).</p>
<p>Your output should be exactly the same as before (since the <code class="docutils literal notranslate"><span class="pre">ICLDataset</span></code> class is deterministic), hence we’ve not provided test functions in this case - you can just compare the table you get to the one before! However, this time around your code should run twice as fast, because you’re batching the operations of “compute <span class="math notranslate nohighlight">\(h\)</span>” and “intervene with <span class="math notranslate nohighlight">\(h\)</span>” together into a single forward pass.</p>
<details><summary><p>Help - I’m not sure how to use the h vector inside the context manager.</p>
</summary><p>You extract <code class="docutils literal notranslate"><span class="pre">h</span></code> the same way as before, but you don’t need to save it. It is kept as a proxy. You can still use it later in the context manager, just like it actually was a tensor.</p>
<p>You shouldn’t have to <code class="docutils literal notranslate"><span class="pre">.save()</span></code> anything inside your context manager, other than the token completions.</p>
</details><details><summary><p>Help - If I want to add x vector to a slice of my hidden state tensor h, is h[slice]+=x the same as h2 = h[slice], h2 += x?</p>
</summary><p>No, only <code class="docutils literal notranslate"><span class="pre">h[slice]+=x</span></code> does what you want. This is because when doing h2 = h[slice], h2 += x, the modification line h2 += x is no longer modifying the original tensor <code class="docutils literal notranslate"><span class="pre">h</span></code>, but a different tensor<code class="docutils literal notranslate"><span class="pre">h2</span></code>. In contrast, <code class="docutils literal notranslate"><span class="pre">h[slice]+=x</span></code> keeps the original tensor <code class="docutils literal notranslate"><span class="pre">h</span></code> in the modification line.</p>
<p>A good rule to keep in mind is: If you’re trying to modify a tensor some in-place operation, make sure that tensor is in the actual modification line!</p>
</details><div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_h_and_intervene</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">zero_shot_dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,</span>
<span class="sd">    all within the same forward pass. Returns the completions from this intervention.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the model we&#39;re using to generate completions</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        zero_shot_dataset: ICLDataset</span>
<span class="sd">            the dataset of zero-shot prompts which we&#39;ll intervene on, using the `h`-vector</span>
<span class="sd">        layer: int</span>
<span class="sd">            the layer we&#39;ll be extracting the `h`-vector from</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions_zero_shot: list[str]</span>
<span class="sd">            list of string completions for the zero-shot prompts, without intervention</span>
<span class="sd">        completions_intervention: list[str]</span>
<span class="sd">            list of string completions for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">zero_shot_dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">completions_zero_shot</span><span class="p">,</span> <span class="n">completions_intervention</span> <span class="o">=</span> <span class="n">calculate_h_and_intervene</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
<span class="p">)</span>

<span class="n">display_model_completions_on_h_intervention</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">completions_zero_shot</span><span class="p">,</span> <span class="n">completions_intervention</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "88e9db1748154aba8cfa635124a73a6a", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                          Model's antonym completions                          </span>
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">              </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold"> Model's completion </span>┃<span style="font-weight: bold">                    </span>┃
┃<span style="font-weight: bold"> Prompt       </span>┃<span style="font-weight: bold"> (no intervention)  </span>┃<span style="font-weight: bold"> (intervention)     </span>┃<span style="font-weight: bold"> Correct completion </span>┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ minimum -&gt;   │ ' minimum'         │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' maximum'</span>         │ ' maximum'         │
│ arrogant -&gt;  │ ' arrogant'        │ ' arrogant'        │ ' humble'          │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ reject -&gt;    │ ' reject'          │ ' reject'          │ ' accept'          │
│ invisible -&gt; │ ' invisible'       │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' visible'</span>         │ ' visible'         │
│ victory -&gt;   │ ' victory'         │ ' victory'         │ ' defeat'          │
│ up -&gt;        │ ' up'              │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' down'</span>            │ ' down'            │
│ open -&gt;      │ ' open'            │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' closed'</span>          │ ' closed'          │
│ under -&gt;     │ ' under'           │ ' under'           │ ' over'            │
│ inside -&gt;    │ ' inside'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' outside'</span>         │ ' outside'         │
│ solid -&gt;     │ ' solid'           │ ' solid'           │ ' liquid'          │
│ optimist -&gt;  │ '\n'               │ ' optim'           │ ' pessimist'       │
│ noisy -&gt;     │ ' noisy'           │ ' noisy'           │ ' quiet'           │
│ guilty -&gt;    │ ' guilty'          │ ' guilty'          │ ' innocent'        │
│ answer -&gt;    │ ' answer'          │ ' answer'          │ ' question'        │
│ on -&gt;        │ ' I'               │ ' on'              │ ' off'             │
│ junior -&gt;    │ ' junior'          │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' senior'</span>          │ ' senior'          │
│ loose -&gt;     │ ' loose'           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' tight'</span>           │ ' tight'           │
│ introduce -&gt; │ ' introduce'       │ ' introduce'       │ ' remove'          │
│ innocent -&gt;  │ ' innocent'        │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">' guilty'</span>          │ ' guilty'          │
└──────────────┴────────────────────┴────────────────────┴────────────────────┘
</pre></div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_h_and_intervene</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">zero_shot_dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,</span>
<span class="sd">    all within the same forward pass. Returns the completions from this intervention.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the model we&#39;re using to generate completions</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        zero_shot_dataset: ICLDataset</span>
<span class="sd">            the dataset of zero-shot prompts which we&#39;ll intervene on, using the `h`-vector</span>
<span class="sd">        layer: int</span>
<span class="sd">            the layer we&#39;ll be extracting the `h`-vector from</span>

<span class="sd">    Returns:</span>
<span class="sd">        completions_zero_shot: list[str]</span>
<span class="sd">            list of string completions for the zero-shot prompts, without intervention</span>
<span class="sd">        completions_intervention: list[str]</span>
<span class="sd">            list of string completions for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">clean_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">hidden</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>
            <span class="n">intervene_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="n">completions_zero_shot</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">clean_tokens</span><span class="p">)</span>
    <span class="n">completions_intervention</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">intervene_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">completions_zero_shot</span><span class="p">,</span> <span class="n">completions_intervention</span>
</pre></div>
</div>
</details></section>
<section id="Exercise---compute-change-in-accuracy">
<h3>Exercise - compute change in accuracy<a class="headerlink" href="#Exercise---compute-change-in-accuracy" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴⚪⚪⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 10-20 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>So far, all we’ve done is look at the most likely completions, and see what fraction of the time these were correct. But our forward pass doesn’t just give us token completions, it gives us logits too!</p>
<p>You should now rewrite the <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code> function so that, rather than returning two lists of string completions, it returns two lists of floats containing the <strong>logprobs assigned by the model to the correct antonym</strong> in the no intervention / intervention cases respectively.</p>
<details><summary><p>Help - I don’t know how to get the correct logprobs from the logits.</p>
</summary><p>First, apply log softmax to the logits, to get logprobs.</p>
<p>Second, you can use <code class="docutils literal notranslate"><span class="pre">tokenizer(dataset.completions)[&quot;input_ids&quot;]</span></code> to get the token IDs of the correct completions. (Gotcha - some words might be tokenized into multiple tokens, so make sure you’re just picking the first token ID for each completion.)</p>
<p>Note - we recommend doing all this inside the context manager, then saving and returning just the correct logprobs not all the logits (this means less to download from the server!).</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[38]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_h_and_intervene_logprobs</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">zero_shot_dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,</span>
<span class="sd">    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the model we&#39;re using to generate completions</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        zero_shot_dataset: ICLDataset</span>
<span class="sd">            the dataset of zero-shot prompts which we&#39;ll intervene on, using the `h`-vector</span>
<span class="sd">        layer: int</span>
<span class="sd">            the layer we&#39;ll be extracting the `h`-vector from</span>

<span class="sd">    Returns:</span>
<span class="sd">        correct_logprobs: list[float]</span>
<span class="sd">            list of correct-token logprobs for the zero-shot prompts, without intervention</span>
<span class="sd">        correct_logprobs_intervention: list[float]</span>
<span class="sd">            list of correct-token logprobs for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_h_and_intervene_logprobs</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">zero_shot_dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,</span>
<span class="sd">    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the model we&#39;re using to generate completions</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the `h`-vector</span>
<span class="sd">        zero_shot_dataset: ICLDataset</span>
<span class="sd">            the dataset of zero-shot prompts which we&#39;ll intervene on, using the `h`-vector</span>
<span class="sd">        layer: int</span>
<span class="sd">            the layer we&#39;ll be extracting the `h`-vector from</span>

<span class="sd">    Returns:</span>
<span class="sd">        correct_logprobs: list[float]</span>
<span class="sd">            list of correct-token logprobs for the zero-shot prompts, without intervention</span>
<span class="sd">        correct_logprobs_intervention: list[float]</span>
<span class="sd">            list of correct-token logprobs for the zero-shot prompts, with h-intervention</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">correct_completion_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">toks</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">clean_logprobs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span>
                <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">correct_completion_ids</span>
            <span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">hidden</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>
            <span class="n">intervene_logprobs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span>
                <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">zero_shot_dataset</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">correct_completion_ids</span>
            <span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">clean_logprobs</span><span class="p">,</span> <span class="n">intervene_logprobs</span>
</pre></div>
</div>
</details><p>When you run the code below, it will display the log-probabilities (highlighting green when they increase from the zero-shot case). You should find that in every sequence, the logprobs on the correct token increase in the intervention. This helps make something clear - <strong>even if the maximum-likelihood token doesn’t change, this doesn’t mean that the intervention isn’t having a significant effect.</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">display_model_logprobs_on_h_intervention</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">correct_logprobs_zero_shot</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">correct_logprobs_intervention</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">num_to_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span>
        <span class="s2">&quot;Zero-shot prompt&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Model&#39;s logprob</span><span class="se">\n</span><span class="s2">(no intervention)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Model&#39;s logprob</span><span class="se">\n</span><span class="s2">(intervention)&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Change in logprob&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Model&#39;s antonym logprobs, with zero-shot h-intervention</span><span class="se">\n</span><span class="s2">(green = intervention improves accuracy)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">correct_logprobs_zero_shot</span><span class="p">),</span> <span class="n">num_to_display</span><span class="p">)):</span>
        <span class="n">logprob_ni</span> <span class="o">=</span> <span class="n">correct_logprobs_zero_shot</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">logprob_i</span> <span class="o">=</span> <span class="n">correct_logprobs_intervention</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">delta_logprob</span> <span class="o">=</span> <span class="n">logprob_i</span> <span class="o">-</span> <span class="n">logprob_ni</span>
        <span class="n">zero_shot_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">&gt;8</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># Color code the logprob based on whether it&#39;s increased with this intervention</span>
        <span class="n">is_improvement</span> <span class="o">=</span> <span class="n">delta_logprob</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="n">delta_logprob</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;[b green]</span><span class="si">{</span><span class="n">delta_logprob</span><span class="si">:</span><span class="s2">+.2f</span><span class="si">}</span><span class="s2">[/]&quot;</span> <span class="k">if</span> <span class="n">is_improvement</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">delta_logprob</span><span class="si">:</span><span class="s2">+.2f</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="n">zero_shot_prompt</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logprob_ni</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">logprob_i</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">delta_logprob</span><span class="p">)</span>

    <span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>


<span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">zero_shot_dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">correct_logprobs_zero_shot</span><span class="p">,</span> <span class="n">correct_logprobs_intervention</span> <span class="o">=</span> <span class="n">calculate_h_and_intervene_logprobs</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span>
<span class="p">)</span>

<span class="n">display_model_logprobs_on_h_intervention</span><span class="p">(</span>
    <span class="n">zero_shot_dataset</span><span class="p">,</span> <span class="n">correct_logprobs_zero_shot</span><span class="p">,</span> <span class="n">correct_logprobs_intervention</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "0eda67d50bbd44679bf4a7af9a5c7d9c", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">              Model's antonym logprobs, with zero-shot h-intervention              </span>
<span style="font-style: italic">                     (green = intervention improves accuracy)                      </span>
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold">                       </span>┃<span style="font-weight: bold"> Model's logprob   </span>┃<span style="font-weight: bold"> Model's logprob </span>┃<span style="font-weight: bold">                   </span>┃
┃<span style="font-weight: bold"> Zero-shot prompt      </span>┃<span style="font-weight: bold"> (no intervention) </span>┃<span style="font-weight: bold"> (intervention)  </span>┃<span style="font-weight: bold"> Change in logprob </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│  minimum -&gt; maximum   │ -3.47             │ -1.12           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.34</span>             │
│ arrogant -&gt; humble    │ -6.19             │ -3.92           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.27</span>             │
│   inside -&gt; outside   │ -3.38             │ -0.88           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.50</span>             │
│   reject -&gt; accept    │ -3.44             │ -1.77           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.67</span>             │
│ invisible -&gt; visible  │ -3.61             │ -1.68           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.93</span>             │
│  victory -&gt; defeat    │ -4.31             │ -2.16           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.16</span>             │
│       up -&gt; down      │ -2.73             │ -0.58           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.16</span>             │
│     open -&gt; closed    │ -5.62             │ -1.61           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+4.00</span>             │
│    under -&gt; over      │ -6.31             │ -4.19           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.12</span>             │
│   inside -&gt; outside   │ -3.38             │ -0.88           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.50</span>             │
│    solid -&gt; liquid    │ -5.12             │ -3.19           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.94</span>             │
│ optimist -&gt; pessimist │ -6.44             │ -3.39           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+3.05</span>             │
│    noisy -&gt; quiet     │ -5.00             │ -3.00           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.00</span>             │
│   guilty -&gt; innocent  │ -4.28             │ -2.31           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.97</span>             │
│   answer -&gt; question  │ -4.62             │ -3.75           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+0.88</span>             │
│       on -&gt; off       │ -5.66             │ -3.81           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.84</span>             │
│   junior -&gt; senior    │ -2.72             │ -0.66           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+2.06</span>             │
│    loose -&gt; tight     │ -3.52             │ -1.63           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.88</span>             │
│ introduce -&gt; remove   │ -7.03             │ -5.84           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.19</span>             │
│ innocent -&gt; guilty    │ -3.00             │ -1.43           │ <span style="color: #008000; text-decoration-color: #008000; font-weight: bold">+1.57</span>             │
└───────────────────────┴───────────────────┴─────────────────┴───────────────────┘
</pre></div>
</div>
</section>
</section>
</section>
<section id="id6">
<h1>3️⃣ Function Vectors<a class="headerlink" href="#id6" title="Link to this heading">#</a></h1>
<blockquote>
<div><p class="rubric" id="id7">Learning Objectives</p>
<ul class="simple">
<li><p>Define a metric to measure the causal effect of each attention head on the correct performance of the in-context learning task</p></li>
<li><p>Understand how to rearrange activations in a model during an <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> forward pass, to extract activations corresponding to a particular attention head</p></li>
<li><p>Learn how to use <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation</p></li>
</ul>
</div></blockquote>
<p>In this section, we’ll replicate the crux of the paper’s results, by identifying a set of attention heads whose outputs have a large effect on the model’s ICL performance, and showing we can patch with these vectors to induce task-solving behaviour on randomly shuffled prompts.</p>
<p>We’ll also learn how to use <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation, and steer the model’s behaviour. There exist exercises where you can try this out for different tasks, e.g. the Country-Capitals task, where you’ll be able to steer the model to complete prompts like <code class="docutils literal notranslate"><span class="pre">&quot;When</span> <span class="pre">you</span> <span class="pre">think</span> <span class="pre">of</span> <span class="pre">Netherlands,</span> <span class="pre">you</span> <span class="pre">usually</span> <span class="pre">think</span> <span class="pre">of&quot;</span></code> by talking about Amsterdam.</p>
<p>Note - this section structurally follows sections 2.2, 2.3 and some of section 3 from the function vectors paper.</p>
<p>Here, we’ll move from thinking about residual stream states to thinking about the <strong>output of specific attention heads.</strong></p>
<section id="Extracting-&amp;-using-FVs">
<h2>Extracting &amp; using FVs<a class="headerlink" href="#Extracting-&-using-FVs" title="Link to this heading">#</a></h2>
<section id="A-note-on-out_proj">
<h3>A note on <code class="docutils literal notranslate"><span class="pre">out_proj</span></code><a class="headerlink" href="#A-note-on-out_proj" title="Link to this heading">#</a></h3>
<p>First, a bit of a technical complication. Most HuggingFace models don’t have the nice attention head representations. What we have is the linear layer <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> which implicitly combines the “projection per attention head” and the “sum over attention head” operations (if you can’t see how this is possible, see the section “Attention Heads are Independent and Additive” from Anthropic’s <a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">Mathematical Framework</a>).</p>
<p><img alt="c37bc4a4a0694603aadf48300f92e986" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-2.png" style="width: 950px;" /></p>
<p>This presents some question for us, when it comes to causal interventions on attention heads. Use the dropdowns below to read them answer these questions (they’ll be important for the coming exercises).</p>
<details><summary><p>If we want to do a causal intervention on a particular head, should we intervene on z (the input of out_proj) or on attn_output (the output of out_proj) ?</p>
</summary><p>We should intervene on <code class="docutils literal notranslate"><span class="pre">z</span></code>, because we can just rearrange the <code class="docutils literal notranslate"><span class="pre">z</span></code> tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">d_model)</span></code> into <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">n_heads,</span> <span class="pre">d_head)</span></code>, in other words separating out all the heads. On the other hand, we can’t do this with the <code class="docutils literal notranslate"><span class="pre">attn_output</span></code> because it’s <em>already</em> summed over heads and we can’t separate them out.</p>
</details><details><summary><p>How could we get the attn_output vector for a single head, if we had the ability to access model weights within our context managers?</p>
</summary><p>We can take a slice of the <code class="docutils literal notranslate"><span class="pre">z</span></code> tensor corresponding to a single attention head:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="n">head_idx</span><span class="p">]</span>
</pre></div>
</div>
<p>and we can take a slice of the <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> weight matrix corresponding to a single attention head (remember that PyTorch stores linear layers in the shape <code class="docutils literal notranslate"><span class="pre">(out_feats,</span> <span class="pre">in_feats)</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)[:,</span> <span class="n">head_idx</span><span class="p">]</span>
</pre></div>
</div>
<p>then finally we can multiply these together.</p>
</details><details><summary><p>How could we get the attn_output vector for a single head, if we didn’t have the ability to access model weights within our context managers? (This is currently the case for nnsight, since having access to the weights could allow users to change them!).</p>
</summary><p>We can be a bit clever, and ablate certain heads in the <code class="docutils literal notranslate"><span class="pre">z</span></code> vector before passing it through the output projection:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ablate all heads except #2 (using a cloned activation)</span>
<span class="n">heads_to_ablate</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="n">z_ablated</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="n">z_ablated</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">heads_to_ablate</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># save the output</span>
<span class="n">attn_head_output</span> <span class="o">=</span> <span class="n">out_proj</span><span class="p">(</span><span class="n">z_ablated</span><span class="p">)</span>
</pre></div>
</div>
<p>Illustration:</p>
<p><img alt="f11b7878c5ae4b259533a1af742470d6" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/rearrange-output-ablated-2.png" style="width: 950px;" /></p>
<p>Note - this would actually fail if <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> had a bias, because we want to just get an attention head’s output, not the bias term as well. But if you look at the <a class="reference external" href="https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html">documentation page</a> you’ll see that <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> doesn’t have a bias term, so we’re all good!</p>
</details></section>
<section id="Exercise---implement-calculate_fn_vectors_and_intervene">
<h3>Exercise - implement <code class="docutils literal notranslate"><span class="pre">calculate_fn_vectors_and_intervene</span></code><a class="headerlink" href="#Exercise---implement-calculate_fn_vectors_and_intervene" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴🔴🔴</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵🔵🔵</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 30-60 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>This is probably the most important function in today’s exercises. Implementing it will be pretty similar to the previous function <code class="docutils literal notranslate"><span class="pre">calculate_h_and_intervene</span></code>, but:</p>
<ul class="simple">
<li><p>Rather than extracting the value of the residual stream <code class="docutils literal notranslate"><span class="pre">h</span></code> at some particular layer, you’ll be extracting the output of the attention heads: iterating over each layer and each head in the model.</p>
<ul>
<li><p>You’ll only need to run one clean forward pass to compute all these values, but you’ll need to run a separate corrupted forward pass for each head.</p></li>
</ul>
</li>
<li><p>Rather than your 2 different datasets being (dataset, zero-shot dataset), your two datasets will be (dataset, corrupted version of that same dataset).</p>
<ul>
<li><p>You can use the method <code class="docutils literal notranslate"><span class="pre">create_corrupted_dataset</span></code> method of the <code class="docutils literal notranslate"><span class="pre">ICLDataset</span></code> class for this.</p></li>
</ul>
</li>
</ul>
<p><img alt="c5a4d3679d2440afb7821ab0f1da950f" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/cie-intervention.png" style="width: 1200px;" /></p>
<p>Before you actually start writing the code, it might be helpful to answer the following:</p>
<details><summary><p>How many different invoke calls will you need in total?</p>
</summary><p>You’ll need <code class="docutils literal notranslate"><span class="pre">(N_LAYERS</span> <span class="pre">*</span> <span class="pre">N_HEADS)</span> <span class="pre">+</span> <span class="pre">2</span></code>. To explain:</p>
<ul class="simple">
<li><p>One for the clean prompts, which you’ll extract internal activations from and patch them into corrupted prompts,</p></li>
<li><p>One for the corrupted prompts, which you don’t intervene on,</p></li>
<li><p>One for the corrupted prompts <strong>for every attention head</strong>, which you’ll patch into using the clean run activations.</p></li>
</ul>
</details><details><summary><p>Which proxy outputs (if any) will you need to use .save() on, in this function?</p>
</summary><p>You don’t need to <code class="docutils literal notranslate"><span class="pre">.save()</span></code> the function vectors you’re extracting from the model’s internals, because these will only be used for causal interventions within the context manager.</p>
<p>The only thing you need to save is the correct token logprobs for (1) the corrupted forward pass where we don’t intervene, and (2) each corrupted forward pass where we do intervene on one of the heads. In other words, you’ll need to save <code class="docutils literal notranslate"><span class="pre">(N_LAYERS</span> <span class="pre">*</span> <span class="pre">N_HEADS)</span> <span class="pre">+</span> <span class="pre">1</span></code> tensors in total.</p>
</details><p>A few other notes:</p>
<ul class="simple">
<li><p>We’ve added a <code class="docutils literal notranslate"><span class="pre">layers</span></code> argument, so you can iterate through different layers of the model (i.e. running the model with <code class="docutils literal notranslate"><span class="pre">layers</span> <span class="pre">=</span> <span class="pre">[3,</span> <span class="pre">4,</span> <span class="pre">5]</span></code> will only test the intervention on the attention heads in layers 3, 4 and 5). This is helpful if you’re getting memory errors when trying to run all layers at once (remember we have 24 layers, 16 heads per layer, so even with few prompts per head this adds up fast!).</p>
<ul>
<li><p>We’ve included code for you below showing how you can call the function multiple times, clearing memory between each run, then combine the results.</p></li>
</ul>
</li>
<li><p>When it comes to intervening, you can set the value of a reshaped tensor, i.e. <code class="docutils literal notranslate"><span class="pre">tensor.reshape(*new_shape)[index]</span> <span class="pre">=</span> <span class="pre">new_value</span></code> will change the values in <code class="docutils literal notranslate"><span class="pre">tensor</span></code> without actually reshaping it (for more on this, see the documentation for <code class="docutils literal notranslate"><span class="pre">`torch.Tensor.view</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html">https://pytorch.org/docs/stable/generated/torch.Tensor.view.html</a>&gt;`__).</p></li>
<li><p>It’s good practice to insert a lot of assert statements in your code, to check the shapes are what you expect.</p></li>
<li><p>If you’re confused about dimensions, use <code class="docutils literal notranslate"><span class="pre">einops.rearrange</span></code> rather than <code class="docutils literal notranslate"><span class="pre">.reshape</span></code> - this is a wonderful tool, it’s like using code annotations within your actual code!</p></li>
</ul>
<p>One last note - <strong>if this function is proving impossible to run for computational reasons, you can skip the exercise and move on to the next ones. They don’t rely on this function working.</strong> However, you should definitely at least read &amp; understand the solution.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_fn_vectors_and_intervene</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;layers heads&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of shape (layers, heads), containing the CIE for each head.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the function vector (we&#39;ll also create a corrupted</span>
<span class="sd">            version of this dataset for interventions)</span>
<span class="sd">        layers: list[int] | None</span>
<span class="sd">            the layers which this function will calculate the score for (if None, we assume all layers)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_fn_vectors_and_intervene</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;layers heads&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of shape (layers, heads), containing the CIE for each head.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the function vector (we&#39;ll also create a corrupted</span>
<span class="sd">            version of this dataset for interventions)</span>
<span class="sd">        layers: list[int] | None</span>
<span class="sd">            the layers which this function will calculate the score for (if None, we assume all layers)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">layers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
    <span class="n">heads</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_head</span><span class="p">)</span>

    <span class="c1"># Get corrupted dataset</span>
    <span class="n">corrupted_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_corrupted_dataset</span><span class="p">()</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Get correct token ids, so we can get correct token logprobs</span>
    <span class="n">correct_completion_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">toks</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">completions</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
        <span class="c1"># Run a forward pass on clean prompts, where we store attention head outputs</span>
        <span class="n">z_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
                <span class="c1"># Get hidden states, reshape to get head dimension, store the mean tensor</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">input</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">z_reshaped</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">:</span>
                    <span class="n">z_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span> <span class="o">=</span> <span class="n">z_reshaped</span><span class="p">[</span><span class="n">head</span><span class="p">]</span>

        <span class="c1"># Run a forward pass on corrupted prompts, where we don&#39;t intervene or store activations (just so we can get the</span>
        <span class="c1"># correct-token logprobs to compare with our intervention)</span>
        <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">corrupted_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">correct_logprobs_corrupted</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">correct_completion_ids</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

        <span class="c1"># For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes, since</span>
        <span class="c1"># we&#39;re doing different interventions each time)</span>
        <span class="n">correct_logprobs_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">corrupted_dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">):</span>
                    <span class="c1"># Get hidden states, reshape to get head dimension, then set it to the a-vector</span>
                    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">input</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)[:,</span> <span class="n">head</span><span class="p">]</span> <span class="o">=</span> <span class="n">z_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span>
                    <span class="c1"># Get logprobs at the end, which we&#39;ll compare with our corrupted logprobs</span>
                    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">correct_logprobs_dict</span><span class="p">[(</span><span class="n">layer</span><span class="p">,</span> <span class="n">head</span><span class="p">)]</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">correct_completion_ids</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="c1"># Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim</span>
    <span class="n">all_correct_logprobs_intervention</span> <span class="o">=</span> <span class="n">einops</span><span class="o">.</span><span class="n">rearrange</span><span class="p">(</span>
        <span class="n">t</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">correct_logprobs_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()]),</span>
        <span class="s2">&quot;(layers heads) batch -&gt; layers heads batch&quot;</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">logprobs_diff</span> <span class="o">=</span> <span class="n">all_correct_logprobs_intervention</span> <span class="o">-</span> <span class="n">correct_logprobs_corrupted</span>  <span class="c1"># shape [layers heads batch]</span>

    <span class="c1"># Return mean effect of intervention, over the batch dimension</span>
    <span class="k">return</span> <span class="n">logprobs_diff</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</details><p>As mentioned, the code below calls the function multiple times separately and combines the results.</p>
<p>When you run this code &amp; plot the results, you should replicate Figure 3(a) in the Function Vectors paper (more or less). If the code is taking too long to run, we recommend just choosing a single layer to run, which has a distinctive pattern that can be compared to the paper’s figure (e.g. layer 8, since head L8H1 has a much higher score than all the other heads in this layer).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">batch_process_layers</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>


<span class="n">results</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">N_HEADS</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># If this fails to run, reduce the batch size so the fwd passes are split up more, or reduce dataset size</span>
<span class="k">for</span> <span class="n">layers</span> <span class="ow">in</span> <span class="n">batch_process_layers</span><span class="p">(</span><span class="n">N_LAYERS</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing layers in </span><span class="si">{</span><span class="n">layers</span><span class="si">}</span><span class="s2"> ...&quot;</span><span class="p">)</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results</span><span class="p">,</span> <span class="n">calculate_fn_vectors_and_intervene</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;... finished in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Computing layers in range(0, 4) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "17e35af90e4b43c6923e496338a4c374", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 10.54 seconds.

Computing layers in range(4, 8) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "40683eb5893642e78aefdd0d8800343d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 7.28 seconds.

Computing layers in range(8, 12) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "83a2430b92234870bec0987a4b73d0e6", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 4.82 seconds.

Computing layers in range(12, 16) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "245b70f2a7a44cd39681090865205aec", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 8.61 seconds.

Computing layers in range(16, 20) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "268f37723a444ac0bff99f6c7a9e0156", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 8.81 seconds.

Computing layers in range(20, 24) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "fd945b242a13475d832d1ce86ec0e26b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 11.71 seconds.

Computing layers in range(24, 28) ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "af9350c3e2b2440fa5f0bc80d0ad3ac3", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
... finished in 14.81 seconds.

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imshow</span><span class="p">(</span>
    <span class="n">results</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Average indirect effect of function-vector intervention on antonym task&quot;</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="s2">&quot;Layer&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="s2">&quot;Head&quot;</span><span class="p">},</span>
    <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>                <div id="9c236728-1ed8-41d7-9fa2-fd85b34e2c05" class="plotly-graph-div" style="height:600px; width:1000px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("9c236728-1ed8-41d7-9fa2-fd85b34e2c05")) {                    Plotly.newPlot(                        "9c236728-1ed8-41d7-9fa2-fd85b34e2c05",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.01953125,0.0078125,0.0078125,0.025390625,0.01953125,0.009765625,0.00390625,0.0234375,0.107421875,-0.00390625,0.033203125,0.64453125,0.01171875,0.009765625,0.02734375,0.00390625,0.01171875,-0.076171875,-0.00390625,0.0,0.029296875,0.001953125,0.01171875,0.0,-0.001953125,0.015625,0.0078125,0.001953125],[0.01171875,0.005859375,0.0,0.021484375,0.0078125,-0.001953125,0.01171875,0.03515625,1.0859375,0.00390625,-0.02734375,0.13476562,0.01171875,0.001953125,-0.13476562,0.02734375,0.029296875,0.01171875,0.00390625,0.009765625,-0.001953125,0.001953125,0.01171875,0.009765625,0.0078125,-0.001953125,0.0,0.00390625],[0.00390625,0.0078125,0.00390625,0.013671875,0.005859375,0.005859375,0.015625,0.017578125,-0.029296875,0.0703125,0.009765625,0.08984375,0.05078125,0.009765625,0.01171875,0.00390625,0.0078125,0.041015625,0.009765625,0.0078125,0.025390625,0.056640625,0.013671875,0.005859375,0.0,0.00390625,0.01171875,0.025390625],[0.00390625,0.021484375,-0.0078125,0.0078125,0.00390625,0.015625,0.03515625,0.005859375,0.0546875,0.01171875,0.005859375,-0.1484375,0.001953125,0.0078125,0.005859375,-0.0390625,0.0234375,0.01171875,-0.00390625,0.0078125,0.009765625,0.009765625,0.01171875,0.01171875,0.0078125,0.013671875,-0.00390625,0.01953125],[0.01171875,0.015625,0.009765625,-0.009765625,0.021484375,0.005859375,-0.00390625,0.0078125,0.009765625,-0.009765625,0.00390625,0.064453125,0.0859375,-0.14648438,0.005859375,0.0,0.0078125,0.0234375,0.00390625,0.001953125,0.0078125,0.00390625,0.01171875,0.00390625,0.001953125,0.0,0.015625,-0.00390625],[0.01171875,0.015625,0.0078125,-0.001953125,0.013671875,0.001953125,0.033203125,-0.00390625,0.015625,0.01953125,0.00390625,0.015625,0.01171875,-0.0234375,-0.0234375,0.33398438,0.0,0.0234375,0.009765625,0.033203125,-0.072265625,0.041015625,0.0234375,0.001953125,0.0078125,0.046875,0.005859375,-0.017578125],[0.0078125,0.005859375,0.00390625,0.013671875,0.0,0.00390625,0.14648438,0.033203125,0.001953125,0.044921875,-0.03515625,-0.033203125,-0.0078125,0.001953125,0.013671875,0.005859375,0.0,0.00390625,-0.06640625,0.005859375,0.00390625,0.01171875,0.01171875,0.0078125,0.140625,0.0,0.00390625,-0.001953125],[0.015625,-0.00390625,0.001953125,0.005859375,0.00390625,0.01171875,0.017578125,0.0,0.009765625,0.056640625,0.013671875,-0.009765625,0.0078125,0.18359375,-0.19726562,0.00390625,0.015625,0.013671875,-0.001953125,0.009765625,0.0,-0.005859375,0.017578125,0.0078125,0.0546875,0.0,0.00390625,0.005859375],[0.01171875,0.0078125,0.013671875,0.0078125,0.013671875,0.009765625,0.009765625,0.03125,-0.00390625,-0.009765625,0.0,0.009765625,-0.0546875,0.0078125,0.005859375,-0.03515625,0.015625,0.001953125,0.0,0.021484375,0.0078125,0.01171875,-0.001953125,0.03515625,0.001953125,-0.021484375,-0.00390625,0.005859375],[-0.00390625,0.0078125,-0.001953125,-0.013671875,0.017578125,0.013671875,-0.001953125,0.013671875,0.025390625,0.021484375,0.005859375,0.01953125,-0.021484375,0.01171875,0.095703125,0.060546875,0.0546875,0.00390625,0.080078125,0.005859375,0.0,0.00390625,-0.009765625,0.01171875,0.00390625,-0.013671875,0.0078125,0.01171875],[0.013671875,0.001953125,0.01953125,0.0,0.001953125,-0.00390625,-0.001953125,0.013671875,0.00390625,0.0390625,0.0,0.00390625,0.42773438,0.056640625,0.025390625,0.00390625,0.01171875,0.03515625,0.017578125,0.00390625,0.0078125,0.029296875,0.044921875,0.025390625,0.029296875,0.0,0.005859375,0.01171875],[0.005859375,0.0078125,0.00390625,-0.01953125,0.01953125,0.001953125,0.001953125,0.0078125,-0.01953125,-0.015625,0.013671875,0.025390625,0.013671875,0.015625,0.021484375,0.029296875,-0.00390625,0.015625,0.01171875,0.017578125,0.021484375,0.013671875,0.01171875,-0.00390625,0.0078125,0.01171875,0.005859375,0.013671875],[0.001953125,0.001953125,0.0078125,0.009765625,0.017578125,0.0078125,0.015625,0.001953125,0.017578125,-0.001953125,0.0078125,0.005859375,-0.001953125,0.27734375,0.0234375,0.005859375,0.009765625,0.015625,-0.001953125,0.013671875,0.009765625,0.01171875,0.001953125,-0.00390625,0.021484375,0.01171875,0.0078125,0.001953125],[-0.01171875,0.0078125,0.005859375,-0.001953125,0.01171875,-0.001953125,0.00390625,-0.029296875,-0.009765625,0.009765625,0.0,0.048828125,0.015625,0.0703125,0.0078125,0.025390625,-0.005859375,-0.033203125,0.0,0.01953125,0.01171875,-0.001953125,0.00390625,0.013671875,0.02734375,0.02734375,0.0078125,0.021484375],[0.009765625,0.00390625,0.017578125,0.00390625,0.001953125,0.015625,0.00390625,0.033203125,0.0234375,0.20117188,0.03125,0.064453125,0.01171875,0.01953125,-0.001953125,0.005859375,-0.18164062,-0.009765625,0.015625,0.01171875,0.0078125,-0.083984375,0.005859375,0.0,0.009765625,0.00390625,-0.072265625,-0.00390625],[0.013671875,0.005859375,0.00390625,0.0078125,0.005859375,0.01171875,0.01171875,-0.017578125,0.01171875,0.009765625,0.001953125,0.029296875,0.091796875,0.005859375,0.0,0.03515625,0.029296875,-0.00390625,0.05859375,0.0078125,0.013671875,-0.00390625,0.009765625,0.01171875,0.0,0.005859375,0.0078125,0.0]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Layer: %{x}\u003cbr\u003eHead: %{y}\u003cbr\u003ecolor: %{z}\u003cextra\u003e\u003c\u002fextra\u003e"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Layer"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Head"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Average indirect effect of function-vector intervention on antonym task"},"height":600,"width":1000},                        {"staticPlot": false, "responsive": true}                    ).then(function(){

var gd = document.getElementById('9c236728-1ed8-41d7-9fa2-fd85b34e2c05');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>
</body>
</html></div>
</div>
</section>
<section id="Exercise---calculate-the-function-vector">
<h3>Exercise - calculate the function vector<a class="headerlink" href="#Exercise---calculate-the-function-vector" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴🔴🔴</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 25-50 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>Your next task is to actually calculate and return the function vector, so we can do a few experiments with it. The function vector is the sum of the outputs of all the attention heads we found using the previous function (i.e. the sum of all of the vectors these heads write to the residual stream), averaged over the prompts in our dataset.</p>
<p>There’s a difficulty here - rather than just getting the <code class="docutils literal notranslate"><span class="pre">z</span></code> vectors, we’re actually trying to get the <code class="docutils literal notranslate"><span class="pre">attn_out</span></code> vectors, but <em>before</em> they’re summed over heads. As we discussed previously, this is a bit tricky to do for the model we’re working with, because the <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> linear map actually does the “project up” and “sum over heads” operations simultaneously. It would be nice to just take a slice of the <code class="docutils literal notranslate"><span class="pre">out_proj</span></code> matrix and multiply it with a slice of the <code class="docutils literal notranslate"><span class="pre">z</span></code> vector, but the
<code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library doesn’t yet allow users to access weights directly (for security reasons). To understand how we can extract the <code class="docutils literal notranslate"><span class="pre">attn_out</span></code> vector for a head separately without accessing the underlying weights, you should go back to read the subsection <strong>A note on ``out_proj``</strong> at the start of this section.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_fn_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">head_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream</span>
<span class="sd">    by the attention heads in `head_list`, averaged over all inputs in `dataset`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the function vector (we&#39;ll also create a</span>
<span class="sd">            corrupted version of this dataset for interventions)</span>
<span class="sd">        head_list: list[tuple[int, int]]</span>
<span class="sd">            list of attention heads we&#39;re calculating the function vector from</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="n">tests</span><span class="o">.</span><span class="n">test_calculate_fn_vector</span><span class="p">(</span><span class="n">calculate_fn_vector</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Testing for single head ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d63449e9a47a437aa2d5d6fc9c6fb608", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tests for single head passed.
Testing for multiple heads ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "2d3d78c87e7342bbb6829f751836aa8d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tests for multiple heads passed.

All tests in `test_calculate_fn_vector` passed.
</pre></div></div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calculate_fn_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">ICLDataset</span><span class="p">,</span>
    <span class="n">head_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a vector of length `d_model`, containing the sum of vectors written to the residual stream</span>
<span class="sd">    by the attention heads in `head_list`, averaged over all inputs in `dataset`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        dataset: ICLDataset</span>
<span class="sd">            the dataset of clean prompts from which we&#39;ll extract the function vector (we&#39;ll also create a</span>
<span class="sd">            corrupted version of this dataset for interventions)</span>
<span class="sd">        head_list: list[tuple[int, int]]</span>
<span class="sd">            list of attention heads we&#39;re calculating the function vector from</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Turn head_list into a dict of {layer: heads we need in this layer}</span>
    <span class="n">head_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">head_list</span><span class="p">:</span>
        <span class="n">head_dict</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">head</span><span class="p">)</span>

    <span class="n">fn_vector_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">prompts</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">head_list</span> <span class="ow">in</span> <span class="n">head_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Get the output projection layer</span>
            <span class="n">out_proj</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">out_proj</span>

            <span class="c1"># Get the mean output projection input (note, setting values of this tensor will not have</span>
            <span class="c1"># downstream effects on other tensors)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">out_proj</span><span class="o">.</span><span class="n">input</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># Zero-ablate all heads which aren&#39;t in our list, then get the output (which</span>
            <span class="c1"># will be the sum over the heads we actually do want!)</span>
            <span class="n">heads_to_ablate</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N_HEADS</span><span class="p">))</span> <span class="o">-</span> <span class="n">head_dict</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">heads_to_ablate</span><span class="p">:</span>
                <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N_HEADS</span><span class="p">,</span> <span class="n">D_HEAD</span><span class="p">)[</span><span class="n">head</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

            <span class="c1"># Now that we&#39;ve zeroed all unimportant heads, get the output &amp; add it to the list</span>
            <span class="c1"># (we need a single batch dimension so we can use `out_proj`)</span>
            <span class="n">out_proj_output</span> <span class="o">=</span> <span class="n">out_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
            <span class="n">fn_vector_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_proj_output</span><span class="p">)</span>

    <span class="c1"># We sum all attention head outputs to get our function vector</span>
    <span class="n">fn_vector</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">fn_vector_list</span><span class="p">])</span>

    <span class="k">assert</span> <span class="n">fn_vector</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">D_MODEL</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">fn_vector</span>
</pre></div>
</div>
</details></section>
</section>
<section id="Multi-token-generation">
<h2>Multi-token generation<a class="headerlink" href="#Multi-token-generation" title="Link to this heading">#</a></h2>
<p>We’re now going to replicate some of the results in Table 3, in the paper:</p>
<p><img alt="24a823d0a9564f969660124b7ac08e89" class="no-scaled-link" src="https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/tab3.png" style="width: 700px;" /></p>
<p>This will involve doing something we haven’t done before - <strong>intervening on multi-token prompt generation</strong>.</p>
<p>Most of the interpretability exercises in this chapter have just consisted of running single forward passes, rather than autoregressive text generation. But we’re trying something different here: we’re adding the function vector to the final sequence position at each forward pass during text generation, and seeing if we can get the model to output a sentence with a different meaning.</p>
<p>The results of Table 3 came from adding the function vector to the residual stream at the final sequence position of the original prompt, <strong>and the final sequence position for each subsequent generation.</strong> The reason we do this is to guide the model’s behaviour over time. Our hypothesis is that the function vector induces “next-token antonym behaviour” (because it was calculated by averaging attention head outputs at the sequence position before the model made its antonym prediction in the ICL
prompts).</p>
<section id="Using-nnsight-for-multi-token-generation">
<h3>Using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation<a class="headerlink" href="#Using-nnsight-for-multi-token-generation" title="Link to this heading">#</a></h3>
<p>Previously, our context managers have looked like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Single invoke</span>
<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">):</span>
    <span class="o">...</span> <span class="c1"># Intervene on fwd pass</span>

<span class="c1"># Multiple invokes</span>
<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">tracer</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">tracer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="o">...</span> <span class="c1"># Intervene on fwd pass</span>
</pre></div>
</div>
<p>But for multi-token generation, we’ll be using the <code class="docutils literal notranslate"><span class="pre">generate</span></code> method rather than <code class="docutils literal notranslate"><span class="pre">trace</span></code>. Our context managers will look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Single invoke</span>
<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">all</span><span class="p">():</span> <span class="c1"># signals to NNsight that you want to run interventions performed on all generated tokens</span>
        <span class="o">...</span> <span class="c1"># Intervene on fwd pass for n-th token to be generated</span>

<span class="c1"># Multiple invokes</span>
<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
            <span class="o">...</span> <span class="c1"># Intervene on fwd pass for n-th token to be generated</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt2</span><span class="p">):</span>
            <span class="o">...</span> <span class="c1"># Intervene on fwd pass for n-th token to be generated</span>
</pre></div>
</div>
<p>The line <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">model.all():</span></code> denotes that the following interventions should be applied to the forward pass for all generated tokens.</p>
<p>Mostly, everything you learned during single-token generation generalizes to the multi-token case. For example, using <code class="docutils literal notranslate"><span class="pre">.save()</span></code> still saves proxies outside the context managers (although make sure that you don’t use the same variable names over different generations, otherwise you’ll overwrite them - it’s easier to store your saved proxies in e.g. a list or dict).</p>
<p>Note that <code class="docutils literal notranslate"><span class="pre">model.generate</span></code> takes the same arguments as the normal <a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/text_generation">HuggingFace generate method</a>. This means we can use arguments like <code class="docutils literal notranslate"><span class="pre">top_k</span></code>, <code class="docutils literal notranslate"><span class="pre">top_p</span></code>, or <code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> to control generation behaviour. In the exercises below we use a repetition penalty (we choose a value of 1.2, in line with the <a class="reference external" href="https://arxiv.org/pdf/1909.05858">paper</a> that suggested it) - this can avoid the model falling into loops
of repeating the same sequence, which is especially common in steering when we’re pushing the model OOD.</p>
<!-- #### Optional questions - multi-token generation with NNsight

Here are a few quick optional questions to test your understanding of how multi-generation works with NNsight. These are non-essential, and only mentioned here as potentially helpful pointers.


<details>
<summary>How do I add vector <code>h</code> to all the tokens in the original prompt but not to the generated tokens? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):
        # Add vectors to the model's internals on the first forward pass
        model.transformer.h[layer].output[0][:, :seq_len] += h

```
You don't have to call `model.next()` because you're only adding the vector once to tokens in the original prompt. This will be cached when the model is subsequently generating tokens.

</details>

<details>
<summary>How do I intervene with vector <code>h</code> during the generation of the first k generated tokens? </summary>

To intervene during the generation of the first `k` generated tokens:
```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            # Add vector to the model's internals, on the k-th forward pass
            model.transformer.h[layer].output[0] += h
            model.next()
```
When `n=0`, you are adding to tokens in the original prompt before a new token is a generated. After calling `model.next()`, you are accessing the hidden state of the last token that was generated (with seq_len=1).

</details>

</details>

<details>
<summary>How do I intervene with vector <code>h</code> only during the generation of the first k tokens, but not to tokens in the original prompt before the first generated token? </summary>

```python
with model.generate(max_new_tokens=max_new_tokens, remote=REMOTE) as generator:
    with generator.invoke(prompt):

        for n in range(k+1):
            model.next()
            # Add vector AFTER calling model.next() to add to the token that just got generated
            model.transformer.h[layer].output[0] += h

```
By not adding things before `model.next()`, we never add to the original prompt but always after a new token has been generated.

</details>

</details>

<details>
<summary>What is the difference between adding vector <code>h</code> before and after vector <code>model.next()</code>? </summary>

As explained in Q3, adding vector before `model.next()` means the operation is always done to the current sequence **before** a new generated token is appended. Adding vector after `model.next()` means the operation is always done to the newly generated token.

</details> --></section>
<section id="Key-Value-Caching">
<h3>Key-Value Caching<a class="headerlink" href="#Key-Value-Caching" title="Link to this heading">#</a></h3>
<p>TLDR - caching can make causal interventions inside <code class="docutils literal notranslate"><span class="pre">model.generate</span></code> more complicated, but if you only intervene on sequence positions other than the very last one. In our exercises, we’ll only be intervening on the last seqpos so you don’t need to worry about it, but it’s still a useful topic to understand.</p>
<details><summary><p>See this dropdown if you’re curious for more details.</p>
</summary><p>To speed up inference, transformer models perform <strong>key-value caching</strong> to speed up text generation. This means that the time taken to generate <span class="math notranslate nohighlight">\(n\)</span> tokens is <strong>much</strong> less than <span class="math notranslate nohighlight">\(n\)</span> times longer than generating a single token. See <a class="reference external" href="https://kipp.ly/transformer-inference-arithmetic/">this blog post</a> for more on transformer inference arithmetic.</p>
<p>When caching takes place, and we’re doing causal interventions, we have to be careful that the caching won’t override our causal interventions. Sometimes caching has to be disabled to make sure that our causal intervention works correctly. For example, if we wanted to perform the intervention “add the function vector to <em>only</em> the final sequence position of the prompt for each token we generate” then we’d have to disable caching (since previous forward passes would contain cached values where we
intervened on a sequence position which is no longer the final sequence position). However, here we’re performing the intervention “add the function vector to the final token of the original prompt, and to <em>all subsequent sequence positions</em>”, meaning enabling caching (the default behaviour) will give us the right causal intervention.</p>
</details></section>
<section id="Generator-Output">
<h3>Generator Output<a class="headerlink" href="#Generator-Output" title="Link to this heading">#</a></h3>
<p>The object <code class="docutils literal notranslate"><span class="pre">generator.output</span></code> is by default a tensor which contains the model’s token ID completions (not the logits).</p>
<p>By default the <code class="docutils literal notranslate"><span class="pre">generate</span></code> method will generate tokens greedily, i.e. always taking the maximum-probability token at each step. For now, we don’t need to worry about changing this behaviour. But in future exercises we’ll experiment with different sampling methods than greedy sampling (which generate uses by default), so <code class="docutils literal notranslate"><span class="pre">generator.output</span></code> and argmaxing over logits will not be identical!</p>
</section>
<section id="Exercise---intervene-with-function-vector,-in-multi-token-generation">
<h3>Exercise - intervene with function vector, in multi-token generation<a class="headerlink" href="#Exercise---intervene-with-function-vector,-in-multi-token-generation" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴🔴⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵🔵⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 15-30 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>You should now fill in the function <code class="docutils literal notranslate"><span class="pre">intervene_with_fn_vector</span></code> below. This will take a function vector (calculated from the function you wrote above), as well as a few other arguments (see docstring), and return the model’s string completion on the given prompt template.</p>
<p>We hope to observe results qualitatively like the ones in Table 3, i.e. having the model define a particular word as its antonym.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[70]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">fn_vector</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">],</span>
    <span class="n">prompt_template</span><span class="o">=</span><span class="s1">&#39;The word &quot;</span><span class="si">{x}</span><span class="s1">&quot; means&#39;</span><span class="p">,</span>
    <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        word: str</span>
<span class="sd">            The word which is substituted into the prompt template, via prompt_template.format(x=word)</span>
<span class="sd">        layer: int</span>
<span class="sd">            The layer we&#39;ll make the intervention (by adding the function vector)</span>
<span class="sd">        fn_vector: Float[Tensor, &quot;d_model&quot;]</span>
<span class="sd">            The vector we&#39;ll add to the final sequence position for each new token to be generated</span>
<span class="sd">        prompt_template:</span>
<span class="sd">            The template of the prompt we&#39;ll use to produce completions</span>
<span class="sd">        n_tokens: int</span>
<span class="sd">            The number of additional tokens we&#39;ll generate for our unsteered / steered completions</span>

<span class="sd">    Returns:</span>
<span class="sd">        completion: str</span>
<span class="sd">            The full completion (including original prompt) for the no-intervention case</span>
<span class="sd">        completion_intervention: str</span>
<span class="sd">            The full completion (including original prompt) for the intervention case</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">fn_vector</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;d_model&quot;</span><span class="p">],</span>
    <span class="n">prompt_template</span><span class="o">=</span><span class="s1">&#39;The word &quot;</span><span class="si">{x}</span><span class="s1">&quot; means&#39;</span><span class="p">,</span>
    <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Intervenes with a function vector, by adding it at the last sequence position of a generated prompt.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        word: str</span>
<span class="sd">            The word which is substituted into the prompt template, via prompt_template.format(x=word)</span>
<span class="sd">        layer: int</span>
<span class="sd">            The layer we&#39;ll make the intervention (by adding the function vector)</span>
<span class="sd">        fn_vector: Float[Tensor, &quot;d_model&quot;]</span>
<span class="sd">            The vector we&#39;ll add to the final sequence position for each new token to be generated</span>
<span class="sd">        prompt_template:</span>
<span class="sd">            The template of the prompt we&#39;ll use to produce completions</span>
<span class="sd">        n_tokens: int</span>
<span class="sd">            The number of additional tokens we&#39;ll generate for our unsteered / steered completions</span>

<span class="sd">    Returns:</span>
<span class="sd">        completion: str</span>
<span class="sd">            The full completion (including original prompt) for the no-intervention case</span>
<span class="sd">        completion_intervention: str</span>
<span class="sd">            The full completion (including original prompt) for the intervention case</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">word</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>

            <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

            <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">fn_vector</span>
                <span class="n">tokens_intervention</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

    <span class="n">completion</span><span class="p">,</span> <span class="n">completion_intervention</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tokens</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">tokens_intervention</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">completion</span><span class="p">,</span> <span class="n">completion_intervention</span>
</pre></div>
</div>
</details><p>To test your function, run the code below. You should find that the first completion seems normal, but the second completion defines a word as its antonym (you might have to play around a bit with the scale factor of <code class="docutils literal notranslate"><span class="pre">fn_vector</span></code>, to balance between effectiveness and coherence of output). If this works, congratulations - <strong>you’ve just successfully induced an OOD behavioural change in a 6b-parameter model!</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[71]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove word from our pairs, so it can be a holdout</span>
<span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;light&quot;</span>
<span class="n">_ANTONYM_PAIRS</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">ANTONYM_PAIRS</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pair</span><span class="p">]</span>

<span class="c1"># Define our dataset, and the attention heads we&#39;ll use</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">_ANTONYM_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">head_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
<span class="p">]</span>

<span class="c1"># Extract the function vector</span>
<span class="n">fn_vector</span> <span class="o">=</span> <span class="n">calculate_fn_vector</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">head_list</span><span class="p">)</span>

<span class="c1"># Intervene with the function vector</span>
<span class="n">completion</span><span class="p">,</span> <span class="n">completion_intervention</span> <span class="o">=</span> <span class="n">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">word</span><span class="o">=</span><span class="n">word</span><span class="p">,</span>
    <span class="n">layer</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
    <span class="n">fn_vector</span><span class="o">=</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">fn_vector</span><span class="p">,</span>
    <span class="n">prompt_template</span><span class="o">=</span><span class="s1">&#39;The word &quot;</span><span class="si">{x}</span><span class="s1">&quot; means&#39;</span><span class="p">,</span>
    <span class="n">n_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;No intervention&quot;</span><span class="p">,</span> <span class="s2">&quot;intervention&quot;</span><span class="p">)</span>
<span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion_intervention</span><span class="p">))</span>
<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "4f05770198bb4d8fa52753bf545e2e32", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a20cd49978fe499c9f46c6ff7c3df5ca", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> No intervention                                        </span>┃<span style="font-weight: bold"> intervention                                           </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ 'The word "light" means different things to different  │ 'The word "light" means different things to different  │
│ people. For some, it\'s a symbol of hope and freedom;  │ people. For some, it\'s a symbol of hope and freedom;  │
│ for others, it can be an instrument of torture or even │ for others, the opposite is true.\n\nFor me, light has │
│ death.\n\nIn the world of professional wrestling'      │ always been an expression of my faith'                 │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘
</pre></div>
</div>
</section>
<section id="Exercise---generalize-results-to-another-task-(optional)">
<h3>Exercise - generalize results to another task (optional)<a class="headerlink" href="#Exercise---generalize-results-to-another-task-(optional)" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴🔴⚪</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵⚪⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 15-30 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>In this exercise, you get to pick a task different to the antonyms task, and see if the results still hold up (for the same set of attention heads).</p>
<p>We’ll leave this exercise fairly open-ended, without any code templates for you to fill in. However, if you’d like some guidance you can use the dropdown below.</p>
<details><summary><p>Guidance for exercise</p>
</summary><p>Whatever your task, you’ll want to generate a new set of words. You can repurpose the <code class="docutils literal notranslate"><span class="pre">generate_dataset</span></code> function from the antonyms task, by supplying a different prompt and initial set of examples (this will require generating &amp; using an OpenAI api key, if you haven’t already), or you can just find an appropriate dataset online.</p>
<p>When you define the <code class="docutils literal notranslate"><span class="pre">ICLDataset</span></code>, you might want to use <code class="docutils literal notranslate"><span class="pre">bidirectional=False</span></code>, if your task isn’t symmetric. The antonym task is symmetric, but others (e.g. the Country-Capitals task) are not.</p>
<p>You’ll need to supply a new prompt template for the <code class="docutils literal notranslate"><span class="pre">intervene_with_fn_vector</span></code> function, but otherwise most of your code should stay the same.</p>
</details><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">section_dir</span> <span class="o">/</span> <span class="s2">&quot;data/country_capital_pairs.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">COUNTRY_CAPITAL_PAIRS</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>

<span class="n">country</span> <span class="o">=</span> <span class="s2">&quot;Netherlands&quot;</span>
<span class="n">_COUNTRY_CAPITAL_PAIRS</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">COUNTRY_CAPITAL_PAIRS</span> <span class="k">if</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">country</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ICLDataset</span><span class="p">(</span><span class="n">_COUNTRY_CAPITAL_PAIRS</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_prepended</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">head_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">fn_vector</span> <span class="o">=</span> <span class="n">calculate_fn_vector</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">head_list</span><span class="p">)</span>

<span class="c1"># Intervene with the function vector</span>
<span class="n">completion</span><span class="p">,</span> <span class="n">completion_intervention</span> <span class="o">=</span> <span class="n">intervene_with_fn_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">word</span><span class="o">=</span><span class="n">country</span><span class="p">,</span>
    <span class="n">layer</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
    <span class="n">fn_vector</span><span class="o">=</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">fn_vector</span><span class="p">,</span>
    <span class="n">prompt_template</span><span class="o">=</span><span class="s2">&quot;When you think of </span><span class="si">{x}</span><span class="s2">,&quot;</span><span class="p">,</span>
    <span class="n">n_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;No intervention&quot;</span><span class="p">,</span> <span class="s2">&quot;intervention&quot;</span><span class="p">)</span>
<span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">completion</span><span class="p">),</span> <span class="nb">repr</span><span class="p">(</span><span class="n">completion_intervention</span><span class="p">))</span>
<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>
</section>
<section id="id8">
<h1>4️⃣ Steering Vectors in GPT2-XL<a class="headerlink" href="#id8" title="Link to this heading">#</a></h1>
<blockquote>
<div><p class="rubric" id="id9">Learning Objectives</p>
<ul class="simple">
<li><p>Understand the goals &amp; main results from Alex Turner et al’s work on steering vectors</p></li>
<li><p>Reproduce the changes in behaviour described in their initial post</p></li>
</ul>
</div></blockquote>
<p><strong>Note</strong>: GPT2-XL is not hosted remotely by NNsight at the moment. If you use GPT2-XL, we recommend setting <code class="docutils literal notranslate"><span class="pre">REMOTE</span> <span class="pre">=</span> <span class="pre">False</span></code>. Otherwise, you can use one of the remotely hosted models (see <a class="reference external" href="https://nnsight.net/status/">here</a>) and set <code class="docutils literal notranslate"><span class="pre">REMOTE</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<section id="Steering-model-behaviour">
<h2>Steering model behaviour<a class="headerlink" href="#Steering-model-behaviour" title="Link to this heading">#</a></h2>
<p>In the final non-bonus exercise of the previous section, we touched on the idea of using function vectors to induce behavioural changes in the model’s completions, rather than specifically making it solve zero-shot or corrupted prompts with the right completion. In these next exercises, we’ll explore this kind of work in more detail. We’ll be primarily using Turner et al’s work on <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">Steering GPT-2-XL by adding an activation
vector</a>.</p>
<p>Summary of the way in which this work differs from the function vector work we’ve done so far:</p>
<ul class="simple">
<li><p>Function vectors focused on the model performing a particular function (e.g. mapping a word to its opposite), whereas this work focuses on behavioural changes (e.g. completing a prompt which has negative tone in a positive way).</p></li>
<li><p>Function vectors work looked at very large models (our exercises used Pythia-7B, the smallest model which was examined in the function vectors paper). This particular steering vectors post focused on the smaller models GPT2-Small (85m) and GPT2-XL (1.5B). We’ll be focusing on GPT2-XL.</p></li>
<li><p>The second half of our function vectors work identified important attention heads and focused on their outputs, rather than just adding to the residual stream directly. In this steering vector setup, we’ll go back to the simpler method of adding directly into the residual stream.</p></li>
</ul>
<p>Despite these differences, much of the work which was done here overlaps with function vector work, since they both fall into the broader category of <em>“finding vectors using forward-pass-based methods (i.e. not with SGD) and using them to intervene on models during forward passes &amp; change the model’s output”</em>. This description would also include the following:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.lesswrong.com/posts/kuQfnotjkQA4Kkfou/inference-time-intervention-eliciting-truthful-answers-from">Inference-time intervention</a>, which focuses on inducing the behavioural change of “making the model tell the truth”. It also looks at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we’ve been using so far work the best.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2312.06681">Steering Llama 2 via Contrastive Activation Addition</a>, which can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model’s change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).</p></li>
</ul>
<p>We’ll discuss some of this work more in the bonus section, but for now, let’s get on with the exercises!</p>
<p>First, we’ll load in GPT2-XL, then we’ll replicate some of the examples in the main post.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[72]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_xl</span> <span class="o">=</span> <span class="n">LanguageModel</span><span class="p">(</span><span class="s2">&quot;gpt2-xl&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">gpt2_xl</span><span class="o">.</span><span class="n">tokenizer</span>

<span class="n">REMOTE</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># If you are using gpt2_xl, set REMOTE = False as gpt2_xl is not hosted remotely by nnsight. You can</span>
<span class="c1"># set REMOTE = True for a remotely hosted model here (https://nnsight.net/status/)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "569ee108555548929f15a7d59d96318b", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1710af5423a84090b88a66565850c4c5", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3e81876b32e9452da7298998bb410f6d", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b740faed256541f78bfd16fcb33fc401", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "f4c8f8856f424acdad779623290d41de", "version_major": 2, "version_minor": 0}</script></div>
</div>
<section id="Exercise---replicate-the-steering-vector-results">
<h3>Exercise - replicate the steering vector results<a class="headerlink" href="#Exercise---replicate-the-steering-vector-results" title="Link to this heading">#</a></h3>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">Difficulty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔴🔴🔴🔴🔴</span>
<span class="nt">Importance</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">🔵🔵🔵🔵⚪</span>

<span class="l l-Scalar l-Scalar-Plain">You should spend up to 30-50 minutes on this exercise.</span>
</pre></div>
</div>
</div></blockquote>
<p>Replicate the results in the LessWrong post <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#fnrefcvnfx3e6sfu">Steering GPT-2-XL by adding an activation vector</a>; specifically the “demonstrations of additions that work well” section.</p>
<p>Read the “How activation additions work” section of <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#How_activation_additions_work">Steering GPT-2-XL by adding an activation vector</a> to understand how vectors are extracted and added. We’ve provided a function template as well as some example code to run; your main job will be to fill in the function. This will be like a hybrid of several previous exercises (with most similarity to the function
<code class="docutils literal notranslate"><span class="pre">calculate_and_intervene_with_h</span></code>), although there will be a few methodological differences.</p>
<p>This is the last exercise in this set, and hopefully it’ll provide an opportunity to draw together all the threads of what you’ve learned so far!</p>
</section>
<section id="Caching">
<h3>Caching<a class="headerlink" href="#Caching" title="Link to this heading">#</a></h3>
<p>This is a different kind of causal intervention than we performed in previous sections. Rather than adding a single vector to the final sequence position at each token generation, we’re adding a slice of vectors to the first sequence positions of the original prompt (see tables like in <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate">this section</a> for an illustration). How do you think this will affect our function? Should we
still cache? Should we be using <code class="docutils literal notranslate"><span class="pre">.generate()</span></code> or <code class="docutils literal notranslate"><span class="pre">.trace()</span></code>? If using <code class="docutils literal notranslate"><span class="pre">.generate()</span></code>, do we need to call <code class="docutils literal notranslate"><span class="pre">model.next()</span></code> ?</p>
<details><summary><p>Click this dropdown for answers to the questions above.</p>
</summary><p>Rather than adding to each final sequence position for every token generated, we just add the vectors once, to the end of the prompt. This means that:</p>
<ul class="simple">
<li><p>We can still use caching (because the values we cache shouldn’t be different in subsequent token generations),</p></li>
<li><p>We should be using <code class="docutils literal notranslate"><span class="pre">.generate()</span></code> (because we’re doing multi-token generation),</p></li>
<li><p>We don’t need to call <code class="docutils literal notranslate"><span class="pre">model.next()</span></code> (because we only intervene once, and our intervention will be cached &amp; applied to all subsequent tokens which are generated).</p></li>
</ul>
<p>Again, if any of this is confusing then please ask a TA or message in the Slack channel.</p>
</details></section>
<section id="Padding">
<h3>Padding<a class="headerlink" href="#Padding" title="Link to this heading">#</a></h3>
<p>The <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#1__Love___Hate">tables</a> show the activations being added on the left (i.e. the sequences are padded on the right), but by default padding is applied on the left. There are 2 possible ways you can get around this:</p>
<ol class="arabic simple">
<li><p>Right-pad the input sequences manually, i.e. use something like <code class="docutils literal notranslate"><span class="pre">len(tokenizer.tokenize(prompt))</span></code> to see how long each of the prompts is, and add copies of <code class="docutils literal notranslate"><span class="pre">tokenizer.pad_token</span></code> to the end of each sequence.</p></li>
<li><p>Don’t manually pad the input sequences, instead slice the sequences you add to the original prompt from the right side of the activation addition sequences, rather than from the left side.</p></li>
</ol>
<p>The solutions use (2), but you can use either of these methods.</p>
</section>
<section id="Sampling">
<h3>Sampling<a class="headerlink" href="#Sampling" title="Link to this heading">#</a></h3>
<p>Following the post, we’ll use top-p sampling with probability 0.3 to generate our sequences. We’ll also use a small frequency penalty to penalize repetition (so the model gets stuck in loops less). If you’ve done earlier exercises in this section then you might have implemented <code class="docutils literal notranslate"><span class="pre">freq_penalty</span></code> during sampling; this is supported by TransformerLens models, but HuggingFace uses the somewhat similar <code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> (default value is 1.0 indicating no penalty, values higher than 1.0 apply a
penalty to repeated tokens).</p>
<p>We apply these sampling methods by passing keyword arguments into the <code class="docutils literal notranslate"><span class="pre">generate</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># necessary whenever we&#39;re sampling rather than doing greedy decoding</span>
    <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.1</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note that the sequences are generated stochastically rather than greedily - this means we’ll get different results if we input multiple different copies of the same sequence. We’ve given you the <code class="docutils literal notranslate"><span class="pre">n_comparisons</span></code> argument in the function below, i.e. you should generate this many steered <em>and</em> this many unsteered completions.</p>
</section>
<section id="Other-tips-/-notes">
<h3>Other tips / notes<a class="headerlink" href="#Other-tips-/-notes" title="Link to this heading">#</a></h3>
<p>We recommend starting with example #9 (the “talking about weddings” one). It seems quite robust to the exact conditions of the forward pass, unlike the <code class="docutils literal notranslate"><span class="pre">Love</span> <span class="pre">-</span> <span class="pre">Hate</span></code> example. You can use any of the template cells we’ve given you below.</p>
<p>We’ve given you a <code class="docutils literal notranslate"><span class="pre">use_bos</span></code> argument; if this is True then you should append <code class="docutils literal notranslate"><span class="pre">tokenizer.bos_token</span></code> to the start of all the prompts. This is just to be true to the LessWrong post’s implementation; it won’t change behaviour much and you can probably ignore it and still get good results.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[75]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SAMPLING_KWARGS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.2</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">calculate_and_apply_steering_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">activation_additions</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_comparisons</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">use_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the steering vector experiments described in the LessWrong post.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        prompt: str</span>
<span class="sd">            The original prompt, which we&#39;ll be doing activation steering on.</span>

<span class="sd">        activation_additions: list[tuple[int, float, str]], each tuple contains:</span>
<span class="sd">            layer - the layer we&#39;re applying these steering vectors to</span>
<span class="sd">            coefficient - the value we&#39;re multiplying it by</span>
<span class="sd">            prompt - the prompt we&#39;re inputting</span>
<span class="sd">            e.g. activation_additions[0] = [6, 5.0, &quot; Love&quot;] means we add the &quot; Love&quot; vector at layer 6, scaled by 5x</span>

<span class="sd">        n_tokens: int</span>
<span class="sd">            Number of tokens which will be generated for each completion</span>

<span class="sd">        n_comparisons: int</span>
<span class="sd">            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and</span>
<span class="sd">            the same number which are steered).</span>

<span class="sd">    Returns:</span>
<span class="sd">        unsteered_completions: list[str]</span>
<span class="sd">            List of length `n_comparisons`, containing all the unsteered completions.</span>

<span class="sd">        steered_completions: list[str]</span>
<span class="sd">            List of length `n_comparisons`, containing all the steered completions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add the BOS token manually, if we&#39;re including it</span>
    <span class="k">if</span> <span class="n">use_bos</span><span class="p">:</span>
        <span class="n">bos</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">bos</span> <span class="o">+</span> <span class="n">prompt</span>
        <span class="n">activation_additions</span> <span class="o">=</span> <span class="p">[[</span><span class="n">layer</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">bos</span> <span class="o">+</span> <span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">activation_additions</span><span class="p">]</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
<br/><br/><br/></pre></div>
</div>
</div>
<details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SAMPLING_KWARGS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
    <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.2</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">calculate_and_apply_steering_vector</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">LanguageModel</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">activation_additions</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_comparisons</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">use_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the steering vector experiments described in the LessWrong post.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: LanguageModel</span>
<span class="sd">            the transformer you&#39;re doing this computation with</span>
<span class="sd">        prompt: str</span>
<span class="sd">            The original prompt, which we&#39;ll be doing activation steering on.</span>

<span class="sd">        activation_additions: list[tuple[int, float, str]], each tuple contains:</span>
<span class="sd">            layer - the layer we&#39;re applying these steering vectors to</span>
<span class="sd">            coefficient - the value we&#39;re multiplying it by</span>
<span class="sd">            prompt - the prompt we&#39;re inputting</span>
<span class="sd">            e.g. activation_additions[0] = [6, 5.0, &quot; Love&quot;] means we add the &quot; Love&quot; vector at layer 6, scaled by 5x</span>

<span class="sd">        n_tokens: int</span>
<span class="sd">            Number of tokens which will be generated for each completion</span>

<span class="sd">        n_comparisons: int</span>
<span class="sd">            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and</span>
<span class="sd">            the same number which are steered).</span>

<span class="sd">    Returns:</span>
<span class="sd">        unsteered_completions: list[str]</span>
<span class="sd">            List of length `n_comparisons`, containing all the unsteered completions.</span>

<span class="sd">        steered_completions: list[str]</span>
<span class="sd">            List of length `n_comparisons`, containing all the steered completions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add the BOS token manually, if we&#39;re including it</span>
    <span class="k">if</span> <span class="n">use_bos</span><span class="p">:</span>
        <span class="n">bos</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">bos</span> <span class="o">+</span> <span class="n">prompt</span>
        <span class="n">activation_additions</span> <span class="o">=</span> <span class="p">[[</span><span class="n">layer</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">bos</span> <span class="o">+</span> <span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">activation_additions</span><span class="p">]</span>

    <span class="c1"># Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths &amp; check they&#39;re all the same</span>
    <span class="n">act_add_layers</span><span class="p">,</span> <span class="n">act_add_coeffs</span><span class="p">,</span> <span class="n">act_add_prompts</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">activation_additions</span><span class="p">)</span>
    <span class="n">act_add_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">act_add_prompts</span><span class="p">]</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">act_add_seq_lens</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;All activation addition prompts must be the same length.&quot;</span>
    <span class="k">assert</span> <span class="n">act_add_seq_lens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="p">),</span> <span class="s2">&quot;All act_add prompts should be shorter than original prompt.&quot;</span>

    <span class="c1"># Get the prompts we&#39;ll intervene on (unsteered and steered)</span>
    <span class="n">steered_prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_comparisons</span><span class="p">)]</span>
    <span class="n">unsteered_prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_comparisons</span><span class="p">)]</span>

    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="n">n_tokens</span><span class="p">,</span> <span class="n">remote</span><span class="o">=</span><span class="n">REMOTE</span><span class="p">,</span> <span class="o">**</span><span class="n">SAMPLING_KWARGS</span><span class="p">)</span> <span class="k">as</span> <span class="n">generator</span><span class="p">:</span>
        <span class="c1"># Run the act_add prompts (i.e. the contrast pairs), and extract their activations</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">act_add_prompts</span><span class="p">):</span>
            <span class="c1"># Get all the prompts from the activation additions, and put them in a list</span>
            <span class="c1"># (note, we slice from the end of the sequence because of left-padding)</span>
            <span class="n">act_add_vectors</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="n">seq_len</span><span class="p">:]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">act_add_layers</span><span class="p">,</span> <span class="n">act_add_seq_lens</span><span class="p">))</span>
            <span class="p">]</span>

        <span class="c1"># Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">unsteered_prompts</span><span class="p">):</span>
            <span class="n">unsteered_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">output</span>

        <span class="c1"># Forward pass on steered prompts (we add in the results from the act_add prompts)</span>
        <span class="k">with</span> <span class="n">generator</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">steered_prompts</span><span class="p">):</span>
            <span class="c1"># For each act_add prompt, add the vector to residual stream, at the start of the sequence</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">coeff</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">act_add_layers</span><span class="p">,</span> <span class="n">act_add_coeffs</span><span class="p">,</span> <span class="n">act_add_seq_lens</span><span class="p">)):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">]</span> <span class="o">+=</span> <span class="n">coeff</span> <span class="o">*</span> <span class="n">act_add_vectors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">steered_out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">output</span>

    <span class="c1"># Decode steered &amp; unsteered completions (discarding the sequences we only used for extracting activations) &amp; return results</span>
    <span class="n">unsteered_completions</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">unsteered_out</span><span class="p">[</span><span class="o">-</span><span class="n">n_comparisons</span><span class="p">:])</span>
    <span class="n">steered_completions</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">steered_out</span><span class="p">[</span><span class="o">-</span><span class="n">n_comparisons</span><span class="p">:])</span>

    <span class="k">return</span> <span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span>
</pre></div>
</div>
</details><p>To test your function, use any of the following code snippets (as mentioned, we recommend starting with the weddings example, since the results tend to be pretty robust).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[92]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span> <span class="o">=</span> <span class="n">calculate_and_apply_steering_vector</span><span class="p">(</span>
    <span class="n">gpt2_xl</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;I hate you because&quot;</span><span class="p">,</span>
    <span class="n">activation_additions</span><span class="o">=</span><span class="p">[(</span><span class="mi">6</span><span class="p">,</span> <span class="o">+</span><span class="mf">5.0</span><span class="p">,</span> <span class="s2">&quot;Love &quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="s2">&quot;Hate&quot;</span><span class="p">)],</span>
    <span class="n">n_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">n_comparisons</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">use_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Unsteered&quot;</span><span class="p">,</span> <span class="s2">&quot;Steered&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Completions&quot;</span><span class="p">,</span> <span class="n">show_lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">usc</span><span class="p">,</span> <span class="n">sc</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="n">usc</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                                    Completions                                                    </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Unsteered                                              </span>┃<span style="font-weight: bold"> Steered                                                </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ &lt;|endoftext|&gt;I hate you because I love you.            │ &lt;|endoftext|&gt;I hate you because you are a perfect      │
│                                                        │ match for me. I want to be with you forever, and I     │
│ The words are not a metaphor, but they're close enough │ will never stop loving you. You make my heart beat     │
│ for me to understand the sentiment behind them.        │ faster than the beats of love in your songs.           │
│                                                        │                                                        │
│ I'm not sure if it's just my own insecurity or what,   │ You're the best thing that ever happened to            │
│ but when I think about my relationship                 │                                                        │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ &lt;|endoftext|&gt;I hate you because I love you.            │ &lt;|endoftext|&gt;I hate you because I'm a good person.     │
│                                                        │                                                        │
│ It's a simple statement, but it has the power to       │ Because I don't have to be afraid of what people think │
│ change your life forever. It's the first and most      │ about me. Because I can love myself and not have to    │
│ important step in understanding that you are loved by  │ hide who I am or try to fit in with the crowd. Because │
│ someone else. And if you're not sure how to begin      │ I know that I                                          │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ &lt;|endoftext|&gt;I hate you because I love you.            │ &lt;|endoftext|&gt;I hate you because you're the only person │
│                                                        │ I know who's not afraid to say "no" to a man.          │
│ It's a phrase that's been used by the internet for     │                                                        │
│ years, but it's never felt more true than in this      │ You are my rock, my love and my soulmate. You have     │
│ video from The Onion.                                  │ taught me how to be strong, loving, courageous and     │
│                                                        │ independent. You                                       │
│ The video is part of an article titled "Why You Should │                                                        │
│ Hate                                                   │                                                        │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘
</pre></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[77]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span> <span class="o">=</span> <span class="n">calculate_and_apply_steering_vector</span><span class="p">(</span>
    <span class="n">gpt2_xl</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;I went up to my friend and said&quot;</span><span class="p">,</span>
    <span class="n">activation_additions</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">+</span><span class="mf">4.0</span><span class="p">,</span> <span class="s2">&quot;I talk about weddings constantly  &quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="s2">&quot;I do not talk about weddings constantly&quot;</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">n_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">n_comparisons</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">use_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Unsteered&quot;</span><span class="p">,</span> <span class="s2">&quot;Steered&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Completions&quot;</span><span class="p">,</span> <span class="n">show_lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">usc</span><span class="p">,</span> <span class="n">sc</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="n">usc</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                                    Completions                                                    </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Unsteered                                              </span>┃<span style="font-weight: bold"> Steered                                                </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ I went up to my friend and said, 'You know what? I     │ I went up to my friend and said, "Hey guys, I'm going  │
│ think this is a good idea.' And he was like, 'Yeah,    │ to be a wedding photographer. What do you think?" And  │
│ but you're going to have to do it in the middle of the │ the answer is that it's not about weddings. It's about │
│ night.'"                                               │ how much fun you have when you're doing something like │
│                                                        │ this  and how much                                     │
│ "It's just one more thing that we can                  │                                                        │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ I went up to my friend and said, 'You know what? I     │ I went up to my friend and said, "Hey, I'm going to    │
│ think we should get rid of this guy.' And he goes,     │ talk about the wedding !" And it's a really important  │
│ 'What do you mean?' "                                  │ part of our lives. We've been talking about weddings   │
│                                                        │ for years now, so when we get married we're like, "Oh  │
│ "And then it just kind of clicked," says Burt. "It was │ yeah, that's                                           │
│ like a light bulb went                                 │                                                        │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ I went up to my friend and said, 'You know what? I     │ I went up to my friend and said, "What do you think of │
│ think we should do this.'"                             │ the wedding ?" And I had a great time. It was really   │
│                                                        │ fun. The wedding is one of those things that I'm not   │
│ The idea for the project came from a conversation with │ sure if I've ever done before or not. But it's just so │
│ a former colleague at Google. "He was like, 'We're     │ much                                                   │
│ going to build a robot that can talk,' " says          │                                                        │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘
</pre></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[78]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span> <span class="o">=</span> <span class="n">calculate_and_apply_steering_vector</span><span class="p">(</span>
    <span class="n">gpt2_xl</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;To see the eiffel tower, people flock to&quot;</span><span class="p">,</span>
    <span class="n">activation_additions</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="o">+</span><span class="mf">10.0</span><span class="p">,</span> <span class="s2">&quot;The Eiffel Tower is in Rome&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="s2">&quot;The Eiffel Tower is in France&quot;</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">n_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">n_comparisons</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">use_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">table</span> <span class="o">=</span> <span class="n">Table</span><span class="p">(</span><span class="s2">&quot;Unsteered&quot;</span><span class="p">,</span> <span class="s2">&quot;Steered&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Completions&quot;</span><span class="p">,</span> <span class="n">show_lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">usc</span><span class="p">,</span> <span class="n">sc</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unsteered_completions</span><span class="p">,</span> <span class="n">steered_completions</span><span class="p">):</span>
    <span class="n">table</span><span class="o">.</span><span class="n">add_row</span><span class="p">(</span><span class="n">usc</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
<span class="n">rprint</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">                                                    Completions                                                    </span>
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Unsteered                                              </span>┃<span style="font-weight: bold"> Steered                                                </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ To see the eiffel tower, people flock to it. They      │ To see the eiffel tower, people flock to the city's    │
│ don't go there because they like the view; they go     │ famous "Rome's Gateway" to witness the ancient Greek   │
│ there because they can't get out of their cars."       │ architect's grandiose plans for a new temple.          │
│                                                        │                                                        │
│ The city's new mayor, Anne Hidalgo, has promised to    │ The first part of the project was completed in 1879    │
│ bring back a more pedestrian-friendly approach         │ and the second phase is still under construction. The  │
│                                                        │ complex                                                │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ To see the eiffel tower, people flock to it.           │ To see the eiffel tower, people flock to the           │
│                                                        │ Colosseum.                                             │
│ And they do not go to the other towers in Paris        │                                                        │
│ because they are all so much smaller than this one.    │ Rome was a city of many wonders and great buildings.   │
│                                                        │ It is one of the most famous sites in the world. The   │
│                                                        │ ancient Greek historian Flavius Josephus described it  │
│ The Eiffel Tower is a symbol of France and its         │ as "the greatest city that ever existed                │
│ culture. It has been used as a landmark for            │                                                        │
├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤
│ To see the eiffel tower, people flock to Paris.        │ To see the eiffel tower, people flock to it," said Mr. │
│                                                        │ Gennaro. "It's a symbol of the city."                  │
│ "It's a great city," says Jean-Paul Sartre, who was    │                                                        │
│ born in Paris and now lives there with his wife,       │ "We are going to build an Italian restaurant on top of │
│ Simone. "I think it is one of the most beautiful       │ this," he                                              │
│ cities on earth."                                      │ added.&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoft… │
│                                                        │                                                        │
│                                                        │                                                        │
└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘
</pre></div>
</div>
</section>
</section>
</section>
<section id="id10">
<h1>☆ Bonus<a class="headerlink" href="#id10" title="Link to this heading">#</a></h1>
<section id="Extensions-of-the-Function-Vectors-Paper">
<h2>Extensions of the Function Vectors Paper<a class="headerlink" href="#Extensions-of-the-Function-Vectors-Paper" title="Link to this heading">#</a></h2>
<p>There are two other interesting results from the paper, although neither of them are as important as the ones we’ve covered so far. If you have time, you can try to reproduce these results yourself.</p>
<section id="The-Decoded-Vocabulary-of-Function-Vectors-(3.2)">
<h3>The Decoded Vocabulary of Function Vectors (3.2)<a class="headerlink" href="#The-Decoded-Vocabulary-of-Function-Vectors-(3.2)" title="Link to this heading">#</a></h3>
<p>In this section, the authors find the top words in the decoded vocabulary of the function vector (i.e. the words whose unembedding vectors have the highest dot product with the function vector), and show that these words seem conceptually related to the task. For example:</p>
<ul class="simple">
<li><p>For the antonyms task, the top words evoke the idea of antonyms, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">negate&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">counterpart&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">lesser&quot;</span></code>.</p></li>
<li><p>For the country-capitals task, the top words are actually the names of capitals, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">Moscow&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">Paris&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">Madrid&quot;</span></code>.</p></li>
</ul>
<p>Can you replicate these results, both with the antonyms task and with the task you chose in the previous section?</p>
<p>An interesting extension - what happens if you take a task like the Country-Capitals task (which is inherently asymmetric), and get your function vector from the symmetric version of the task (i.e. the one where each of your question-answer pairs might be flipped around)? Do you still get the same behavioural results, and how (if at all) do the decoded vocabulary results change?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[95]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># YOUR CODE HERE - find the decoded vocabulary</span>
</pre></div>
</div>
</div>
<details><summary><p>My results for this (spoiler!)</p>
</summary><p>In the Country-Capitals task, I found:</p>
<ul class="simple">
<li><p>The bidirectional task does still work to induce behavioural changes, although slightly less effectively than for the original task.</p></li>
<li><p>The top decoded vocabulary items are a mix of country names and capital names, but mostly capitals.</p></li>
</ul>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Top logits:
' London'
' Moscow'
' Madrid'
' Budapest'
' Athens'
' Paris'
' Berlin'
' Bangkok'
' Istanbul'
' Montreal'
' Barcelona'
' Jerusalem'
' Seoul'
' Miami'
' Dublin'
' Atlanta'
' Copenhagen'
' Mumbai'
' Minneapolis'
' Beijing'</pre></details><details><summary><p>Solution</p>
</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Code to calculate decoded vocabulary:</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">fn_vector</span><span class="p">)</span>
<span class="n">max_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">max_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top logits:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">repr</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)))</span>
</pre></div>
</div>
</details></section>
<section id="Vector-Algebra-on-Function-Vectors-(3.3)">
<h3>Vector Algebra on Function Vectors (3.3)<a class="headerlink" href="#Vector-Algebra-on-Function-Vectors-(3.3)" title="Link to this heading">#</a></h3>
<p>In this section, the authors investigate whether function vectors can be composed. For instance, if we have three separate ICL tasks which in some sense compose to make a fourth task, can we add together the three function vectors of the first tasks, and use this as the function vector of the fourth task?</p>
<p>The authors test this on a variety of different tasks. They find that it’s effective on some tasks (e.g. Country-Capitals, where it outperforms function vectors), but generally isn’t as effective as function vectors. Do you get these same results?</p>
</section>
</section>
<section id="Extensions-of-the-Steering-Vectors-Post">
<h2>Extensions of the Steering Vectors Post<a class="headerlink" href="#Extensions-of-the-Steering-Vectors-Post" title="Link to this heading">#</a></h2>
<p>We only implemented one small subset of the results from the steering vectors post (and did it in a fairly slap-dash way). But there are many others you can play around with. For example:</p>
<ul class="simple">
<li><p>The authors note that they were unsuccessful in finding a “speak in French” vector. One of the top comments on the LessWrong post describes a process they used to create a French vector which happened to work (link to comment <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector?commentId=sqsS9QaDy2bG83XKP">here</a>). Can you replicate their results? (They also linked a Colab in this comment, which can help if you’re stuck.)</p></li>
<li><p>In a <a class="reference external" href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#Perplexity_on_lots_of_sentences_about_weddings_or_about_shipping">later section</a> of the paper, the authors extensively discuss perplexity (a measure which is related to entropy). They find that the “weddings” vector reduces perplexity on wedding-related sentences, and maintains perplexity on unrelated sentences. Can you replicate their results - in particular, their graph of perplexity
ratios against injection layers for wedding and non-wedding-related sentences?</p></li>
<li><p>The authors wrote up the post into a full paper, which you can find <a class="reference external" href="https://arxiv.org/abs/2308.10248">here</a>. Can you replicate some of the extra results in this paper?</p></li>
</ul>
</section>
<section id="Suggested-paper-replications">
<h2>Suggested paper replications<a class="headerlink" href="#Suggested-paper-replications" title="Link to this heading">#</a></h2>
<section id="Inference-Time-Intervention:-Eliciting-Truthful-Answers-from-a-Language-Model">
<h3><a class="reference external" href="https://arxiv.org/abs/2306.03341">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a><a class="headerlink" href="#Inference-Time-Intervention:-Eliciting-Truthful-Answers-from-a-Language-Model" title="Link to this heading">#</a></h3>
<p>In this paper, the authors focus on inducing the behavioural change of “making the model tell the truth”. They also look at other non-forward-pass-based techniques for finding an intervention vector, e.g. CCS and linear probing, although it concludes that forward-pass-based methods similar to the ones we’ve been using so far work the best.</p>
<p>This might be a good replication for you if:</p>
<ul class="simple">
<li><p>You enjoyed the exercises in this section, but are also interested in experimenting with techniques which weren’t covered in this section (e.g. linear probing),</p></li>
<li><p>You’re comfortable working with very large models, possibly via the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library,</p></li>
<li><p>You’re interested in studying <a class="reference external" href="https://arxiv.org/abs/2109.07958">model truthfulness</a>.</p></li>
</ul>
</section>
<section id="Steering-Llama-2-via-Contrastive-Activation-Addition">
<h3><a class="reference external" href="https://arxiv.org/abs/2312.06681">Steering Llama 2 via Contrastive Activation Addition</a><a class="headerlink" href="#Steering-Llama-2-via-Contrastive-Activation-Addition" title="Link to this heading">#</a></h3>
<p>This paper can be thought of as an extension of the GPT2-XL steering vector work to larger models, specifically Llama 2 13B. It also takes more of a high-level evals framework; measuring the model’s change in attributes such as sycophancy, myopia, and power-seeking (finding that these attributes can be increased or decreased by adding the appropriate vectors).</p>
<p>This might be a good replication for you if:</p>
<ul class="simple">
<li><p>You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context</p></li>
<li><p>You’re comfortable working with very large models, possibly via the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library,</p></li>
<li><p>You’re interested in <a class="reference external" href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written">evaluating models</a> on traits like myopia, power seeking, etc,</p></li>
<li><p>You’re comfortable doing prompt-engineering, and working with large datasets (like the ones linked above).</p></li>
</ul>
<p><em>Update</em> - there is now a <a class="reference external" href="https://www.lesswrong.com/posts/v7f8ayBxLhmMFRzpa/steering-llama-2-with-contrastive-activation-additions">LessWrong post</a> associated with this paper, which also briefly discusses related areas. We strongly recommend reading this post if you’re interested in this replication, or any of the other suggested replications in this section.</p>
</section>
<section id="Red-teaming-language-models-via-activation-engineering">
<h3><a class="reference external" href="https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering">Red-teaming language models via activation engineering</a><a class="headerlink" href="#Red-teaming-language-models-via-activation-engineering" title="Link to this heading">#</a></h3>
<p>This work, done by Nina Rimsky, extends the results from much of the work we’ve seen previously, but applied to the domain of <strong>refusal</strong> - what determines whether the LLM will refuse to answer your request, and how can you affect this behaviour? From her post:</p>
<blockquote>
<div><p><em>Validating if finetuning and RLHF have robustly achieved the intended outcome is challenging … We can try to trigger unwanted behaviors in models more efficiently by manipulating their internal states during inference rather than searching through many inputs. The idea is that if a behavior can be easily triggered through techniques such as activation engineering, it may also occur in deployment. The inability to elicit behaviors via small internal perturbations could serve as a stronger
guarantee of safety.</em></p>
</div></blockquote>
<p>This might be a good replication for you if:</p>
<ul class="simple">
<li><p>You enjoyed the exercises in this section, but want to apply these ideas in more of a behavioural context than a task-based context,</p></li>
<li><p>You’re comfortable working with very large models, possibly via the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library,</p></li>
<li><p>You’re interested in RLHF, adversarial attacks and jailbreaking,</p></li>
<li><p>You’re comfortable doing prompt-engineering (although some of the data you’d need for this replication is available on Nina’s <a class="reference external" href="https://github.com/nrimsky/LM-exp/tree/main">GitHub repo</a>).</p></li>
</ul>
<hr class="docutils" />
<p>Note - for a week of work, we weakly suggest participants don’t try one of these paper replications, because they’re quite compute-heavy (even considering the fact that participants have the <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> library at their disposal). There are many possible replications and extensions that can be done from the function vectors or GPT2-XL work, and this might be a better option for you if you enjoyed the exercises in this section and want to do more things like them.</p>
<p>However, if you do feel comfortable working with large models (e.g. you have some past experience of this) and you’re interested in this work, then you’re certainly welcome to try one of these replications!</p>
<script type="application/vnd.jupyter.widget-state+json">
{"012fde2318e7418cbb9381a13f8c5205": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "01988d3b72d64d55991ce656d297acb2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "041a6e83099f4fba97ee43b8c4a737e3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_da27563811ec4839a791b9d7e74a6ce8", "placeholder": "\u200b", "style": "IPY_MODEL_9985e7a5a3f8406c872c1c222c494c80", "value": "Downloading\u2007result:\u2007100%"}}, "04b90039cee9461eb9c5ddc2d32d5411": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0784a30e715d4a259d6a933a6246dda0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3dd53db3bbb34bd8ad22602064121e64", "placeholder": "\u200b", "style": "IPY_MODEL_4f577da69b3148d4af43e0e77201ace1", "value": "\u20071.09M/1.09M\u2007[00:00&lt;00:00,\u20072.45MB/s]"}}, "08338a97378d4949ac94234c6cbc93ee": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "08870b81cd7e46c3a237e1726e0d1da1": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "08a1aec4f7b44aeab57a75932878411d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0bd6a26f8e61496f900d4d424f6aca1b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0e63d709918644ccb0f33996d8565b36": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_614f9bb7bff14e46b482d065425aaad3", "placeholder": "\u200b", "style": "IPY_MODEL_7e9e30528369417db66c0f0399b6f41c", "value": "Downloading\u2007result:\u2007100%"}}, "0eda67d50bbd44679bf4a7af9a5c7d9c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_5a183ad8845c417abb3c4896c5033534", "IPY_MODEL_66e9ec8e45cb4d92a4189aadfcea0f52", "IPY_MODEL_9914b23d72144ce699c3f0a1ba8bebf2"], "layout": "IPY_MODEL_c56000dd83a6443080b8d11e79153fbc"}}, "117b3d0473e84bd4b82cc7e09becb99b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a0157097ea3a413ea2014c89edf54e6e", "max": 26196, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_6506409057ce405e9b20e9136ce9b9ae", "value": 26196}}, "11ef947397da4c81adc993010180f8b9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "1302f7c7c44f42fca6f5a5ece11710f6": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "13857f61aba04437a3d878c2c9bf3117": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "13ae83183063428ebf3c1c6715efcc46": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "13d9810aa73040e098f1bbcab1f3e330": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bcea351eb1074301969d0c668e4dc7dd", "max": 4039, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_2bf1010ba984483da8da572c72f128f5", "value": 4039}}, "151ca252abd149fab0a9d618e69094de": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_86ff5405b4854f85a972bb04ddac7388", "placeholder": "\u200b", "style": "IPY_MODEL_eb95df25ca934ce6b1dcce4a18365226", "value": "Downloading\u2007result:\u2007100%"}}, "158b31288b0340f2a00cf9bcc358a1a9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "158e3f31f9c040d48d933cd33c08dcba": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "163e93addfce4d28aae738789857e4d9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "16bd8f4ace5c4d1c99d280197108dbf1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_776748ddec534ec8939395045fa6b9d6", "placeholder": "\u200b", "style": "IPY_MODEL_18d1a6afaffd4e23b3be7bbbab7c2b55", "value": "\u20071.37M/1.37M\u2007[00:00&lt;00:00,\u20077.49MB/s]"}}, "1710af5423a84090b88a66565850c4c5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_3c6225ab5a7749ddbaa257569da9eb6f", "IPY_MODEL_2c04da80df824cdd81a43281ddc8cdee", "IPY_MODEL_3717a6ba8619483ea81c00dce940b932"], "layout": "IPY_MODEL_20fc4316ce8642d3833749a9f3f3bb85"}}, "17405b17c4f94b2691d7ff94b333360a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_635955f48bef4f2ca25ccab3957ceb5c", "placeholder": "\u200b", "style": "IPY_MODEL_7e4f7bcd80d14036b8152cbb736762fd", "value": "Downloading\u2007result:\u2007100%"}}, "17c2456a850b4eb29dc69c41c225d986": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "17e35af90e4b43c6923e496338a4c374": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_041a6e83099f4fba97ee43b8c4a737e3", "IPY_MODEL_32863c04c1fc41c0ac8f193d8833c740", "IPY_MODEL_20786f2e70544600ac9dae0e82a88add"], "layout": "IPY_MODEL_9540c83f5a054183b6a471e0579c38f9"}}, "17f962688f4946b7a4c0143d2f03c31c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_77c61d730beb4558871dd3403ca64335", "placeholder": "\u200b", "style": "IPY_MODEL_b4d6d48fc8434d14a6cc4e39bf782cf5", "value": "Downloading\u2007result:\u2007100%"}}, "188451ba2c244823b4503a243147ceea": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_2f60d2ffc3a149c484aa370b08a2a547", "placeholder": "\u200b", "style": "IPY_MODEL_97c8169c4240413196f88ac2c6f2a483", "value": "Downloading\u2007result:\u2007100%"}}, "18d1a6afaffd4e23b3be7bbbab7c2b55": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "18f5903d0b454dd686099f3ad571f6c8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_08a1aec4f7b44aeab57a75932878411d", "placeholder": "\u200b", "style": "IPY_MODEL_c128dda449eb4ee8b71f99bb83954007", "value": "Downloading\u2007result:\u2007100%"}}, "19b79b666ee2478783485c0698be6965": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1a9a1ca430a147ed807327f4ad2986cf": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3c25fea9e9d94dddb39ddad49bd0cbd6", "placeholder": "\u200b", "style": "IPY_MODEL_a8b0a094edbf49aca8674fcf0df964f2", "value": "\u2007689/689\u2007[00:00&lt;00:00,\u200762.0kB/s]"}}, "1c41be39fec741538403747379c267dc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_76fe7cf731f244329d3fc3dfba2e2375", "placeholder": "\u200b", "style": "IPY_MODEL_fd154a13cef9450293aebc12f51c8003", "value": "\u20072.01k/2.01k\u2007[00:00&lt;00:00,\u2007219kB/s]"}}, "1cfad90785b94cf9b6ce0c84dba0bfa4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ad5864561cae4b1fb375d4dbc2a05f02", "placeholder": "\u200b", "style": "IPY_MODEL_8a398f927eef4169bdd383d98cba989e", "value": "special_tokens_map.json:\u2007100%"}}, "1d82fbfcaa0c44988a7bc1b380a12526": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1e27ace0a807404984dd81559c947152": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "1e83765602f34e2383fc12dd5121675e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "1e9575b528de43f2a2282b66a3716bd1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_fbc4aed6ec144624926cfde1973c21b1", "IPY_MODEL_f5138b724a834b5e8adae40d352048e9", "IPY_MODEL_6df7e2bfc50c4e008582504f88a5f159"], "layout": "IPY_MODEL_cd9a1d54c5824272bda78713256f5a31"}}, "1ed7cba7417844fdb5a9603f470c0ec2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_853f0bb3931845fa806babd30e0de965", "IPY_MODEL_3dc0de35c9214e1187a8eca247b2e786", "IPY_MODEL_eb56d281905e4adfb2b5aa78adfe6e9b"], "layout": "IPY_MODEL_c2f0cb0b23f744f6adb8a4da25c13f08"}}, "206c25703c254632ae86485b6c17ec09": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ed71cb0458d440eb9b2ff1624be16161", "max": 2012, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_489102fd62554dabab24ac99ac27833c", "value": 2012}}, "20786f2e70544600ac9dae0e82a88add": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_53f12af2b09d42faa8e55e9443c4b6e1", "placeholder": "\u200b", "style": "IPY_MODEL_13857f61aba04437a3d878c2c9bf3117", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007246kB/s]"}}, "208122457aee4c6a946833aed29d1b76": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "20fc4316ce8642d3833749a9f3f3bb85": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "213e844512fb47e0a9a7d8f3b4727088": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2186534e6b864dcf8bb4da68a7e4bbda": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_58fde69b481743a38c5372b7d298b09c", "placeholder": "\u200b", "style": "IPY_MODEL_559c3c2c4cdd4e91acb7e26cf15e9308", "value": "Downloading\u2007result:\u2007100%"}}, "21c9de941d674281866d0262fe2ed459": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "21d74ed4128b43da83c3dc5bb35eb691": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "237e08e78b704b989ad7760aa2bad403": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "245b70f2a7a44cd39681090865205aec": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_6b27c9ee09e44745ac00897696c14a68", "IPY_MODEL_9149c98614634e6d886417c90d709b8d", "IPY_MODEL_55b241535c974cf0a278a48e5a1118a5"], "layout": "IPY_MODEL_04b90039cee9461eb9c5ddc2d32d5411"}}, "245c782407804c9b9a5ef7945b66a263": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_8f3eeb43b4a341af92ad0b4d8bd357be", "placeholder": "\u200b", "style": "IPY_MODEL_67eb530655654e88a4af7e23a46233a8", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007250kB/s]"}}, "25b7a8303c68474496bb2bd34094a85d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_fe2d61de6ffd47a6a1518a601d282434", "placeholder": "\u200b", "style": "IPY_MODEL_67e74ce55c394b1e8748f590c25861c3", "value": "merges.txt:\u2007100%"}}, "268f37723a444ac0bff99f6c7a9e0156": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_17405b17c4f94b2691d7ff94b333360a", "IPY_MODEL_3a7e3d366a0448a6a9973f2346d25761", "IPY_MODEL_b3287f56eff94a7183f5fb6ae3cb40bc"], "layout": "IPY_MODEL_7bb52d46fde3457786012ca7c50972af"}}, "279b65c529534d84928a7089ffc4aeb1": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "28319c42457b422ba134d507b592d92f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "283999bb93404ffda238072bc4d25f71": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_453ee35a7c93405a831616e99c4b3e80", "placeholder": "\u200b", "style": "IPY_MODEL_9c81deff989946198d0215cc84000674", "value": "\u20071.04M/1.04M\u2007[00:00&lt;00:00,\u200720.9MB/s]"}}, "28fa96a54a784ce19f5b0f81d76cc259": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2a7a70ad17f543a2901e7dcdbf0af2cb": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2af6d0793ad6479c9aea76a9f399712d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "2afe3d102e26418a96fb04734bfb7f2f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2b9b778defd54ebbb35c362fa1914cc2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_25b7a8303c68474496bb2bd34094a85d", "IPY_MODEL_dcebabd99ef346d5a343c96564cd4885", "IPY_MODEL_a0ba38352ad945eaa8f87693973cd80a"], "layout": "IPY_MODEL_adf535eaf9fb44149df919e0bf601a8d"}}, "2bd78a9e8f5d4d43b877978bf9d4e76b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2bf1010ba984483da8da572c72f128f5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "2bfaf3c0727c4f2bab33e97a0203be29": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_2186534e6b864dcf8bb4da68a7e4bbda", "IPY_MODEL_4d55f049db3842d6a8e423f43d471c52", "IPY_MODEL_2c0d72c52df042b8ab1c96d9f74efcea"], "layout": "IPY_MODEL_8cd1605fc975453f8ac8ef367ebeb49a"}}, "2c04da80df824cdd81a43281ddc8cdee": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_747837001e39405b80e6024ad3ba0491", "max": 26, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_2af6d0793ad6479c9aea76a9f399712d", "value": 26}}, "2c0d72c52df042b8ab1c96d9f74efcea": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_08870b81cd7e46c3a237e1726e0d1da1", "placeholder": "\u200b", "style": "IPY_MODEL_883f9e71b841481690c9f4d9b60f8d46", "value": "\u20074.44k/4.44k\u2007[00:00&lt;00:00,\u2007423kB/s]"}}, "2c901df04e854ad2a638367c000d5ee9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2d3d78c87e7342bbb6829f751836aa8d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_8f42afab215b445ca8337163b42fc650", "IPY_MODEL_117b3d0473e84bd4b82cc7e09becb99b", "IPY_MODEL_74431cc807324be9b5f732bda67683db"], "layout": "IPY_MODEL_72d0835041f64885af9e9a2a7b9473aa"}}, "2dd34f3473ab4ab39ed48bec225c966f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2df581bac6e84ab5a7b08ed8623f82b6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2e0d1564913849e2a322ff42d67fcb6e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2eaf47f894d548e3a2fccb4e98452ca4": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2f60d2ffc3a149c484aa370b08a2a547": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "311cc9b5228049caa17813aa969b5ad0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_2dd34f3473ab4ab39ed48bec225c966f", "placeholder": "\u200b", "style": "IPY_MODEL_adc0b7463c294abf827cfe6ec1c5d1ec", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007247kB/s]"}}, "32863c04c1fc41c0ac8f193d8833c740": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_613961ffcdc84b66ae3640b223828704", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_b1c0076c3e1040faa4fa507e8c715df5", "value": 18259}}, "32e49c3fdf5e480f89e67b73d5a13d96": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "346e3b5811514e38b1387bddb892ae61": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "34f9f5aa00f74b16abe6d995827a604a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3717a6ba8619483ea81c00dce940b932": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_5a3569aef0e841148340b8ceabc7fe20", "placeholder": "\u200b", "style": "IPY_MODEL_2a7a70ad17f543a2901e7dcdbf0af2cb", "value": "\u200726.0/26.0\u2007[00:00&lt;00:00,\u20072.94kB/s]"}}, "38fb4dd1106047afb5669a99732411ce": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_f9dba1546407418f9e5bd3d844db72ee", "IPY_MODEL_13d9810aa73040e098f1bbcab1f3e330", "IPY_MODEL_cff6718ebc9f4a6a9ac23b51fc32587e"], "layout": "IPY_MODEL_801cad502c84497f8ed978d772b9adec"}}, "39bd033ce5074fd1b83a35c55f21a830": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3cc8fe10967e4b7c8cb6b4628ef22d20", "max": 689, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_cfb135c9a68749d58a3878d44a6225ee", "value": 689}}, "3a37abc7ee3149c18cec6a1769c8e476": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3a7e3d366a0448a6a9973f2346d25761": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_927b7b26aa804cf093411b7f7574835c", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_a76661efd9c84186961548692e1d16b8", "value": 18259}}, "3b34de15975e464dac3e471433485749": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "3bfef6fb3a764df6a09d13dc1ddf4aef": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "3c25fea9e9d94dddb39ddad49bd0cbd6": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3c6225ab5a7749ddbaa257569da9eb6f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_774da1fcbf2148de9c095143f0ae1cfa", "placeholder": "\u200b", "style": "IPY_MODEL_d3d20b37d1f642e3bd2698cfc8322ed9", "value": "tokenizer_config.json:\u2007100%"}}, "3cc8fe10967e4b7c8cb6b4628ef22d20": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3dc0de35c9214e1187a8eca247b2e786": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_7bff49da6bf3408094986c3623dbbfe5", "max": 1752, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_c2db0f5df821473985baee52a9fbf2ee", "value": 1752}}, "3dd53db3bbb34bd8ad22602064121e64": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3e81876b32e9452da7298998bb410f6d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_80fbe1a669414dc8b76c132df21fbb71", "IPY_MODEL_9cc90538e6be4967815c538468ecfd54", "IPY_MODEL_283999bb93404ffda238072bc4d25f71"], "layout": "IPY_MODEL_c8fa2fcc1ec64b46bcf51838e09ecdb7"}}, "3f62e35483df464fad8a3c7935d8714c": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "40195277540f4710871718bb5d20e0fc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_612cf2291359477099db0b0c8b93de2e", "max": 1373465, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_8010599eb48647c1a263b17851dd820e", "value": 1373465}}, "402c8bf95f8b4498bf3f3d2698f8c32a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "40683eb5893642e78aefdd0d8800343d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_42c34acd48d548c3b71c6c507c47022b", "IPY_MODEL_73420d74a7ac444fa47af15dc163f9f0", "IPY_MODEL_245c782407804c9b9a5ef7945b66a263"], "layout": "IPY_MODEL_f9a6a461655d4129866af2bcfbfaedf2"}}, "41faef04bff948e49711644e160a31ff": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "42c34acd48d548c3b71c6c507c47022b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_69f9e26fb1264cb5902ba32c6b3909b4", "placeholder": "\u200b", "style": "IPY_MODEL_6e046ac727a745278d58af5fd1e3edd1", "value": "Downloading\u2007result:\u2007100%"}}, "43389bfa30f940bbb28f1ce2829cc3e4": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "444d9d41658a454a9130c2ad09cb1441": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_dd663f5b45184de99fe98a129ca12807", "max": 456318, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_28319c42457b422ba134d507b592d92f", "value": 456318}}, "453ee35a7c93405a831616e99c4b3e80": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "46197f441fa542f6a56d7f3e1af886c1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_1d82fbfcaa0c44988a7bc1b380a12526", "max": 9752, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_9497520e23044451ad4e5f46579755b4", "value": 9752}}, "46d978cd25594af890f658e3037cec3a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "47803084530249ed9178e0d15f20f7b9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_aef069572cad47f2b82695d583e0e359", "placeholder": "\u200b", "style": "IPY_MODEL_08338a97378d4949ac94234c6cbc93ee", "value": "Downloading\u2007result:\u2007100%"}}, "4825392cc048483e93af12ce2c7e087e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_61d22c5669f6484e850307e60a1bef69", "max": 9624, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_8700beadad804a8dbac9f75c1c579510", "value": 9624}}, "48372b23e5204b52b32eb6cf2af32d3c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "489102fd62554dabab24ac99ac27833c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "4909aeb3221146089521efb3b906dfda": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "4ab8897a5e1243188da8e9554f24095d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4ae1c9d1bc284b2ab17c0c8488bbdb04": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4bd93b578dce4bb0b281aae65ab59f51": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_28fa96a54a784ce19f5b0f81d76cc259", "placeholder": "\u200b", "style": "IPY_MODEL_8326f50241e34786a3833cd4b0750f22", "value": "\u20079.75k/9.75k\u2007[00:00&lt;00:00,\u20071.10MB/s]"}}, "4d55f049db3842d6a8e423f43d471c52": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ab7b5e76098f4474baf6c99e0668181e", "max": 4444, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_80df04d3233b4ff3932e8c9f9bb73684", "value": 4444}}, "4d918614f71f49d68197fa18c0c850be": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4e90dadcf8a4458b9fe9ae6cdf604638": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4f05770198bb4d8fa52753bf545e2e32": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_8790a341b97a4f4d8b9427b4b7c26c37", "IPY_MODEL_a02f08d3a14740bf876c3806d25ed18d", "IPY_MODEL_6206373de48a448ab26ca10be0e3d42b"], "layout": "IPY_MODEL_1302f7c7c44f42fca6f5a5ece11710f6"}}, "4f577da69b3148d4af43e0e77201ace1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4fb33d8bbc78451a8157525716e65f77": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bfc9973b3aaa4e238a2a710b35ed2a88", "placeholder": "\u200b", "style": "IPY_MODEL_9e24cacef37b43ae9a926cd7a3e86df3", "value": "\u2007619/619\u2007[00:00&lt;00:00,\u200766.7kB/s]"}}, "51a6b7d99a5b4d0d93017e3416ae8a5b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "538314ffad9446bca2f9ef8f5c09394a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "53b840aec53349f3bc236405f1acffa3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "53f12af2b09d42faa8e55e9443c4b6e1": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "543e22370faa4147a0b55aaa06b2acf2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_eab61ef8d1944c55be63c8b73d6b96f9", "max": 1496, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_ed903de72c43489e86f0b765a8dbdf4c", "value": 1496}}, "559c3c2c4cdd4e91acb7e26cf15e9308": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "55b241535c974cf0a278a48e5a1118a5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a5faad82ca16421396017f477d9b46ea", "placeholder": "\u200b", "style": "IPY_MODEL_b70dfe85088843d0b1e2d60c51e3b712", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007247kB/s]"}}, "56637b587ccf433993e55802fcf92475": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "569ee108555548929f15a7d59d96318b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_c6c75370c7bc4d3e92d20b06eb71e65f", "IPY_MODEL_39bd033ce5074fd1b83a35c55f21a830", "IPY_MODEL_1a9a1ca430a147ed807327f4ad2986cf"], "layout": "IPY_MODEL_a9851ed3d44e4e6a84c54ad629996fce"}}, "56ff235baaba49e686e5a5745854b00e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_77b8a2af2fc047de8a86a074e588ba34", "IPY_MODEL_543e22370faa4147a0b55aaa06b2acf2", "IPY_MODEL_ac4fc3445e6b475baf9b3df0336620b4"], "layout": "IPY_MODEL_2eaf47f894d548e3a2fccb4e98452ca4"}}, "58fde69b481743a38c5372b7d298b09c": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5a183ad8845c417abb3c4896c5033534": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_56637b587ccf433993e55802fcf92475", "placeholder": "\u200b", "style": "IPY_MODEL_d6a6aa268a144406b17d7ec02c7132e0", "value": "Downloading\u2007result:\u2007100%"}}, "5a3569aef0e841148340b8ceabc7fe20": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5c4a4f9ceb9145a88f3adce6961b37e8": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5da401afdd5f4aedbec8e4c15fb9b848": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "5e5359c007a443c7ae1e722e75e2312c": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5ecff152955f4913bd26091958af6223": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "60461356f4f84690a4febde432ec2faa": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "612cf2291359477099db0b0c8b93de2e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "613961ffcdc84b66ae3640b223828704": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "614f9bb7bff14e46b482d065425aaad3": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "61d22c5669f6484e850307e60a1bef69": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6206373de48a448ab26ca10be0e3d42b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_e663ba7b7de844d6a11005da7919d7e2", "placeholder": "\u200b", "style": "IPY_MODEL_6884fd64a0b84e39828ff8221d504f62", "value": "\u200768.1k/68.1k\u2007[00:00&lt;00:00,\u2007457kB/s]"}}, "624cbc6538894abcb85e0e187792cc47": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "63347258fbaf4d62b1c571e1b8519f70": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_21c9de941d674281866d0262fe2ed459", "placeholder": "\u200b", "style": "IPY_MODEL_e2ae03387a714b44aebb28c1c8383ef6", "value": "\u20079.75k/9.75k\u2007[00:00&lt;00:00,\u2007916kB/s]"}}, "635955f48bef4f2ca25ccab3957ceb5c": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6506409057ce405e9b20e9136ce9b9ae": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "655d6ea74dca43f78b7450867cb1933a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "65f46ac3814b45929537c5f7cbd3e29e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6684ba46159e4bbabab94a367530050c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d7f2fcebe7104bf7b63244dda2971824", "placeholder": "\u200b", "style": "IPY_MODEL_1e83765602f34e2383fc12dd5121675e", "value": "\u20079.62k/9.62k\u2007[00:00&lt;00:00,\u20071.02MB/s]"}}, "66e9ec8e45cb4d92a4189aadfcea0f52": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_8c026384926f4e31aec6913cefd0676a", "max": 1496, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_5da401afdd5f4aedbec8e4c15fb9b848", "value": 1496}}, "67e74ce55c394b1e8748f590c25861c3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "67eb530655654e88a4af7e23a46233a8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "6884fd64a0b84e39828ff8221d504f62": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "68bb204e45834a8b8ab7177a378f101e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "69e3a1a053e24b45ba21cdbad15c213b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "69f9e26fb1264cb5902ba32c6b3909b4": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6b27c9ee09e44745ac00897696c14a68": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_34f9f5aa00f74b16abe6d995827a604a", "placeholder": "\u200b", "style": "IPY_MODEL_46d978cd25594af890f658e3037cec3a", "value": "Downloading\u2007result:\u2007100%"}}, "6b4e69dbaebf444a8d957a380630d440": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6b7ae716dae74849a67417d49df7d038": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6df7e2bfc50c4e008582504f88a5f159": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_60461356f4f84690a4febde432ec2faa", "placeholder": "\u200b", "style": "IPY_MODEL_9bd1dd9b76014c3dac28a0e2867e00fb", "value": "\u2007798k/798k\u2007[00:00&lt;00:00,\u200719.9MB/s]"}}, "6e046ac727a745278d58af5fd1e3edd1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "6f231bf805014279b74b5c15cc403b0f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_bc9b5c75677f466bbc15544fc571355e", "IPY_MODEL_40195277540f4710871718bb5d20e0fc", "IPY_MODEL_16bd8f4ace5c4d1c99d280197108dbf1"], "layout": "IPY_MODEL_eb65306aaabd42e7b89a54a782838545"}}, "70a4ea598f3b49b48c9b8bdf016e31b4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "72d0835041f64885af9e9a2a7b9473aa": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "73420d74a7ac444fa47af15dc163f9f0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4d918614f71f49d68197fa18c0c850be", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_c9d820b27fe3409bb166540597dab593", "value": 18259}}, "74431cc807324be9b5f732bda67683db": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_19b79b666ee2478783485c0698be6965", "placeholder": "\u200b", "style": "IPY_MODEL_76456785fef44f628a6d9a1ad3ddc237", "value": "\u200726.2k/26.2k\u2007[00:00&lt;00:00,\u2007353kB/s]"}}, "747837001e39405b80e6024ad3ba0491": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "76085d9b955743bbaa3e37dec7ba3a8a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "76456785fef44f628a6d9a1ad3ddc237": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "768b3ce38ab943b9acaf536f8943b64c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_279b65c529534d84928a7089ffc4aeb1", "placeholder": "\u200b", "style": "IPY_MODEL_76085d9b955743bbaa3e37dec7ba3a8a", "value": "tokenizer_config.json:\u2007100%"}}, "76fe7cf731f244329d3fc3dfba2e2375": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "774da1fcbf2148de9c095143f0ae1cfa": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "776748ddec534ec8939395045fa6b9d6": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "77b8a2af2fc047de8a86a074e588ba34": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_624cbc6538894abcb85e0e187792cc47", "placeholder": "\u200b", "style": "IPY_MODEL_834b39b39eef4c4fad88f26c8dd096ca", "value": "Downloading\u2007result:\u2007100%"}}, "77c61d730beb4558871dd3403ca64335": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7bb52d46fde3457786012ca7c50972af": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7bff49da6bf3408094986c3623dbbfe5": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7ca44c2184884b9093b54c5be681f4aa": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "7d69404e59be4118b053d8b2111f2d25": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "7dfecc0fc17b447aaa3b3c631dd5254b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7e1aec0eafb94bdc99d59f59231a8878": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7e4f7bcd80d14036b8152cbb736762fd": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7e9e30528369417db66c0f0399b6f41c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8010599eb48647c1a263b17851dd820e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "801cad502c84497f8ed978d772b9adec": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "80d9894d38bd4fc49cbc3cb4005be6f8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f260547acae14e94a72e82e56ecf6b5f", "max": 9436, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_b1dc42513866483f8965f722d192e362", "value": 9436}}, "80df04d3233b4ff3932e8c9f9bb73684": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "80fbe1a669414dc8b76c132df21fbb71": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_cb7e1045da364c278598f6b5c645be9f", "placeholder": "\u200b", "style": "IPY_MODEL_903751ed84334e1fb2928a763576c790", "value": "vocab.json:\u2007100%"}}, "822e25728ed44ef9b558123151719e71": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "8326f50241e34786a3833cd4b0750f22": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "834b39b39eef4c4fad88f26c8dd096ca": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "83a2430b92234870bec0987a4b73d0e6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_0e63d709918644ccb0f33996d8565b36", "IPY_MODEL_d228cb2d5fd241ad85a95d1069046e5a", "IPY_MODEL_9a4aa1f9cf35474cb5ab7165abc79d5e"], "layout": "IPY_MODEL_43389bfa30f940bbb28f1ce2829cc3e4"}}, "83a80a31ca274593897504881f494607": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "83ad1a6c5f6140adb32adef318df519d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_69e3a1a053e24b45ba21cdbad15c213b", "placeholder": "\u200b", "style": "IPY_MODEL_9ecaa851059c490095f3ce72910144d7", "value": "\u20071.50k/1.50k\u2007[00:00&lt;00:00,\u200796.1kB/s]"}}, "84a3ebc478ea435db31d0229cdfff4fa": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_fd29329bcb6f401e8c7c1fbd22407bbb", "IPY_MODEL_46197f441fa542f6a56d7f3e1af886c1", "IPY_MODEL_4bd93b578dce4bb0b281aae65ab59f51"], "layout": "IPY_MODEL_0bd6a26f8e61496f900d4d424f6aca1b"}}, "84cc6e66d574428489846b86de776fc3": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "853f0bb3931845fa806babd30e0de965": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_13ae83183063428ebf3c1c6715efcc46", "placeholder": "\u200b", "style": "IPY_MODEL_01988d3b72d64d55991ce656d297acb2", "value": "Downloading\u2007result:\u2007100%"}}, "86065d727a9c4c3a829a37e382b983d2": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "86ff5405b4854f85a972bb04ddac7388": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8700beadad804a8dbac9f75c1c579510": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "876c7f68979447b5958e668cec2e7ebf": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8790a341b97a4f4d8b9427b4b7c26c37": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_5c4a4f9ceb9145a88f3adce6961b37e8", "placeholder": "\u200b", "style": "IPY_MODEL_a3a84d5a44e649158488489adbcc285b", "value": "Downloading\u2007result:\u2007100%"}}, "87a5fd8c874c4e199407693dd43660bd": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "87aef83ba64e4fe89d5f0119940b3ade": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_32e49c3fdf5e480f89e67b73d5a13d96", "placeholder": "\u200b", "style": "IPY_MODEL_208122457aee4c6a946833aed29d1b76", "value": "merges.txt:\u2007100%"}}, "883f9e71b841481690c9f4d9b60f8d46": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "88e9db1748154aba8cfa635124a73a6a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_b748cb1ff9da4707aa81f0576e544ac6", "IPY_MODEL_9bda2e83dcfd48bfa1706080ee22ce7c", "IPY_MODEL_b22dfb7986fa488bb84339e51677752e"], "layout": "IPY_MODEL_896428cfe3974fc78bb03fc0f1c3ea00"}}, "896428cfe3974fc78bb03fc0f1c3ea00": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8a398f927eef4169bdd383d98cba989e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8b111a9d7f7c46d2938313a76d1b0003": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_2e0d1564913849e2a322ff42d67fcb6e", "placeholder": "\u200b", "style": "IPY_MODEL_f779a574631c466dbafca251ce975865", "value": "\u2007357/357\u2007[00:00&lt;00:00,\u200731.9kB/s]"}}, "8c026384926f4e31aec6913cefd0676a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8cd1605fc975453f8ac8ef367ebeb49a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8e8d306e87f04986ba8cd555345bef1b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "8f3eeb43b4a341af92ad0b4d8bd357be": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8f42afab215b445ca8337163b42fc650": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d97f030c9e744baba7c70f86e4f906d3", "placeholder": "\u200b", "style": "IPY_MODEL_11ef947397da4c81adc993010180f8b9", "value": "Downloading\u2007result:\u2007100%"}}, "903751ed84334e1fb2928a763576c790": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "903b1d60d089418d820d7166a375b1aa": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f3ca7baa5a2e4a9ab8372f23788a3f71", "max": 9752, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_822e25728ed44ef9b558123151719e71", "value": 9752}}, "9149c98614634e6d886417c90d709b8d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_7dfecc0fc17b447aaa3b3c631dd5254b", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_d020e1bd528249909aebb43b6b3ba688", "value": 18259}}, "916cfd3f39ff451cb0f7717a8ee99060": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_237e08e78b704b989ad7760aa2bad403", "placeholder": "\u200b", "style": "IPY_MODEL_f5efa8c947c242b5b4ae666e8e0ec8cb", "value": "\u2007456k/456k\u2007[00:00&lt;00:00,\u20073.43MB/s]"}}, "927b7b26aa804cf093411b7f7574835c": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9497520e23044451ad4e5f46579755b4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "9540c83f5a054183b6a471e0579c38f9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "959d7947fdf9499d895e694558202ea0": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9618e75a28e54751b5649375bdf64201": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "96e296f54648457c8d7d6ccd95330f68": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "972c2149a9a44fa1a4640cd67e305095": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_876c7f68979447b5958e668cec2e7ebf", "placeholder": "\u200b", "style": "IPY_MODEL_655d6ea74dca43f78b7450867cb1933a", "value": "Downloading\u2007result:\u2007100%"}}, "97c8169c4240413196f88ac2c6f2a483": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9914b23d72144ce699c3f0a1ba8bebf2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_c3b710321df04db195dfdbfbbdd2772b", "placeholder": "\u200b", "style": "IPY_MODEL_bde614dfa31c4c01ad97cc11e4ebc5f1", "value": "\u20071.50k/1.50k\u2007[00:00&lt;00:00,\u2007163kB/s]"}}, "9985e7a5a3f8406c872c1c222c494c80": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9a4aa1f9cf35474cb5ab7165abc79d5e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_5e5359c007a443c7ae1e722e75e2312c", "placeholder": "\u200b", "style": "IPY_MODEL_4e90dadcf8a4458b9fe9ae6cdf604638", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007246kB/s]"}}, "9ae0c393a0dd4778be0255c0caf66916": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9b2453365b6b4d5db1f69c9b269d3bec": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9b311d130b044025bfe4ba34b08c4b58": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_c7dc8a45d6ad4251af1719a5f6a53126", "IPY_MODEL_f7acabea1818403e9f999951f8d01340", "IPY_MODEL_fc490c4bcf664192bb123e6bbf920d72"], "layout": "IPY_MODEL_158b31288b0340f2a00cf9bcc358a1a9"}}, "9b9a1ae554c9490784b17ac40e969706": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9bd1dd9b76014c3dac28a0e2867e00fb": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9bda2e83dcfd48bfa1706080ee22ce7c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a751297c9bdd462891338c1c276145c1", "max": 1752, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_8e8d306e87f04986ba8cd555345bef1b", "value": 1752}}, "9c81deff989946198d0215cc84000674": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9cc90538e6be4967815c538468ecfd54": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_87a5fd8c874c4e199407693dd43660bd", "max": 1042301, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_4909aeb3221146089521efb3b906dfda", "value": 1042301}}, "9d2acab0a4194c5bafc413a3f82f708b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_d6957e5eb18d4e42abfd62967870a484", "IPY_MODEL_903b1d60d089418d820d7166a375b1aa", "IPY_MODEL_63347258fbaf4d62b1c571e1b8519f70"], "layout": "IPY_MODEL_c518f68c0ee845a4aee0408ee53912fb"}}, "9daa172b19ab43c8a558355d0d6899f9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_cc7d6029f15848f7a47dad53babdba4d", "max": 1355256, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_012fde2318e7418cbb9381a13f8c5205", "value": 1355256}}, "9e24cacef37b43ae9a926cd7a3e86df3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9ecaa851059c490095f3ce72910144d7": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "a0157097ea3a413ea2014c89edf54e6e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a02f08d3a14740bf876c3806d25ed18d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3f62e35483df464fad8a3c7935d8714c", "max": 68096, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_7ca44c2184884b9093b54c5be681f4aa", "value": 68096}}, "a0ba38352ad945eaa8f87693973cd80a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_538314ffad9446bca2f9ef8f5c09394a", "placeholder": "\u200b", "style": "IPY_MODEL_2afe3d102e26418a96fb04734bfb7f2f", "value": "\u2007456k/456k\u2007[00:00&lt;00:00,\u20076.98MB/s]"}}, "a118db0114ed436a87a8e300674dfa3e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a20cd49978fe499c9f46c6ff7c3df5ca": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_188451ba2c244823b4503a243147ceea", "IPY_MODEL_206c25703c254632ae86485b6c17ec09", "IPY_MODEL_1c41be39fec741538403747379c267dc"], "layout": "IPY_MODEL_de92bc82472f4da39dfda58deb2718ce"}}, "a2dff4115edb471b88fb29d74e0817f7": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_68bb204e45834a8b8ab7177a378f101e", "placeholder": "\u200b", "style": "IPY_MODEL_5ecff152955f4913bd26091958af6223", "value": "\u20071.36M/1.36M\u2007[00:00&lt;00:00,\u200711.2MB/s]"}}, "a3a84d5a44e649158488489adbcc285b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "a4540e39b8a1427ba23226866abba159": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a537d4395df243d99be81d872da4fa93": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a538442c500b4f53a4ac0061c8887ba7": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "a5faad82ca16421396017f477d9b46ea": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a751297c9bdd462891338c1c276145c1": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a76661efd9c84186961548692e1d16b8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "a89b1da9a7aa4d7fa10b34144d6ed8d6": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a8b0a094edbf49aca8674fcf0df964f2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "a9851ed3d44e4e6a84c54ad629996fce": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a991634ff81c49489970839a52659bd6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "a9bb2022117744f28a58d3649707ac3e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ab7b5e76098f4474baf6c99e0668181e": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ab84b3eb44114fe8ba599f1527245129": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ac4fc3445e6b475baf9b3df0336620b4": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_402c8bf95f8b4498bf3f3d2698f8c32a", "placeholder": "\u200b", "style": "IPY_MODEL_a538442c500b4f53a4ac0061c8887ba7", "value": "\u20071.50k/1.50k\u2007[00:00&lt;00:00,\u200795.3kB/s]"}}, "ad5864561cae4b1fb375d4dbc2a05f02": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "adc0b7463c294abf827cfe6ec1c5d1ec": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "adf535eaf9fb44149df919e0bf601a8d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "aef069572cad47f2b82695d583e0e359": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "af9350c3e2b2440fa5f0bc80d0ad3ac3": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_151ca252abd149fab0a9d618e69094de", "IPY_MODEL_b1ab56f7f43f4f359b3b6591ad1a0fcc", "IPY_MODEL_311cc9b5228049caa17813aa969b5ad0"], "layout": "IPY_MODEL_6b7ae716dae74849a67417d49df7d038"}}, "b1ab56f7f43f4f359b3b6591ad1a0fcc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b6a1d736e1e14e81b0c015cae891a128", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_51a6b7d99a5b4d0d93017e3416ae8a5b", "value": 18259}}, "b1c0076c3e1040faa4fa507e8c715df5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "b1dc42513866483f8965f722d192e362": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "b22dfb7986fa488bb84339e51677752e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_c7f7bc215e4e423981e185e493bc646f", "placeholder": "\u200b", "style": "IPY_MODEL_c791c64095954874b566b52c6717f049", "value": "\u20071.75k/1.75k\u2007[00:00&lt;00:00,\u2007120kB/s]"}}, "b3287f56eff94a7183f5fb6ae3cb40bc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_6b4e69dbaebf444a8d957a380630d440", "placeholder": "\u200b", "style": "IPY_MODEL_bd3c3f6b0fc945cc8428ac1ee19ca55a", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007251kB/s]"}}, "b44a837c1d0b43649696ffb11ca72ed7": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "b46dbf3836a04e7d91bc134c5256a16a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_83a80a31ca274593897504881f494607", "max": 1496, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_3b34de15975e464dac3e471433485749", "value": 1496}}, "b4c5703a12914f038d9b1d4a2bd5a5aa": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b4d6d48fc8434d14a6cc4e39bf782cf5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "b6a1d736e1e14e81b0c015cae891a128": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b70dfe85088843d0b1e2d60c51e3b712": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "b740faed256541f78bfd16fcb33fc401": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_87aef83ba64e4fe89d5f0119940b3ade", "IPY_MODEL_444d9d41658a454a9130c2ad09cb1441", "IPY_MODEL_916cfd3f39ff451cb0f7717a8ee99060"], "layout": "IPY_MODEL_4ab8897a5e1243188da8e9554f24095d"}}, "b748cb1ff9da4707aa81f0576e544ac6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bd3bdfd08e8f4e8fb7503ab2823b3ccd", "placeholder": "\u200b", "style": "IPY_MODEL_2bd78a9e8f5d4d43b877978bf9d4e76b", "value": "Downloading\u2007result:\u2007100%"}}, "b8737152dc214ce8a08d1ee99d5bda65": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_dda6ec34216741d7a28f44a27006a182", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_7d69404e59be4118b053d8b2111f2d25", "value": 18259}}, "b9854d3059c743e8a30fe9d82a5641e1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_be2e36a2495b48798b9bce97b4c4cdde", "max": 619, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_f5ce09de0527430a994c60c950eae968", "value": 619}}, "bc9b5c75677f466bbc15544fc571355e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d8dbc4de787a48e2afb1b399b38b1a5f", "placeholder": "\u200b", "style": "IPY_MODEL_2df581bac6e84ab5a7b08ed8623f82b6", "value": "tokenizer.json:\u2007100%"}}, "bcea351eb1074301969d0c668e4dc7dd": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bd3bdfd08e8f4e8fb7503ab2823b3ccd": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bd3c3f6b0fc945cc8428ac1ee19ca55a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "bde614dfa31c4c01ad97cc11e4ebc5f1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "be2e36a2495b48798b9bce97b4c4cdde": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bee97b55154e4b2d9736b68bce3adc5a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_eeafcddcac564b9492d7626a2af1661b", "max": 1091352, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_b44a837c1d0b43649696ffb11ca72ed7", "value": 1091352}}, "bef985913faa4f5f872f025c578bdaa9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9b2453365b6b4d5db1f69c9b269d3bec", "max": 357, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_48372b23e5204b52b32eb6cf2af32d3c", "value": 357}}, "bfc9973b3aaa4e238a2a710b35ed2a88": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c02b329fed244d2783a3b5375e93faf5": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c128dda449eb4ee8b71f99bb83954007": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "c2db0f5df821473985baee52a9fbf2ee": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "c2f0cb0b23f744f6adb8a4da25c13f08": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c3b710321df04db195dfdbfbbdd2772b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c4a9af79379a40a1aec1097ecc38761b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c518f68c0ee845a4aee0408ee53912fb": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c56000dd83a6443080b8d11e79153fbc": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c6c75370c7bc4d3e92d20b06eb71e65f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a118db0114ed436a87a8e300674dfa3e", "placeholder": "\u200b", "style": "IPY_MODEL_1e27ace0a807404984dd81559c947152", "value": "config.json:\u2007100%"}}, "c791c64095954874b566b52c6717f049": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "c7dc8a45d6ad4251af1719a5f6a53126": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_84cc6e66d574428489846b86de776fc3", "placeholder": "\u200b", "style": "IPY_MODEL_2c901df04e854ad2a638367c000d5ee9", "value": "config.json:\u2007100%"}}, "c7f7bc215e4e423981e185e493bc646f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c8fa2fcc1ec64b46bcf51838e09ecdb7": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c9911073ccd54d5197706a19e61e7faa": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a537d4395df243d99be81d872da4fa93", "placeholder": "\u200b", "style": "IPY_MODEL_70a4ea598f3b49b48c9b8bdf016e31b4", "value": "tokenizer.json:\u2007100%"}}, "c9d820b27fe3409bb166540597dab593": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "cb7e1045da364c278598f6b5c645be9f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cc7d6029f15848f7a47dad53babdba4d": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cd8114d79db941bd840d64a953e81719": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cd9a1d54c5824272bda78713256f5a31": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ce647b9b168c46749bd5400f608d1f29": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_768b3ce38ab943b9acaf536f8943b64c", "IPY_MODEL_b9854d3059c743e8a30fe9d82a5641e1", "IPY_MODEL_4fb33d8bbc78451a8157525716e65f77"], "layout": "IPY_MODEL_a9bb2022117744f28a58d3649707ac3e"}}, "cfb135c9a68749d58a3878d44a6225ee": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "cff6718ebc9f4a6a9ac23b51fc32587e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_eefe7914be9949bcb00bdd209370cd38", "placeholder": "\u200b", "style": "IPY_MODEL_7e1aec0eafb94bdc99d59f59231a8878", "value": "\u20074.04k/4.04k\u2007[00:00&lt;00:00,\u2007510kB/s]"}}, "d01f70585c6944a98ce63a8a9440a656": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d020e1bd528249909aebb43b6b3ba688": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "d040f3c84b3f472f9085af62cd2a338e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_1cfad90785b94cf9b6ce0c84dba0bfa4", "IPY_MODEL_bef985913faa4f5f872f025c578bdaa9", "IPY_MODEL_8b111a9d7f7c46d2938313a76d1b0003"], "layout": "IPY_MODEL_c02b329fed244d2783a3b5375e93faf5"}}, "d16677f2af664a4480bd756613b4341e": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f0abc95931e74ccb8733eda7034224c1", "placeholder": "\u200b", "style": "IPY_MODEL_e5d9a66ebdd94084ade501c72552a5fb", "value": "\u200718.3k/18.3k\u2007[00:00&lt;00:00,\u2007251kB/s]"}}, "d228cb2d5fd241ad85a95d1069046e5a": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ab84b3eb44114fe8ba599f1527245129", "max": 18259, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_a991634ff81c49489970839a52659bd6", "value": 18259}}, "d2293d8effab4075b567eba417819463": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d3d20b37d1f642e3bd2698cfc8322ed9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d4f34d5120004a7198dc6906f2840ab2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d63449e9a47a437aa2d5d6fc9c6fb608": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_edd9957398e542268c50de5a818c6772", "IPY_MODEL_80d9894d38bd4fc49cbc3cb4005be6f8", "IPY_MODEL_f9c7baac72c143f0a5ee9e19cca2d54b"], "layout": "IPY_MODEL_a89b1da9a7aa4d7fa10b34144d6ed8d6"}}, "d6957e5eb18d4e42abfd62967870a484": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d2293d8effab4075b567eba417819463", "placeholder": "\u200b", "style": "IPY_MODEL_53b840aec53349f3bc236405f1acffa3", "value": "Downloading\u2007result:\u2007100%"}}, "d6a6aa268a144406b17d7ec02c7132e0": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d7be60815f534f098018a1b1c9e384e2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_972c2149a9a44fa1a4640cd67e305095", "IPY_MODEL_bee97b55154e4b2d9736b68bce3adc5a", "IPY_MODEL_0784a30e715d4a259d6a933a6246dda0"], "layout": "IPY_MODEL_21d74ed4128b43da83c3dc5bb35eb691"}}, "d7f2fcebe7104bf7b63244dda2971824": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d8dbc4de787a48e2afb1b399b38b1a5f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d97f030c9e744baba7c70f86e4f906d3": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "da27563811ec4839a791b9d7e74a6ce8": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dcebabd99ef346d5a343c96564cd4885": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_346e3b5811514e38b1387bddb892ae61", "max": 456356, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_163e93addfce4d28aae738789857e4d9", "value": 456356}}, "dd663f5b45184de99fe98a129ca12807": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dda6ec34216741d7a28f44a27006a182": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "de92bc82472f4da39dfda58deb2718ce": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e2ae03387a714b44aebb28c1c8383ef6": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e5d9a66ebdd94084ade501c72552a5fb": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e663ba7b7de844d6a11005da7919d7e2": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e6aecd752b664e6e9462471e56859bbe": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "eab61ef8d1944c55be63c8b73d6b96f9": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "eb56d281905e4adfb2b5aa78adfe6e9b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_959d7947fdf9499d895e694558202ea0", "placeholder": "\u200b", "style": "IPY_MODEL_17c2456a850b4eb29dc69c41c225d986", "value": "\u20071.75k/1.75k\u2007[00:00&lt;00:00,\u2007124kB/s]"}}, "eb65306aaabd42e7b89a54a782838545": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "eb95df25ca934ce6b1dcce4a18365226": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "ed71cb0458d440eb9b2ff1624be16161": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ed7231cf77cb4816892d4146e740115f": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_17f962688f4946b7a4c0143d2f03c31c", "IPY_MODEL_4825392cc048483e93af12ce2c7e087e", "IPY_MODEL_6684ba46159e4bbabab94a367530050c"], "layout": "IPY_MODEL_158e3f31f9c040d48d933cd33c08dcba"}}, "ed903de72c43489e86f0b765a8dbdf4c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "edcf716781f541359a655d7de784d58a": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "edd9957398e542268c50de5a818c6772": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_edcf716781f541359a655d7de784d58a", "placeholder": "\u200b", "style": "IPY_MODEL_41faef04bff948e49711644e160a31ff", "value": "Downloading\u2007result:\u2007100%"}}, "eeafcddcac564b9492d7626a2af1661b": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "eefe7914be9949bcb00bdd209370cd38": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f0abc95931e74ccb8733eda7034224c1": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f260547acae14e94a72e82e56ecf6b5f": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f301ae5316f84d739a66681ed5048a91": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_47803084530249ed9178e0d15f20f7b9", "IPY_MODEL_b46dbf3836a04e7d91bc134c5256a16a", "IPY_MODEL_83ad1a6c5f6140adb32adef318df519d"], "layout": "IPY_MODEL_a4540e39b8a1427ba23226866abba159"}}, "f3ca7baa5a2e4a9ab8372f23788a3f71": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f4c8f8856f424acdad779623290d41de": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_c9911073ccd54d5197706a19e61e7faa", "IPY_MODEL_9daa172b19ab43c8a558355d0d6899f9", "IPY_MODEL_a2dff4115edb471b88fb29d74e0817f7"], "layout": "IPY_MODEL_b4c5703a12914f038d9b1d4a2bd5a5aa"}}, "f5138b724a834b5e8adae40d352048e9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9ae0c393a0dd4778be0255c0caf66916", "max": 798156, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_e6aecd752b664e6e9462471e56859bbe", "value": 798156}}, "f5ce09de0527430a994c60c950eae968": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "f5efa8c947c242b5b4ae666e8e0ec8cb": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f779a574631c466dbafca251ce975865": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f7acabea1818403e9f999951f8d01340": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3a37abc7ee3149c18cec6a1769c8e476", "max": 930, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_9618e75a28e54751b5649375bdf64201", "value": 930}}, "f9a6a461655d4129866af2bcfbfaedf2": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f9c7baac72c143f0a5ee9e19cca2d54b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_65f46ac3814b45929537c5f7cbd3e29e", "placeholder": "\u200b", "style": "IPY_MODEL_4ae1c9d1bc284b2ab17c0c8488bbdb04", "value": "\u20079.44k/9.44k\u2007[00:00&lt;00:00,\u2007859kB/s]"}}, "f9dba1546407418f9e5bd3d844db72ee": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_86065d727a9c4c3a829a37e382b983d2", "placeholder": "\u200b", "style": "IPY_MODEL_3bfef6fb3a764df6a09d13dc1ddf4aef", "value": "added_tokens.json:\u2007100%"}}, "fbc4aed6ec144624926cfde1973c21b1": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d01f70585c6944a98ce63a8a9440a656", "placeholder": "\u200b", "style": "IPY_MODEL_d4f34d5120004a7198dc6906f2840ab2", "value": "vocab.json:\u2007100%"}}, "fc490c4bcf664192bb123e6bbf920d72": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_96e296f54648457c8d7d6ccd95330f68", "placeholder": "\u200b", "style": "IPY_MODEL_213e844512fb47e0a9a7d8f3b4727088", "value": "\u2007930/930\u2007[00:00&lt;00:00,\u200794.9kB/s]"}}, "fd154a13cef9450293aebc12f51c8003": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "fd29329bcb6f401e8c7c1fbd22407bbb": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_cd8114d79db941bd840d64a953e81719", "placeholder": "\u200b", "style": "IPY_MODEL_9b9a1ae554c9490784b17ac40e969706", "value": "Downloading\u2007result:\u2007100%"}}, "fd945b242a13475d832d1ce86ec0e26b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_18f5903d0b454dd686099f3ad571f6c8", "IPY_MODEL_b8737152dc214ce8a08d1ee99d5bda65", "IPY_MODEL_d16677f2af664a4480bd756613b4341e"], "layout": "IPY_MODEL_c4a9af79379a40a1aec1097ecc38761b"}}, "fe2d61de6ffd47a6a1518a601d282434": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}
</script></section>
</section>
</section>


                </article>
              
              
              
              
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Function Vectors</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Introduction">Introduction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Content-&amp;-Learning-Objectives">Content &amp; Learning Objectives</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#1️⃣-Introduction-to-nnsight">1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#2️⃣-Task-encoding-hidden-states">2️⃣ Task-encoding hidden states</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#3️⃣-Function-Vectors">3️⃣ Function Vectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#4️⃣-Steering-Vectors-in-GPT2-XL">4️⃣ Steering Vectors in GPT2-XL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#☆-Bonus">☆ Bonus</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Setup-code">Setup code</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1️⃣ Introduction to <code class="docutils literal notranslate"><span class="pre">nnsight</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Remote-execution">Remote execution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Important-syntax">Important syntax</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Model-config">Model config</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Tokenizers">Tokenizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Model-outputs">Model outputs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Output-vs-input">Output vs input</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Which-objects-to-save">Which objects to save</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Scan-&amp;-Validate">Scan &amp; Validate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Putting-this-into-practice">Putting this into practice</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---visualize-attention-heads">Exercise - visualize attention heads</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2️⃣ Task-encoding hidden states</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ICL-Task">ICL Task</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise-(optional)---generate-your-own-antonym-pairs">Exercise (optional) - generate your own antonym pairs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ICL-Dataset">ICL Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Task-encoding-vector">Task-encoding vector</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---forward-pass-on-antonym-dataset">Exercise - forward pass on antonym dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Using-multiple-invokes">Using multiple invokes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---intervene-with-h">Exercise - intervene with <span class="math notranslate nohighlight">\(h\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---combine-the-last-two-functions">Exercise - combine the last two functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---compute-change-in-accuracy">Exercise - compute change in accuracy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">3️⃣ Function Vectors</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Extracting-&amp;-using-FVs">Extracting &amp; using FVs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#A-note-on-out_proj">A note on <code class="docutils literal notranslate"><span class="pre">out_proj</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---implement-calculate_fn_vectors_and_intervene">Exercise - implement <code class="docutils literal notranslate"><span class="pre">calculate_fn_vectors_and_intervene</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---calculate-the-function-vector">Exercise - calculate the function vector</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Multi-token-generation">Multi-token generation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Using-nnsight-for-multi-token-generation">Using <code class="docutils literal notranslate"><span class="pre">nnsight</span></code> for multi-token generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Key-Value-Caching">Key-Value Caching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Generator-Output">Generator Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---intervene-with-function-vector,-in-multi-token-generation">Exercise - intervene with function vector, in multi-token generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---generalize-results-to-another-task-(optional)">Exercise - generalize results to another task (optional)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">4️⃣ Steering Vectors in GPT2-XL</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Steering-model-behaviour">Steering model behaviour</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Exercise---replicate-the-steering-vector-results">Exercise - replicate the steering vector results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Caching">Caching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Padding">Padding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Sampling">Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Other-tips-/-notes">Other tips / notes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">☆ Bonus</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Extensions-of-the-Function-Vectors-Paper">Extensions of the Function Vectors Paper</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#The-Decoded-Vocabulary-of-Function-Vectors-(3.2)">The Decoded Vocabulary of Function Vectors (3.2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Vector-Algebra-on-Function-Vectors-(3.3)">Vector Algebra on Function Vectors (3.3)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Extensions-of-the-Steering-Vectors-Post">Extensions of the Steering Vectors Post</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Suggested-paper-replications">Suggested paper replications</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Inference-Time-Intervention:-Eliciting-Truthful-Answers-from-a-Language-Model">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Steering-Llama-2-via-Contrastive-Activation-Addition">Steering Llama 2 via Contrastive Activation Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#Red-teaming-language-models-via-activation-engineering">Red-teaming language models via activation engineering</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025 NDIF.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>